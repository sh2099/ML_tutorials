{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network regularisation and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll build directly on top of what we did in the amplitude regression task in the previous tutorial.  The first goal is to learn about the issue of over-training, and how to overcome it with network regularisation and early stopping. The second goal is to learn more about the different types of preprocessing that can be used in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline / tasks:\n",
    " - Imports \\& plotting set-up\n",
    " - Loading the data\n",
    "     - limit the training data to just 1000 events, keeping 30k for validation and testing\n",
    "     - this is unrealistic, but a good way to understand over-fitting and how to fix it\n",
    " - Visualising the data\n",
    " - Preprocessing the data\n",
    " - Datasets and dataloaders\n",
    "     - choose a sensible batch size, say 64\n",
    " - Building the neural network\n",
    "     - use a larger network, say 3 layers with hidden dimensions of 50\n",
    " - Optimising (training) the neural network\n",
    "     - train for a long time, 1000-1500 epochs\n",
    " - Plot the train and validation losses as a function of the epochs\n",
    "     - now you should clearly see the over-training problem\n",
    " - Network regularisation\n",
    "     - Dropout\n",
    "         - implement a new network, amp_net_dr, which has a pytorch dropout layer after the input layer and each hidden layer\n",
    "         - re-train the network and check if dropout has fixed the over-fitting\n",
    "     - L1 / L2 regularisation\n",
    "     - Early stopping\n",
    "     - Batch normalisation\n",
    " - Exploring preprocessing options\n",
    "     - check what happens if you don't preprocess your data at all..\n",
    "     - Look at the scikit-learn package a try out a few different data preprocessing functions, e.g. StandardScaler\n",
    "     - See how these affect the data visualisations\n",
    "     - Find preprocessing functions that are suitable for the 4-momentum inputs, and some that are suitable for the amplitudes\n",
    "     \n",
    "Please download the training data `tutorial-2-data.zip` and extract it to the folder `data/tutorial-2-data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run this tutorial in google colab, you can open a new colab and then upload this file. \n",
    "\n",
    "The data can be downloaded using\n",
    "\n",
    "```\n",
    "NEEDS TO BE CHANGED!\n",
    "!wget -O tutorial-2-data.zip https://www.dropbox.com/s/n5e66w91rgmbqz2/dlpp-data.zip?dl=0&file_subpath=%2Fdlpp-data%2Ftutorial-2-data\n",
    "!unzip \"tutorial-2-data.zip\"\n",
    "!mkdir tutorial-2-data\n",
    "!mv dlpp-data/tutorial-2-data/* tutorial-2-data/.\n",
    "!rm -r __MACOSX/\n",
    "!rm -r dlpp-data/\n",
    "!ls\n",
    "```\n",
    "\n",
    "Make sure you switch to a GPU runtime to fully utilize the colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.colors as mcolors\n",
    "import colorsys\n",
    "\n",
    "#labelfont = FontProperties()\n",
    "#labelfont.set_family('serif')\n",
    "#labelfont.set_name('Times New Roman')\n",
    "#labelfont.set_size(14)\n",
    "\n",
    "#axislabelfont = FontProperties()\n",
    "#axislabelfont.set_family('serif')\n",
    "#axislabelfont.set_name('Times New Roman')\n",
    "#axislabelfont.set_size(22)\n",
    "\n",
    "#tickfont = FontProperties()\n",
    "#tickfont.set_family('serif')\n",
    "#tickfont.set_name('Times New Roman')\n",
    "#tickfont.set_size(16)\n",
    "\n",
    "#axisfontsize = 16\n",
    "#labelfontsize = 16\n",
    "\n",
    "#plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "#plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "#plt.rcParams['text.usetex'] = False # this had to be disabled to work on colab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat = np.load(\"data/tutorial-2-data/trn_dat.npy\")\n",
    "trn_amp = np.load(\"data/tutorial-2-data/trn_amp.npy\")\n",
    "\n",
    "val_dat = np.load(\"data/tutorial-2-data/val_dat.npy\")\n",
    "val_amp = np.load(\"data/tutorial-2-data/val_amp.npy\")\n",
    "\n",
    "tst_dat = np.load(\"data/tutorial-2-data/tst_dat.npy\")\n",
    "tst_amp = np.load(\"data/tutorial-2-data/tst_amp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (30000, 5, 4)\n",
      "train amp  shape: (30000,)\n",
      "test  data shape: (30000, 5, 4)\n",
      "test  amp  shape: (30000,)\n",
      "val   data shape: (30000, 5, 4)\n",
      "val   amp  shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data shape: {trn_dat.shape}\")\n",
    "print(f\"train amp  shape: {trn_amp.shape}\")\n",
    "print(f\"test  data shape: {tst_dat.shape}\")\n",
    "print(f\"test  amp  shape: {tst_amp.shape}\")\n",
    "print(f\"val   data shape: {val_dat.shape}\")\n",
    "print(f\"val   amp  shape: {val_amp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have much less data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1000, 5, 4)\n",
      "train amp  shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "trn_dat = trn_dat[0:1000]\n",
    "trn_amp = trn_amp[0:1000]\n",
    "print(f\"train data shape: {trn_dat.shape}\")\n",
    "print(f\"train amp  shape: {trn_amp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly unrealistic, but useful for demonstration purposes.  In practice we might use much larger networks than we use here, and so the number of parameters can be of the same order of magnitude as the number of training events.  This is when we encounter the problem of over-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will make some kinematic plots of the events in the training sample.  Note however that these are not the physical distributions we would measure at the LHC!  In our training data each of these events is associated with an amplitude, which tells us the probability that the event will be produced in the gluon-gluon interaction.  So to get the physical distributions these events would need to be 'weighted' by their amplitude.  However, right now we just want to visualise our training dataset to see what preprocessing we should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_pz(ev):\n",
    "    \"\"\" sums z component of incoming particle's momenta\"\"\"\n",
    "    return ev[:, 0, 3] + ev[:, 1, 3]\n",
    "\n",
    "def get_mass(fv):\n",
    "    \"\"\" gets invariant mass of four vector \"\"\"\n",
    "    msq = np.round(fv[:, 0]**2 - fv[:, 1]**2 - fv[:, 2]**2 - fv[:, 3]**2, 5)\n",
    "    if (msq < 0.).any():\n",
    "        raise Exception(\"mass squared is less than zero\")\n",
    "    return np.sqrt(msq)\n",
    "\n",
    "def get_pt(fv):\n",
    "    \"\"\" returns p_T of given four vector \"\"\"\n",
    "    ptsq = np.round(fv[:, 1]**2 + fv[:, 2]**2, 5)\n",
    "    return np.sqrt(ptsq)\n",
    "\n",
    "def get_met(fv):\n",
    "    \"\"\" returns MET of the event\"\"\"\n",
    "    return np.abs(np.sum(fv[:, :, 1]+fv[:, :, 2], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a histogram of the amplitudes for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "bins=np.logspace(-9, -3, 21)\n",
    "\n",
    "axs.hist(trn_amp, histtype='stepfilled', fill=None, bins=bins)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "axs.set_xscale('log')\n",
    "\n",
    "axs.set_xlabel(\"train amplitudes\")#, fontproperties=axislabelfont)\n",
    "axs.set_ylabel(\"number of events\")#, fontproperties=axislabelfont)\n",
    "\n",
    "#xticks = axs.get_xticks()\n",
    "#axs.set_xticklabels(xticks, fontproperties=tickfont)\n",
    "\n",
    "#yticks = axs.get_yticks()\n",
    "#axs.set_yticklabels(yticks, fontproperties=tickfont)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amplitudes span about 4 orders of magnitude..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the leading photon $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat_leading_photon_pt = get_pt(trn_dat[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "axs.hist(trn_dat_leading_photon_pt, histtype='stepfilled', fill=None)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"leading photon $p_T$ (GeV)\")#, fontproperties=axislabelfont)\n",
    "axs.set_ylabel(\"number of events\")#, fontproperties=axislabelfont)\n",
    "\n",
    "#xticks = axs.get_xticks()\n",
    "#axs.set_xticklabels(xticks, fontproperties=tickfont)\n",
    "\n",
    "#yticks = axs.get_yticks()\n",
    "#axs.set_yticklabels(yticks, fontproperties=tickfont)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the final state gluon $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat_gluon_pt = get_pt(trn_dat[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "axs.hist(trn_dat_gluon_pt, histtype='stepfilled', fill=None)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"leading gluon $p_T$ (GeV)\")#, fontproperties=axislabelfont)\n",
    "axs.set_ylabel(\"number of events\")#, fontproperties=axislabelfont)\n",
    "\n",
    "#xticks = axs.get_xticks()\n",
    "#axs.set_xticklabels(xticks, fontproperties=tickfont)\n",
    "\n",
    "#yticks = axs.get_yticks()\n",
    "#axs.set_yticklabels(yticks, fontproperties=tickfont)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dense network, so the data needs to be in vector format.  We will collapse the data for each event to a single vector of dimension $5\\times4=20$.  The fact that the data is ordered here is important.  To predict the amplitude given the kinematics, the network needs to know which entries correspond to which particles in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_nev = trn_dat.shape[0]\n",
    "val_nev = val_dat.shape[0]\n",
    "tst_nev = tst_dat.shape[0]\n",
    "trn_datf = np.reshape(trn_dat, (trn_nev, -1))\n",
    "val_datf = np.reshape(val_dat, (val_nev, -1))\n",
    "tst_datf = np.reshape(tst_dat, (tst_nev, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_datf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further preprocessing steps we can take.  For example, the inputs are numerically very large $\\mathcal{O}(100)$ and span a large range.  So we could re-scale the inputs by a constant number, or even take the logarithm of the inputs.\n",
    "\n",
    "For now, we'll just re-scale by a constant number, the average final state gluon $p_T$, assuming that this is a natural scale for the problem.  And we should be careful to preprocess the train, validation, and test data in the exact same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = np.mean(trn_dat_gluon_pt)\n",
    "trn_datf = trn_datf / gpt\n",
    "val_datf = val_datf / gpt\n",
    "tst_datf = tst_datf / gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert them to pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_datfp = torch.Tensor(trn_datf)\n",
    "val_datfp = torch.Tensor(val_datf)\n",
    "tst_datfp = torch.Tensor(tst_datf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also preprocess the amplitude data.  As we seen in a plot above, the amplitudes span about 4 orders of magnitude.  This could be difficult for the network to interpolate.  We can aleviate the problem with preprocessing, taking the logarithm of the amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ampl = np.log(trn_amp)\n",
    "val_ampl = np.log(val_amp)\n",
    "tst_ampl = np.log(tst_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_amplp = torch.Tensor(trn_ampl)\n",
    "val_amplp = torch.Tensor(val_ampl)\n",
    "tst_amplp = torch.Tensor(tst_ampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new distribution looks nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "axs.hist(trn_ampl, histtype='stepfilled', fill=None)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"log(train amplitudes)\")\n",
    "axs.set_ylabel(\"number of events\",)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a class for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class amp_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, amp):\n",
    "        self.data = data\n",
    "        self.amp = amp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.amp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.amp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialise them with the data and amplitudes.  We use unsqueeze here so that the shape of the amplitudes matches the shape output from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset = amp_dataset(trn_datfp, trn_amplp.unsqueeze(-1))\n",
    "val_dataset = amp_dataset(val_datfp, val_amplp.unsqueeze(-1))\n",
    "tst_dataset = amp_dataset(tst_datfp, tst_amplp.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a dataloader for each of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check that the shape for the kinematic data and amplitudes come out correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a very simple class for our neural network, which we call amp_net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class amp_net(nn.Module):\n",
    "    \n",
    "    # default hdn_dim is 30, but can be changed upon initialisation\n",
    "    def __init__(self, hdn_dim=3):\n",
    "        \n",
    "        super(amp_net, self).__init__()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ipt_dim, hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, opt_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising (training) the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide what function we want the neural network to optimise, i.e. the loss function.  There are a number of choices to decide from, the key point is that the loss function should be minimised when the neural network correctly predicts the amplitude given the kinematical information on the event.  For this we'll use the MSE, the mean squared error.\n",
    "\n",
    "$\\text{MSE} = ( \\text{pred_amp} - \\text{true_amp} )^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch will compute gradients of this loss function with respect to the trainable parameters of the network.  The network is optimised by updating the parameters using the gradients.  The exact scheme to do this is defined by our choice of optimiser.  A standard choice is the Adam optimiser, which we will use here with it's default hyper-parameters.\n",
    "\n",
    "You can read more about this optimiser here:\n",
    " - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    " - https://arxiv.org/abs/1412.6980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already set the batch size when defining the dataloaders.  Below we set the number of epochs, i.e. the number of times we will iterate through the training data to optimise the network.  This means that the total number of updates to the network parameters will be $\\simeq\\text{size of dataset}\\times \\text{epochs} / \\text{batch size}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a training loop for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    loss_during_opt = 0.\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # pass data through network\n",
    "        pred = model(X)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss_during_opt += loss.item()\n",
    "\n",
    "        # reset gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights with optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print the training loss every 100 updates\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print( f\"current batch loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\" )\n",
    "    \n",
    "    loss_live = loss_during_opt/len(dataloader)\n",
    "    print(f\"avg train loss per batch in training: {loss_live:>8f}\")        \n",
    "    return loss_live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor the performance of the network on the regression task we want to calculate the loss of both the training data and the validation data on the same network, so we have the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_pass(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    vl = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            vl += loss_fn(pred, y).item()\n",
    "\n",
    "    vl /= num_batches\n",
    "    print(f\"avg val loss per batch: {vl:>8f}\")\n",
    "    \n",
    "    return vl\n",
    "\n",
    "def trn_pass(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    tl = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            tl += loss_fn(pred, y).item()\n",
    "\n",
    "    tl /= num_batches\n",
    "    print( f\"avg trn loss per batch: {tl:>8f}\" )\n",
    "    \n",
    "    return tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "amp_net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 8751 trainable parameters\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 209.814240  [    0/ 1000]\n",
      "avg train loss per batch in training: 208.334886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 204.623249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 206.239863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 211.517319  [    0/ 1000]\n",
      "avg train loss per batch in training: 198.182022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 188.338308\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 189.528432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 191.548553  [    0/ 1000]\n",
      "avg train loss per batch in training: 170.577603\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 143.003304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 143.248122\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 145.345459  [    0/ 1000]\n",
      "avg train loss per batch in training: 103.210661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 55.267512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 55.257839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 52.659328  [    0/ 1000]\n",
      "avg train loss per batch in training: 41.453562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 39.249850\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 44.406534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 18.345604  [    0/ 1000]\n",
      "avg train loss per batch in training: 35.005785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 32.623937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 34.581993\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 37.926788  [    0/ 1000]\n",
      "avg train loss per batch in training: 32.026336\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 29.736510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 32.128891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 59.100716  [    0/ 1000]\n",
      "avg train loss per batch in training: 28.784769\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 27.796353\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 30.310134\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.948064  [    0/ 1000]\n",
      "avg train loss per batch in training: 27.215636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 25.645658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 28.045583\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 33.362263  [    0/ 1000]\n",
      "avg train loss per batch in training: 25.025449\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 23.912109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 26.273122\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 19.334484  [    0/ 1000]\n",
      "avg train loss per batch in training: 23.501700\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 22.334623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 24.558069\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 30.712914  [    0/ 1000]\n",
      "avg train loss per batch in training: 21.937977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 20.955357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 23.141018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 26.036917  [    0/ 1000]\n",
      "avg train loss per batch in training: 20.208533\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 19.388080\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 21.331991\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 21.135073  [    0/ 1000]\n",
      "avg train loss per batch in training: 18.599045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 17.677508\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 19.546672\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.943009  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.789501\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 15.809802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 17.745424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.505348  [    0/ 1000]\n",
      "avg train loss per batch in training: 15.097780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 14.093132\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 16.051957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.445509  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.409357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 12.171836\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 13.821644\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.002682  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.563256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 10.518624\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 11.785683\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.081344  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.470546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 8.190288\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 9.801438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.219115  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.474255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.400927\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 7.526695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.969715  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.625302\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.721328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.724062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.285926  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.086704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.349757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.190119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.486310  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.971993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.460866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.382609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.029080  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.106320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.806123\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.526501\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.737052  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.658316\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.468795\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.140151\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.746164  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.347219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.211603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.711427\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.863839  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.168179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.066800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.506829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.092717  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.029422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.944636\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.372633\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.807005  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.924936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.871881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.250764\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.443664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.880230\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.811822\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.176547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.872701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.772787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.753448\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.095031\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.667513  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.738066\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.699594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.029439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.587467  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.698651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.667070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.982214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.548840  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.675862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.623388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.971456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.058369  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.626476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.583986\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.926236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.341035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.595137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.593489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.885045\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.398673  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.574524\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.545335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.869777\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.663918  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.546789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.525135\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.839097\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.437922  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.533985\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.522703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.860819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.446986  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.512250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.500011\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.791712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.626506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.495455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.478361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.800569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.523643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.479311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.462220\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.774748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.402801  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.470503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.455351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.784003\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.512926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.450819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.447064\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.736708\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.587640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.437093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.428632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.750302\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.354516  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.436802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.444074\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.719025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.531261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.425923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.412072\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.707684\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.325338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.414537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.402338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.707950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.392908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.401798\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.389557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.702338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.320697  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.397108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.385483\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.697686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 51\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.270933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.407802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.384706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.726049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 52\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.508519  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.394223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.385449\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.654187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 53\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.422700  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.379038\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.359866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.674284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 54\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.364125  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.362841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.360747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.697476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 55\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.494427  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.361758\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.674299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 56\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.394734  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.356738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339480\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.640600\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 57\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.382633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.346929\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.333650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.627223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 58\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.434896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.343205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.647690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 59\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.353374  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.328359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.614805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 60\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.321702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.329194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308709\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.609437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 61\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.250402  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.322866\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328419\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.595987\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 62\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.256488  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.312738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.302029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.579469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 63\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.310441  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.302144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.583808\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 64\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.292981  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.305095\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.287614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.581079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 65\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.274970  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.292092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.287663\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.575861\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 66\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.332672  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.291909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280519\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.562401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 67\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.318897  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.282148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.560562\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 68\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.238522  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.279248\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.600536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 69\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.307635  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.286547\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265763\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.547358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 70\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.250578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.280003\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.534675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 71\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.346688  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.278244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.610163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 72\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.225895  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.263665\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253758\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.533690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 73\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253634\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.512935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 74\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.315551  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262299\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276326\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.582627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 75\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.249199  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.261349\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.528258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 76\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.315331  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.256156\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.506039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 77\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.237023  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.255441\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.575180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 78\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.253760\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238509\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.491736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 79\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.260557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.244556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 80\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.222645  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.253902\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251160\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.490158\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 81\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.257628  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.258223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237949\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.520200\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 82\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197316  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.253956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254291\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.476618\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 83\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.282469  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.225016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.503273\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 84\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.248178  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.231240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.468068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 85\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.271247  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226989\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.216541\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.491130\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 86\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.267575  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.231041\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.481692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 87\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.228625  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.220719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.474224\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 88\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.332265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.223085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208416\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.469329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 89\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.252915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212491\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.489581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 90\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159486  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.214811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203004\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.464472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 91\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.230148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.214806\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220828\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.500330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 92\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190210  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.216799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.471510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 93\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.216355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.462650\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 94\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146435  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.217620\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 95\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.215209  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.205437\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.457262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 96\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197350  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.194420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.191485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.458580\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 97\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221041  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.193636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188273\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.449202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 98\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182199  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.449134\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 99\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190224  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.187531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183966\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.434784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157613  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.190686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.453210\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221314  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.201183\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181894\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.424723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.240470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.193814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.184263\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.450193\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181630  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.421758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.218766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190272\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.191628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.459678\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161901  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.174461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.414297\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.251207  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.200528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.420965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157654  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191381\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.173567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.416002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169928  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177002\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.184503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.405924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196459  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.489454\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.191169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181176\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.450664\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132791  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.402457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160610  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179468\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.400388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.412301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.185785  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.479288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180016\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139126  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168213\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.405516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164929  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.165292\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163806\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398843\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178820  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.165367\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160019\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162772\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.396971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175697  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.165560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154938\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136216  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177775\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.192964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.485611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.201049  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175712\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160345\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.384371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196001  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163460\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.168274  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.179150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167147  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133920  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155854\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152447\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.379515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156515  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.150920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166284  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157516\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162052\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.424675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178454  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147679\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.201852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.376812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162715\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148644\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170220  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.390807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147973  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160791\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172004\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.388597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.396060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155397  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.400142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166635  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167663\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.373994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.199837  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152300  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.455768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160141  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.402337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134055  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117719  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140926\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134924\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.372893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177558  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152770\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134942\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.379855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154021  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372697\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122632  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141495\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108598  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047839  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138918\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148514  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131480\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355364\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142649\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.400801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104454  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.376333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090776  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356089\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130892\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128355\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122254  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130175\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126731  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133449\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357452\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121598  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131039\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358845\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128084  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124869\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372811\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353826\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154716  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353753\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088088  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361112\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124368\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.351058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122638\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.360272\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122438  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129095\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126190\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352753\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121449\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134306  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145488\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408046\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147813  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123422\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.365878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155080  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119956\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125518\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120996\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119801  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123656\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370367\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187809  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370613\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137625  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122681  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120517\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.360131\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112999  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123216\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.369912\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109746  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124288\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124027\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118327  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122108  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121679\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116921\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131281  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114147\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.341459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093268  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103445  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123041\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 190\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116873\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131549  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345519\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099436  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 193\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 194\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146250  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361452\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 195\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123481\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119671\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 196\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130191  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 197\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111508\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 198\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100324  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114270\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154142\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 199\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131348\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333853\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 200\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116842  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 201\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 202\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337897\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 203\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084056  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115660\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109483\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342029\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 204\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099394  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113715\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 205\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105490  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116749\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355279\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 206\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102068  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.404521\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132591  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120538\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 208\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102073  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107353\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.324666\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 209\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080091  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 210\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115334  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 211\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104865  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105128\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 212\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126250  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108088\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 213\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098606  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114066\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 214\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344986\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 215\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136576  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108683\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 216\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117336\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.320569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118679\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 218\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107421  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.346996\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 219\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157379  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123198\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122710\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 220\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112876\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327804\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 221\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090650  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106769\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 222\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 223\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110610\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121427\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371880\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 224\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.173965  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177616\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452769\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 225\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158197  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 226\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 227\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 228\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162475  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387682\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 229\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127133  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119414\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.360653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124197  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107490\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101714\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 231\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105330  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.320300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 232\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128811  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327841\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 233\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115049  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108321\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323114\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 234\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124890  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315034\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 235\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150928  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 236\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 237\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112676  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108117\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318568\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 238\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113326  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131247\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 239\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308936\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 240\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108962\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 241\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091718  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108074\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316251\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 242\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109281  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112460\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 243\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094163  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103112\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323613\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 244\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 245\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083424  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103620\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 246\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089507  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105398\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 247\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 248\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105996  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310426\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 249\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133031  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 250\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051101  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102726\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095823\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.316117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 251\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092300  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119695\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 252\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095185\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 253\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095852\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322309\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 254\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322529\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 255\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086386  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 256\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071116  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096980\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128994  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102530\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102669\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 259\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089345  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104667\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137442  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109971\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 261\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310166\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 262\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136018  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108771\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 263\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 264\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080428  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120584\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 265\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125449  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 266\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104134  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102123\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 267\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134223\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397083\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 268\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076082  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099632\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 271\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105229\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.307277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 272\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087381  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298094\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 273\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063144  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093033\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317033\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 274\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066637  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096519\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097938\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 275\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090932  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096765\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 276\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060330  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101550\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 277\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101888  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 278\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 279\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095869  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094696\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093956\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094898  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094221\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 281\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 282\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086495  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106251\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 283\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088062  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 284\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102324  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311339\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 285\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087983\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 286\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065068  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302919\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 287\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337679\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 288\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167886  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087883\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 289\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056243  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097368\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 290\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 291\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113226  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094866\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 292\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084801  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092518\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096048\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.296443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 293\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102415  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092885\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096081\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 294\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 295\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 296\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099017  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096825\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087277\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 297\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091796  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093016\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086024\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 298\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098598\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090160\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 299\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 300\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070600  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092822\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.085165\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304379\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 301\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090377\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 302\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084508\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 303\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057725  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290460\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 304\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097112  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088164\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 305\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084352\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 306\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117005  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 307\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120139  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308296\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 308\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086934\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309162\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 309\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323834\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 310\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078969  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093829\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 311\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081118  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103582\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094946\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 312\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092611  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084865\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 313\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057462  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091553\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101262\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.291290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 314\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 315\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096258  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105583\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087619\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 316\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129103  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096825\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097871  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103955\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086681\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 318\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108696  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 319\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130215  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096899\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281501\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 320\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114859  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.080152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 321\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049249  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301674\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 322\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060580  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 323\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076073  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082318\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289304\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 324\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060208  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087622\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288331\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 325\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085407  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084459\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081316\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 326\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104257  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 327\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110122  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083422\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 328\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086755  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098829\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 329\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099117  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 330\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123039  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 331\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070163  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.080680\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 332\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 333\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081049\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 334\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093624\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.328209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 335\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 336\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 337\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096423  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087430\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 338\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063055  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094240\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.324998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 339\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298886\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066811  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081726\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078519\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 341\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080910\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 342\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092614  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 343\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080337  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090622\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 344\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110597  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298673\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 345\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095145\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 346\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082380  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081884\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 347\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077732\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289527\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 348\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094133  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295500\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 349\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059154  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281108\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 350\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114903  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 351\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078414\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 352\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088837\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 353\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075427  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093648\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 354\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104682  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272489\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077442\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074043\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.274633\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 356\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085770  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293987\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 357\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094416\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076439\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 358\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093367\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076050\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285525\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 359\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075573\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 360\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074030  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084200\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 361\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044739  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079986\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 362\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 363\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077904\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288235\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 364\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069252  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 365\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089259  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 366\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109801  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 367\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071340  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 368\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075307\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074402\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278113\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 369\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078306\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 370\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087492  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.080426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270130\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 371\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059192  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.080857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 372\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275073\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 373\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105429  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078295\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 374\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069874  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 375\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064687  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277949\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 376\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093158  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073626\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.278067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 377\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051164  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072439\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 378\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067357  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076565\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 379\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064528  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 380\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074538  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085389\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079038\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 381\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102841  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 382\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103630  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077044\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263649\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 383\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 384\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107546  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 385\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050482  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298637\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 386\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078563  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084974\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.085498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 387\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061773  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124144\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 388\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143600  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.179272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 389\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153559  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 390\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073868\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 391\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077271  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083546\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295823\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 392\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096832  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.287218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 393\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075586\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 394\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091564  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072870\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067942\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 395\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081765  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073797\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 396\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077244  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074186\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069841\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 397\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050437  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089328\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.307474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 398\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106531\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074936\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 399\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058585  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081107\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260637\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 400\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076045  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074062\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078728\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297342\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 401\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102441  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073758\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264198\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 402\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073057  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070931\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 403\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058089  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074624\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 404\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263664\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 405\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106102  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311586\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 406\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084313  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282509\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 407\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069234  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273008\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 408\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095782  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 409\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056201  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068037\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 410\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 411\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071044\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 412\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079848  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072827\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 413\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080427\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 414\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060439  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 415\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071773  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276322\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 416\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062377  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.269137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 417\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059697  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 418\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081083  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067674\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.263857\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 419\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 420\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065892  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066279\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259291\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 421\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 422\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073323  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 423\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043207  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068378\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 424\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071558\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262368\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 425\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080068  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066383\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 426\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.080106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 427\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101273  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073247\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 428\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058283  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068875\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065422\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272753\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 429\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 430\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066631  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098292\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326272\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 431\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115730  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084306\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 432\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 433\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078164  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258811\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 434\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085340  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069374\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280085\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 435\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065281\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 436\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070009  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279492\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 437\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079038  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071294\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072133\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265114\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 438\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067691\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255389\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 439\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064041  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070536\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.249204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 440\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080639  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064419\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 441\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061384\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 442\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043980  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 443\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052575  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253026\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 444\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 445\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 446\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067624  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276450\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 447\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070592\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 448\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062585  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065867\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 449\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087058  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 450\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063296  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065055\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 451\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043238  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064482\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263684\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 452\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074310  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062695\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 453\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 454\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057728  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064348\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060444\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251861\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 455\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062109  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257562\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 456\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081789  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103085\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 457\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087333  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073270\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 458\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064459\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260584\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 459\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050403  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065708\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260853\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 460\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052727  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067910\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060151\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.260048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 461\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067123  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064281\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059967\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 462\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064690\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 463\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075482  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064334\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 464\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076065  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062261\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 465\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093350  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 466\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089130  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265701\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 467\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046216  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243803\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 468\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051161  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064172\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 469\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087349  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062476\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252892\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 470\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054098  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066563\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258906\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 471\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 472\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062121\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257860\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 473\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067586\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065213\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255652\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 474\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062845\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247350\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 475\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 476\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039840  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262112\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066162  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076305\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 478\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101800  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064460\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064537\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 479\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064595\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 480\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067930  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061475\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 481\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075466  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058887\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.259943\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 482\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066545  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 483\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068187\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247698\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 484\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060131\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 485\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047102  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062849\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 486\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055246  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061618\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246346\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 487\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073209  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 488\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074276  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072935\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 489\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066606  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075457\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 490\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076084  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065161\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 491\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086813  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257017\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 492\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086251  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058715\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 493\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055025\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246619\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 494\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050632  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060610\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247580\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 495\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059640\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056681\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251140\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 496\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 497\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043139  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061375\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058137\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259902\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 498\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260421\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 499\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077806  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056459\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249191\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 500\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066878\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 501\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057296\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 502\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051297  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067714\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056810\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.243612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 503\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050610  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 504\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064358  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056590\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238990\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 505\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056057\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 506\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062171  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252491\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 507\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 508\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056229  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 509\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057243  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063167\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262618\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 510\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054788\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240210\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 511\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250106\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 512\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061219  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 513\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059939  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059735\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 514\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267266\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 515\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050698  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072290\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 516\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069668  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 517\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064918  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 518\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064830\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243273\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 519\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251105\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 520\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081913  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243283\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 521\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058843  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 522\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035681  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057557\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 523\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064900\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058597\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.245023\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 524\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051542  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060078\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 525\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046556  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 526\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063169\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 527\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050929  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055565\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233237\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 528\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053037\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246936\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 529\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062039  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 530\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069237  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051011\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246023\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 531\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052756  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236368\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 532\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054831  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 533\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073165  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 534\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046527  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 535\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047323  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053456\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236847\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 536\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055706\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049912\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 537\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058257  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050664\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 538\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049961  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240182\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 539\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050403\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240621\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 540\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060185\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063014\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 541\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069172  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052377\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 542\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043036  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051913\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 543\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066076\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243793\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 544\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069855  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074082\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.294909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 545\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075512  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051539\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 546\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064394  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052854\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 547\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 548\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032523  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058057\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 549\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260988\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 550\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057262  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 551\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078733  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072337\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 552\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086222  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250929\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 553\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065617  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 554\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053611  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 555\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047341  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 556\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047523  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 557\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 558\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069422  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 559\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033005  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050151\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249649\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 560\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065842  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252415\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 561\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051490\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234291\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 562\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036534  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 563\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 564\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 565\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050227  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050920\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045645\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.236004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 566\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067950  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050057\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238312\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 567\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051505  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053391\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047317\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 568\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227460\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 569\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249408\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 570\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066589  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057087\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 571\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047102  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058515\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230350\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 572\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068391  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 573\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045563  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050957\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241243\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 574\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065918  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233005\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 575\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068133\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 576\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085336  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246860\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 577\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045884\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 578\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055511\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 579\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042163  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055323\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240452\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 580\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045126  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064538\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071868\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 581\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055125  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054930\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263267\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 582\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062493  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051276\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 583\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057248\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242247\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 584\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047927\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245032\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 585\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047279  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047399\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228622\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 586\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058450\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.229065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 587\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252559\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 588\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048043  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 589\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053228  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235186\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 590\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043780  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082904\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306526\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 591\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099711  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 592\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053885\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046350\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 593\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042245  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 594\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054819  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058223\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 595\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059149  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242652\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 596\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053072\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 597\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039352  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273127\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 598\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041284  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 599\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090581  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074213\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 600\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070522  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 601\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061419  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048771\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044609\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 602\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037098  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044490\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042509\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 603\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232455\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 604\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055295  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 605\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051622\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 606\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056118  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232140\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 607\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044969  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045186\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.243517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 608\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042711\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 609\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040146  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046551\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 610\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044749\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042038\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237897\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 611\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040678  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 612\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075255  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 613\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049962  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055891\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237077\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 614\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038196  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 615\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039479  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 616\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064774\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235283\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 617\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058931  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 618\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047281  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227859\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 619\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045827  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043088\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247859\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 620\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051453  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 621\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038921  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 622\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053522  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048039\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 623\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038241  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 624\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032471  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045404\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 625\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033861  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227237\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 626\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036723  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042638\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 627\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 628\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046878  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048438\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049391\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.258236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 629\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 630\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043444  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039927\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 631\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223996\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 632\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033762  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041014\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241491\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 633\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042441\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222116\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 634\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045428\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228576\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 635\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054781  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 636\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047749  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056107\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256811\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 637\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040780  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221688\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 638\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236771\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 639\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031205  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 640\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047316\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 641\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045479  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042725\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 642\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047474  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252861\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 643\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 644\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039399  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 645\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035341  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 646\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041876  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 647\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048432  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039037\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 648\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 649\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057898  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038701\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.235448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 650\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039003\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 651\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029604  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048312\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 652\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054406  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044399\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241533\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 653\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043724\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 654\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044378  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 655\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032787  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047610\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044320\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 656\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040686  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 657\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031818  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241551\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 658\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057871  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 659\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047602\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047233\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254063\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 660\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036012  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249383\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 661\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036151  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041444\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231364\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 662\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044004  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041193\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232498\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 663\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034558  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 664\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043725\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036587\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 665\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041452  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044958\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 666\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028598  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036918\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226201\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 667\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049325  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 668\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050595  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043920\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230450\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 669\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041885\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 670\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051550  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051446\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069796\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.290226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 671\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072080  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054188\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236248\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 672\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044368  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041387\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040172\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228521\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 673\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043156  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 674\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031492  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040354\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036817\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 675\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039900\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227335\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 676\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063609  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036980\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221116\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 677\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035319  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 678\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039810  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037970\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 679\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035995  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 680\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031448  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 681\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030346  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038374\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 682\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045842  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040873\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 683\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041040  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 684\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048934\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 685\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 686\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070372  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 687\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043543\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229320\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 688\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036835  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039793\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046822\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 689\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062289  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252941\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 690\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047167  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037277\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 691\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039077\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038306\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.233940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 692\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029252  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037682\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 693\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025576  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036854\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 694\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030736  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036538\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 695\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022237  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243124\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 696\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049903  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 697\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040611  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 698\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032632  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233708\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 699\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034684  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041989\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037240\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 700\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039566  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039116\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038370\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 701\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051614  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 702\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035833  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 703\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039071  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036784\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 704\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031662  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036778\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 705\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041029  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036857\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034384\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223803\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 706\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033804  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222962\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 707\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043237  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042931\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 708\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038724  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045403\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 709\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050274  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042164\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 710\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045513\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036223\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 711\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029229  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044640\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047914\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266345\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 712\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042742  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035937\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.224398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 713\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034419  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 714\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048438  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042070\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036527\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 715\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036907  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037703\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 716\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038334\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034215\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220647\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 717\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021788  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038609\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 718\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034428  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032822\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 719\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031849  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041161\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 720\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037275  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042584\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233403\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 721\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024955  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041133\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225378\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 722\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222311\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 723\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050183  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037254\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 724\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061406  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 725\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031332  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034902\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217862\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 726\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031870  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040232\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 727\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034711  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 728\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035862\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240426\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 729\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042248  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036000\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 730\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033997\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 731\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 732\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040010  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216484\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 733\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023612  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043861\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047423\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.254508\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 734\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059795  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 735\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043225  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215161\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 736\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041606  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037483\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 737\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034026  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 738\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068716  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 739\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047403  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036965\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220270\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 740\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 741\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031924\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 742\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027874  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038824\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 743\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036161\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213359\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 744\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037520  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050611\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 745\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060858  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042800\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 746\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044377  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 747\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086778  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042080\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035005\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 748\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220780\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 749\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037828  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 750\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033110  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232808\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 751\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040567  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035135\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 752\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022528  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046148\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245455\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 753\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039388  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 754\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035297  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036334\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.217222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 755\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035928  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036426\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 756\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051195  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039213\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 757\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 758\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051446\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 759\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053328  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216817\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 760\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038662  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037955\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220594\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 761\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038997\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 762\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028641  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035512\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 763\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020914  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037495\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 764\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035507  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043952\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 765\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034967\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 766\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040783  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222894\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 767\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035956  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 768\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217555\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 769\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250169\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 770\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033253  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 771\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038918  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 772\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029564  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033523\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 773\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030844  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033724\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032321\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221316\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 774\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035591  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037518\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214484\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 775\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036472\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.234471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 776\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031567  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 777\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080791  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057809\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 778\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034541  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043747\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 779\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067301  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058203\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.080656\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 780\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065844  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 781\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041610\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 782\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 783\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025234  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220697\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 784\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 785\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026809  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028992\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220460\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 786\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027141  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 787\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041610\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 788\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033517  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029349\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 789\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043907  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 790\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033711  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048819\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221296\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 791\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056484  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 792\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025837  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232889\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 793\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035538  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030636\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 794\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029052  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035068\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213900\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 795\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 796\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045097  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031664\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.212413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 797\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031682  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222869\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 798\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026055  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 799\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043203  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 800\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038316  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035217\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028638\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 801\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032299  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032745\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 802\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037509  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028941\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223818\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 803\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031790  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 804\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030755  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 805\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039136  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029740\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226824\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 806\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021582  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030074\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 807\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022990  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031252\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 808\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033656\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225172\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 809\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022845  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218259\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 810\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029364  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028394\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 811\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038712  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027587\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216133\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 812\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023850  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038145\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 813\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224088\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 814\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036306  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029705\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028924\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 815\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026213  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032495\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 816\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031693  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034336\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 817\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032377  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047396\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.216392\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 818\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039878  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 819\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050616  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215364\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 820\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039946\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 821\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036768\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218714\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 822\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033635  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035921\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233390\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 823\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026840  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 824\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036744  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029516\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 825\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034769\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038382\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 826\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036890\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 827\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 828\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032385  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 829\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026549  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 830\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024082  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029688\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 831\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030063\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 832\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022834  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223390\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 833\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021688  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 834\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040945\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 835\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042421  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 836\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040105\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042652\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 837\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 838\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023696  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029540\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.218842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 839\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 840\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032259  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255416\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 841\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034768  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 842\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031381  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218572\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 843\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029240\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 844\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 845\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028824\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 846\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027574\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222918\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 847\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 848\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071110  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 849\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028513\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219644\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 850\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024566  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 851\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218726\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 852\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030718  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 853\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025909\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221532\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 854\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035822  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027165\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 855\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037117  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044293\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 856\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 857\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028625  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025994\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 858\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026850\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 859\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016132  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029319\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.247216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 860\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 861\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031599  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029457\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038949\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 862\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027672  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028427\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 863\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018553  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 864\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027990\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 865\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041784  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032770\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 866\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052549  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041306\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 867\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037276\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027266\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 868\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 869\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032762  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216920\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 870\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048168  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 871\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058868  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032179\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 872\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030178  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032559\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025709\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 873\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029866  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030449\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 874\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022563  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 875\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 876\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025049  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031331\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 877\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028644  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026293\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 878\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026160\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 879\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032017  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 880\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023839  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029074\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.231459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 881\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030245  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 882\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043876  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 883\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023499  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229375\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 884\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027010\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235395\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 885\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039858  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 886\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024608  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 887\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 888\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021079  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023618\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227888\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 889\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026106\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024914\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 890\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016892  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 891\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032763  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 892\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024537  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027256\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 893\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020355  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 894\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020048  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027300\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 895\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025312  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029016\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023545\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 896\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238795\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 897\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027133  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279229\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 898\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038403  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.036818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035878\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254657\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 899\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042610  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024616\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234331\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 900\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021199  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026997\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 901\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033550  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027980\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024006\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.236932\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 902\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020091  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 903\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020827  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024745\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 904\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014693  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219639\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 905\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039600  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 906\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029952  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028852\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 907\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027279  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 908\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021392  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023132\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 909\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016342  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 910\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027092  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033116\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 911\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027575  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027559\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026315\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218201\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 912\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023370  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 913\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035575\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260969\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 914\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033890  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029254\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240887\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 915\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016688  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026884\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246769\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 916\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028538  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029853\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 917\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038927\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 918\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059274  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 919\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032507  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029772\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 920\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034489  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030787\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253751\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 921\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 922\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031852\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036442\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.231348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 923\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027384  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 924\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023695\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 925\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025936  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037178\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 926\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036523  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 927\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026307\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023855\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236985\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 928\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026866  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024539\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 929\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024294\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 930\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 931\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024924\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224989\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 932\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027254  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027885\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 933\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 934\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 935\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033159  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228100\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 936\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037595\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 937\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040878  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033844\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 938\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044817  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224845\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 939\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233129\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 940\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019032  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026003\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 941\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 942\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025226  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026164\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023739\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239130\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 943\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025213\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.243244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 944\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017816  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 945\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023239\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 946\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020818  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 947\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031553\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 948\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019126  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221976\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 949\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024116  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023399\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 950\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024550\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 951\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027401\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219249\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 952\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025239  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025174\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 953\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028410\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217872\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 954\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038324  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023828\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 955\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025055  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217351\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 956\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030345  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224101\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 957\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032390  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021589\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 958\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012152  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028368\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264620\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 959\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032395  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030236\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236279\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 960\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022606  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025123\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 961\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032319  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223658\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 962\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035033  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025575\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022868\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 963\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023217\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021428\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227422\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 964\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025360\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.244405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 965\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025985\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239711\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 966\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017890  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 967\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022234  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023343\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227785\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 968\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018722  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020299\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 969\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017364  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022399\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230770\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 970\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026861  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020789\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 971\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020996\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 972\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017755  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023488\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 973\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025704  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027748\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249810\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 974\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032545  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 975\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023700  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021320\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230719\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 976\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032023  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 977\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025863  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024743\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226628\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 978\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027282  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033034\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 979\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028437  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030696\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223513\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 980\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022317\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 981\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021483\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 982\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030047  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027415\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 983\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 984\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022380\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 985\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025690  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021306\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.233786\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 986\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021962\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 987\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023854  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025373\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020616\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 988\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227618\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 989\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012531  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 990\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017580  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226415\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 991\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018646  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215795\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 992\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031070\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028414\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 993\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033587\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232140\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 994\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026353\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243843\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 995\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034733  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260396\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 996\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024101\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 997\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022757\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 998\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020269  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221135\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 999\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025281  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1000\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026515\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031813\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242854\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1001\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033083  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027185\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023740\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1002\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028799\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217751\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1003\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1004\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024525\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1005\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028796\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242551\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1006\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022174  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023187\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.244568\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1007\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025593  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1008\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023262  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025558\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228170\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1009\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017590  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028703\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1010\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020621  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1011\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030220\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023845\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1012\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234754\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1013\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023818  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027081\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241573\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1014\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028328  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030383\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1015\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033242  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025671\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022344\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1016\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028237  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1017\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019499  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223600\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1018\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022247  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020912\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1019\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023956  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018847\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231845\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1020\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250361\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1021\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022167\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1022\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1023\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1024\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024931  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1025\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1026\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025309\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.248063\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1027\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033554\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1028\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031691  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028341\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021938\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235392\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1029\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018121  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023518\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1030\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1031\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021835  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1032\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016670  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289370\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1033\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057954  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1034\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038890  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.034182\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265755\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1035\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231596\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1036\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1037\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021429  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1038\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1039\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023861\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244199\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1040\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019250  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1041\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026517  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239797\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1042\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017158  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1043\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023167  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024521\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022266\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1044\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019244  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1045\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028372\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234903\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1046\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016162  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020053\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.228712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1047\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024706  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024178\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1048\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018477  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027032\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240416\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1049\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032292  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024224\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1050\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022246\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023941\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1051\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020379\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228959\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1052\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018721  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020115\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1053\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032472  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227869\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1054\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019154  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019876\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023148\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222312\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1055\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022078  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228370\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1056\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028376  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232760\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1057\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015872  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1058\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024383  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023131\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245492\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1059\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018551\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229161\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1060\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015012  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1061\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021493\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231509\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1062\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023385  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1063\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024773\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248551\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1064\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021595  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023574\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1065\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017345  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023678\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1066\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031368  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027839\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022517\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.227866\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1067\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016012  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1068\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021598\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1069\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022175  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025437\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1070\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030693\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1071\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039996  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019816\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1072\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014607  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216652\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1073\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017898  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023445\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021629\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1074\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1075\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023136  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1076\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1077\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.010946  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023145\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022689\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232524\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1078\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020103\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1079\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015447  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023040\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1080\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025848  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1081\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029959  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026921\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020127\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223949\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1082\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017165  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1083\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031655  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.037406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1084\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033314  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1085\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1086\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036211  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022478\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028594\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.229199\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1087\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1088\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019226  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029627\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1089\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033026  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022789\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224601\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1090\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023695\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024829\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239019\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1091\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019884  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268804\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1092\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029103  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272993\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1093\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041868  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027370\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1094\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066213  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033583\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1095\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032071  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025858\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019521\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1096\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021075  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018631\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230158\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1097\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026218  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019996\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1098\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021731  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1099\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021084  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022335  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016543  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019220\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019586  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019700\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018066\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228919\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014825  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020374\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019873  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022791\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021162\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.220944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024585  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020778  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027056  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022288\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023646  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228886\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021444  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021190\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017609  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032587\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031776\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037808  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036187\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231874\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032575  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020438  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024804  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022469  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019948\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020882  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229926\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022758\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021407  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248211\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019520  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022265\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251195\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023638  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022420\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031268  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026385\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030542  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022632\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018633\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242280\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020111  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025648\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025129\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.221836\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019477  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019454\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237927\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024725  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247361\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015309  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036243  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220107\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030483\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037863\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035222  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028930\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025588\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029050  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022452\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015972  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023012  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021616\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030036\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263689\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034628  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023423  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021457\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025200\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032770\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227075\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046803  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025247\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020806\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017329\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.233002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016924  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021411  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019203\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025107\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025037  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023384\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024327\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018795\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022682\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020075  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242251\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226293\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017742  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014106  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019883\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017285\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019195\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021745  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020138\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018932\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027540  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020178\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017292  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017759  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019705\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015187  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016861  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236719\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019785  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020140\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.240104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020892\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018597  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235607\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012756  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024706\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019643\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221394\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031216  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034581  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018679\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016106  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020887  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018915\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224572\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018678  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245304\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022198  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018947\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.010661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016955  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019354\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244764\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015162  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022972  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025026\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020790\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024953  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019002\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019456\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018418\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014723  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015617  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041748\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.286787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050604  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.031990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262072\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029539  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225803\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018114  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019928\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244992\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1190\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020085\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023968  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020541  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1193\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025065  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1194\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013441  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023914\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1195\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023874  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018751\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1196\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020499\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1197\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1198\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024822\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231263\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1199\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025903  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017105\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238961\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1200\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019152  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026419\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257440\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1201\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038735  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025214\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.269415\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1202\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1203\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024549  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020330\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1204\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1205\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016785  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1206\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028773  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017060\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.231403\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013797  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224804\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1208\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024852\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1209\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022650\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019353\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1210\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015742  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1211\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020020  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017082\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1212\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1213\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016752  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232689\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1214\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019168\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018504\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1215\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012235  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024312\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264859\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1216\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022116  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036839\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043170  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1218\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019603  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241319\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1219\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014212  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245359\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1220\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024492  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1221\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022238  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016448\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1222\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013377  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1223\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016404  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016445\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240945\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1224\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217917\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1225\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031447  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020956\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1226\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012872  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027994\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.271400\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1227\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030341  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1228\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045471\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292673\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1229\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053950  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024708  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1231\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1232\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019695  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1233\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021400  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022573\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1234\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020284  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1235\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015871  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224866\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1236\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026073  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016811\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234800\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1237\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013142  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017779\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015841\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1238\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013004  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1239\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018747\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1240\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018030\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1241\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015799\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1242\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1243\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019890\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016483\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1244\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012972  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018645\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253427\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1245\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018460  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1246\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019511\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.226418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1247\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020965  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019583\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1248\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012225  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021761\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1249\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017659  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025971\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1250\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023795  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025592\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1251\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020771  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226425\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1252\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027570\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1253\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020103\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1254\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024720  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020552\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017169\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1255\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018023  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1256\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014789  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231911\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019414  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228617\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013590  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1259\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012924  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016317\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015526  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270101\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1261\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019337\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1262\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014872  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021878\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231874\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1263\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020144\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1264\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024307  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016524\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1265\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018146\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1266\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018058  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016368\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.232537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1267\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018886  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1268\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021217  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018584\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022323  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019530\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1271\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022280  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017601\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1272\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019796\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1273\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025505  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238114\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1274\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017194  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021038\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025035\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1275\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020703\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018290\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240059\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1276\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017824  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236866\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1277\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1278\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016415  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019894\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252771\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1279\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272696\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033862  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1281\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025952  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023124\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020918\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1282\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018696  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1283\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019788  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1284\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015292  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1285\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016760  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1286\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018990  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021662\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.239292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1287\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023143\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1288\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024407  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029055\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1289\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032708\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025581\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1290\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025130  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.033246\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271193\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1291\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040516  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247647\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1292\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224500\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1293\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036777  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025168\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237462\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1294\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015349  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242375\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1295\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015007  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017252\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029529\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272862\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1296\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016986\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1297\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014760  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023612\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1298\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016088\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1299\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020839\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1300\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017457  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018965\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229247\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1301\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022334  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023734\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1302\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018962\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1303\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016431\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1304\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013300  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1305\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020740\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1306\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014853  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015572\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.243571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1307\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015725  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018883\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243201\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1308\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1309\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017451\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228283\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1310\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015722\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021598\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1311\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020439  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016423\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228320\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1312\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017566  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018497\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016232\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1313\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019453  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019772\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1314\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019185\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015004\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1315\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016801\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1316\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021510  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021299\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018035\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254830\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1318\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019589\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037174\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226580\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1319\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048745  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1320\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027245  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1321\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1322\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1323\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014140  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023236\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018801\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1324\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021141\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1325\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015393  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017683\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1326\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011827  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017391\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016357\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.230655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1327\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1328\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016471\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225917\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1329\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.008245  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241335\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1330\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015930  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225828\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1331\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020281  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016553\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1332\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228649\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1333\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023795  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1334\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224171\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1335\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017462  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246269\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1336\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016151\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1337\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014527  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017023\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015806\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231922\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1338\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019897  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023962\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1339\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024480\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016773  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1341\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022089\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229670\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1342\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237407\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1343\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015657  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037042\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1344\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036289  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253404\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1345\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019656  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017968\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250124\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1346\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018213\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.251032\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1347\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023343  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1348\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017088  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017200\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233403\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1349\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020198  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016726\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1350\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019695\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1351\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016220  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282429\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1352\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023725  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1353\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015663  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018031\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235140\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1354\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233191\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018742  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020952\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1356\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1357\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029551  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247886\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1358\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019995  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019291\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1359\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020046  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019047\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229789\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1360\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1361\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015777  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048932\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298521\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1362\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053749  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.035966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1363\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026201  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1364\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030664\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1365\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018883  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019136\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1366\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024719  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018509\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016267\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.239911\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1367\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.010709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1368\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018295  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017424\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1369\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016315  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1370\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011435  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015182\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015052\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1371\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018361  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015436\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015251\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1372\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011540  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015586\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249239\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1373\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016240  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020838\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1374\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017353  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1375\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222672\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1376\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014715  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.025768\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034986\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1377\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028342  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018705\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1378\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026605  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020933\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221153\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1379\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018342  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024349\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231777\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1380\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015740\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1381\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.009065  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018779\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1382\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016422\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243751\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1383\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019349  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1384\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044404  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020204\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245976\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1385\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017052  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017074\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1386\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019330  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017402\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017264\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.244145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1387\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1388\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018702\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1389\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232239\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1390\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016234  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235602\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1391\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.009642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023730\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1392\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1393\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020947  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301367\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1394\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018101\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245327\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1395\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020787  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1396\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018962  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1397\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1398\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017597  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015722\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1399\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012159  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018240\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253845\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1400\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017887  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1401\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016406  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1402\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1403\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019991  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016456\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1404\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022808  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274513\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1405\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023743  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018791\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253826\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1406\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018520  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022164\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017220\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.218809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1407\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011639  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019997\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1408\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020757\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015511\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226181\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1409\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012006  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016369\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1410\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016751\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1411\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014340  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1412\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016155\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1413\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023508  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268203\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1414\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1415\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020520  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023662\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233304\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1416\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043659  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017483\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1417\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025150  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016261\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1418\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.009081  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017830\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1419\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022414  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017539\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016613\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229936\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1420\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014759  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017695\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1421\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1422\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014269  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017212\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023961\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1423\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031276  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018708\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1424\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.029641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296012\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1425\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028129  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.024889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262988\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1426\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029597  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022356\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.223837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1427\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017917  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019749\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247970\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1428\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014816\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1429\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016820  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015127\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233700\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1430\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012473  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015169\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1431\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011638  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016844\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218364\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1432\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021711  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017000\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236795\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1433\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013031  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025221\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1434\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025433  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1435\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015374  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016316\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015411\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243530\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1436\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021243  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1437\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013036  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239249\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1438\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019233  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020442\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1439\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018840  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017730\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1440\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017245  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1441\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034221  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.026680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031052\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1442\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.030241\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1443\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023322\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230133\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1444\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019476  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020343\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1445\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024554\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1446\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021460\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022074\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.234265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1447\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029047  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018486\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1448\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012540  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1449\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020980  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.014905\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1450\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018255  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015234\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1451\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019234  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1452\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015026\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.013782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1453\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015877  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1454\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017027  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1455\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.023361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235380\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1456\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017536  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016890\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1457\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018405  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019315\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1458\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025063  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016586\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1459\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011871  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020452\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1460\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023286  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018275\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018840\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1461\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013174  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015293\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1462\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.015534  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244926\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1463\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016123\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014172\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233160\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1464\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.010198  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014141\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1465\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011515  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015907\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014398\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1466\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017456\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015336\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.225783\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1467\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014804  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035088\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1468\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032509\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230594\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1469\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.032451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220097\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1470\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017938  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015788\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1471\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016139\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1472\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014095  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1473\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.010910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015439\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1474\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012733  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017385\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017261\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1475\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019929  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016550\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1476\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.011805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227722\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038172\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1478\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036991  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.021258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1479\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019785  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015638\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1480\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012439  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1481\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017239\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1482\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028537  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.028278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020776\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1483\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014676  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020323\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1484\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014467  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252461\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1485\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025706  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027793\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027437\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1486\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030292  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020368\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.014868\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.230334\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1487\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016719  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025017\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1488\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.027525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1489\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.012869  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253632\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1490\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021572\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1491\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018775  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024913\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1492\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025518  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.022013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1493\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014032  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020526\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1494\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027614  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.017759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1495\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.015941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1496\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.013104  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1497\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.014997  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.016061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1498\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016484  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.018103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243221\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1499\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.019782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237367\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1500\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024296  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.020971\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238924\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# a useful function to present things clearer\n",
    "def seperator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "# define parameters\n",
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50\n",
    "batch_size = 64\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp.to(device), trn_amplp.unsqueeze(-1).to(device))\n",
    "val_dataset = amp_dataset(val_datfp.to(device), val_amplp.unsqueeze(-1).to(device))\n",
    "tst_dataset = amp_dataset(tst_datfp.to(device), tst_amplp.unsqueeze(-1).to(device))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)\n",
    "epochs = 1500\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "hdn_dim = 50\n",
    "model = amp_net(hdn_dim=hdn_dim).to(device)\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "seperator()\n",
    "print(\"model architecture \")\n",
    "seperator()\n",
    "print(model)\n",
    "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses_live = []\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    seperator()\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    seperator()\n",
    "    trn_loss_live = train_epoch(trn_dataloader, model, loss_fn, optimizer)\n",
    "    trn_losses_live.append(trn_loss_live)\n",
    "    seperator()\n",
    "    trn_loss = trn_pass(trn_dataloader, model, loss_fn)\n",
    "    trn_losses.append(trn_loss)\n",
    "    seperator()\n",
    "    val_loss = val_pass(val_dataloader, model, loss_fn)\n",
    "    val_losses.append(val_loss)\n",
    "    seperator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the train and validation losses as a function of the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "c3 = 'tab:orange'\n",
    "\n",
    "axs.plot(trn_losses, label=\"train loss\", color=c1)\n",
    "axs.plot(val_losses, label=\"val   loss\", color=c2)\n",
    "axs.plot(trn_losses_live, label=\"live train loss\", color=c3)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"epoch\")\n",
    "axs.set_ylabel(\"MSE\")\n",
    "\n",
    "axs.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good example of a network over-training.  At some point, the loss evaluated on the training data continues to improve, while the loss on the validation data converges to a lower limit.  In practice, it often happens that the validation loss actually gets worse when the network over-trains, so the problem can be worse than shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a mechanism to reduce over-fitting effects in neural network optimisation.  It was proposed in this paper (I think):\n",
    "\n",
    "------------------\n",
    "\n",
    "**Improving neural networks by preventing co-adaptation of feature detectors**\n",
    "\n",
    "https://arxiv.org/abs/1207.0580\n",
    "\n",
    "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition. \n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is implemented as a layer in pytorch.\n",
    "\n",
    "The description given in the pytorch documentation is:\n",
    "\n",
    "------------------\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors .\n",
    "\n",
    "Furthermore, the outputs are scaled by a factor of $\\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function.\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's build the same model, but with dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class amp_net_dr(nn.Module):\n",
    "    \n",
    "    # default hdn_dim is 30, but can be changed upon initialisation\n",
    "    def __init__(self, hdn_dim=30):\n",
    "        \n",
    "        super(amp_net_dr, self).__init__()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ipt_dim, hdn_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, opt_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When re-training the model we need to be careful.  When we evaluate the network on data to monitor performance, we don't want dropout to affect the results.  To do this we need to temporarily switch the model from `train` mode to `eval` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture\n",
      "-----------------------------------------------\n",
      "amp_net_dr(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): Dropout(p=0.5, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 8751 trainable parameters\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 209.000443  [    0/ 1000]\n",
      "avg train loss per batch in training: 213.992787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 212.624933\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 214.612143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 203.763153  [    0/ 1000]\n",
      "avg train loss per batch in training: 207.929852\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 205.221765\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 206.914349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 200.989655  [    0/ 1000]\n",
      "avg train loss per batch in training: 192.801444\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 179.301460\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 180.175075\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 173.299561  [    0/ 1000]\n",
      "avg train loss per batch in training: 146.187376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 102.463034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 101.644367\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 94.242050  [    0/ 1000]\n",
      "avg train loss per batch in training: 69.615972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 33.534224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 36.675561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 70.508163  [    0/ 1000]\n",
      "avg train loss per batch in training: 70.425244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 40.813139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 41.034150\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 66.790070  [    0/ 1000]\n",
      "avg train loss per batch in training: 61.369958\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 44.186562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 43.782814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 57.561081  [    0/ 1000]\n",
      "avg train loss per batch in training: 58.869600\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 33.578024\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 34.208899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 71.372322  [    0/ 1000]\n",
      "avg train loss per batch in training: 55.306138\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 34.034707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 34.545411\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 47.511383  [    0/ 1000]\n",
      "avg train loss per batch in training: 55.443538\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 31.976944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 32.572798\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 67.353806  [    0/ 1000]\n",
      "avg train loss per batch in training: 54.060076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 32.076910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 32.491959\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 70.601501  [    0/ 1000]\n",
      "avg train loss per batch in training: 52.872722\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 31.179111\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 31.260605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 43.796516  [    0/ 1000]\n",
      "avg train loss per batch in training: 46.650739\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 27.359246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 27.647257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 42.946136  [    0/ 1000]\n",
      "avg train loss per batch in training: 45.270740\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 25.138984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 25.493963\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 69.435249  [    0/ 1000]\n",
      "avg train loss per batch in training: 48.459229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 26.565231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 26.677098\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 42.357140  [    0/ 1000]\n",
      "avg train loss per batch in training: 40.386482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 21.308452\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 21.659103\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 54.088646  [    0/ 1000]\n",
      "avg train loss per batch in training: 42.891018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 22.728226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 22.870574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 30.927414  [    0/ 1000]\n",
      "avg train loss per batch in training: 37.208981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 16.243721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 16.770932\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 30.661804  [    0/ 1000]\n",
      "avg train loss per batch in training: 33.209475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 17.168506\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 17.394488\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 31.792543  [    0/ 1000]\n",
      "avg train loss per batch in training: 35.684562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 17.562100\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 17.662065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 30.899265  [    0/ 1000]\n",
      "avg train loss per batch in training: 31.558309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 13.720727\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 13.988484\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 40.615669  [    0/ 1000]\n",
      "avg train loss per batch in training: 32.390265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 12.583323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 12.814031\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 26.944841  [    0/ 1000]\n",
      "avg train loss per batch in training: 31.035991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 15.102461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 15.115065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 24.682980  [    0/ 1000]\n",
      "avg train loss per batch in training: 29.877638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 10.571037\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 10.645316\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 26.901691  [    0/ 1000]\n",
      "avg train loss per batch in training: 29.939013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 11.914284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 11.892975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 30.169275  [    0/ 1000]\n",
      "avg train loss per batch in training: 25.916867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 8.357020\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 8.411103\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 25.815815  [    0/ 1000]\n",
      "avg train loss per batch in training: 25.441467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 7.789698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 7.867210\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 29.252010  [    0/ 1000]\n",
      "avg train loss per batch in training: 23.369872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.642427\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 6.671295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 22.872154  [    0/ 1000]\n",
      "avg train loss per batch in training: 22.234759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.017672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 6.089663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 28.613501  [    0/ 1000]\n",
      "avg train loss per batch in training: 20.844021\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 7.579177\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 7.563905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.103237  [    0/ 1000]\n",
      "avg train loss per batch in training: 22.115157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.757551\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 6.732030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 17.293852  [    0/ 1000]\n",
      "avg train loss per batch in training: 20.363587\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.155997\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 6.184864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 24.094330  [    0/ 1000]\n",
      "avg train loss per batch in training: 19.783126\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.826764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.861870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 21.602745  [    0/ 1000]\n",
      "avg train loss per batch in training: 19.409147\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 5.833413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.821273\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 20.957993  [    0/ 1000]\n",
      "avg train loss per batch in training: 20.259085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 5.509061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.493475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 17.281776  [    0/ 1000]\n",
      "avg train loss per batch in training: 19.233500\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.335015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.344085\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 23.081125  [    0/ 1000]\n",
      "avg train loss per batch in training: 19.049677\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 5.632699\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.649560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 19.243210  [    0/ 1000]\n",
      "avg train loss per batch in training: 17.985408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.984535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.999348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.887428  [    0/ 1000]\n",
      "avg train loss per batch in training: 19.374136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.238062\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 4.261180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 17.831154  [    0/ 1000]\n",
      "avg train loss per batch in training: 18.418710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.967148\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.980464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.423285  [    0/ 1000]\n",
      "avg train loss per batch in training: 19.071350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.873137\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.887721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.820133  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.244041\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.165156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.188174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.659328  [    0/ 1000]\n",
      "avg train loss per batch in training: 18.113874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.445640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.455001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 19.750599  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.804318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.215858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.235635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.053776  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.571045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 5.037839\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.088813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.849609  [    0/ 1000]\n",
      "avg train loss per batch in training: 17.480339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.022440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.080447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.074211  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.779306\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.674914\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.757658\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.145754  [    0/ 1000]\n",
      "avg train loss per batch in training: 15.580413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.845763\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.917417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 18.322336  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.982998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.900538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.961247\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.787243  [    0/ 1000]\n",
      "avg train loss per batch in training: 15.605342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.011922\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.153120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 51\n",
      "-----------------------------------------------\n",
      "current batch loss: 20.023926  [    0/ 1000]\n",
      "avg train loss per batch in training: 15.690318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.459316\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.524107\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 52\n",
      "-----------------------------------------------\n",
      "current batch loss: 13.293732  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.248113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.980799\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.093172\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 53\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.931671  [    0/ 1000]\n",
      "avg train loss per batch in training: 15.095061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.975296\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.096371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 54\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.278704  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.513435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.243937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.387816\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 55\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.942644  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.607541\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.255387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.381093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 56\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.569502  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.330855\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.464257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.585998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 57\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.433655  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.681145\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.932056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.063514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 58\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.995806  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.132681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.877089\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.018423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 59\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.740679  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.704327\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.359278\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 2.461545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 60\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.866863  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.295191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.325634\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.423669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 61\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.586535  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.549507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.663612\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.812615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 62\n",
      "-----------------------------------------------\n",
      "current batch loss: 13.735500  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.623718\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.374669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.476143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 63\n",
      "-----------------------------------------------\n",
      "current batch loss: 18.578747  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.078166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.492372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.595418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 64\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.188704  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.174367\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.820181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.952561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 65\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.399161  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.752151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.671109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.748431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 66\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.556025  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.954630\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.838494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.971497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 67\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.825678  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.006744\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.606490\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.738983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 68\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.366322  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.837029\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.653212\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.778928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 69\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.576196  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.232876\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.985809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.097144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 70\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.725136  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.836720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.785775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.945234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 71\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.215016  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.928133\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.737070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.906217\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 72\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.593893  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.483837\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.629018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.728758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 73\n",
      "-----------------------------------------------\n",
      "current batch loss: 15.986898  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.659766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.907413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.079410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 74\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.923165  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.948611\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.076765\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.226582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 75\n",
      "-----------------------------------------------\n",
      "current batch loss: 13.224870  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.229277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.300056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.466306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 76\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.262712  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.574661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.372238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.481642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 77\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.668385  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.385903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.039216\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.163999\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 78\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.099776  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.941999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.399823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.560933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 79\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.233576  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.615172\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.440347\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 1.546720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 80\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.193313  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.732638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.511426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.620665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 81\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.767344  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.323817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.466842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.567752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 82\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.570177  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.382301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.130936\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.267667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 83\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.442001  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.339978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.635966\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.742101\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 84\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.978436  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.441333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.607261\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.724651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 85\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.407690  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.169011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.002745\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.134728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 86\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.090058  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.872419\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.073251\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.215527\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 87\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.946543  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.762507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.813723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.949270\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 88\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.619862  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.394200\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.426866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.583593\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 89\n",
      "-----------------------------------------------\n",
      "current batch loss: 13.563959  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.439646\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.271727\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.376131\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 90\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.032322  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.956636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.745133\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.858016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 91\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.159411  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.951328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.795350\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.911280\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 92\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.855355  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.121794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.746417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.837454\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 93\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.210035  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.285945\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.470459\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.571340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 94\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.638214  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.761319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.636346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.736322\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 95\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.483341  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.978212\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.388056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.469581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 96\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.590837  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.740250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.415219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.502217\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 97\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.518253  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.242941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.836859\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.958451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 98\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.392982  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.847189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.650556\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.766540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 99\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.206524  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.349950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.434650\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 1.532087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 100\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.752535  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.815687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.531273\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.639000\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 101\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.831873  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.241180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.296786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.383042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 102\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.973570  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.956060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.259153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.352475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 103\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.357237  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.639930\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.334332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.445391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 104\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.881382  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.770447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.242640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.342079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 105\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.669563  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.323262\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.529925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.643992\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 106\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.657989  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.781232\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.646974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.770980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 107\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.555100  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.954684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.254725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.359461\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 108\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.552249  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.009058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.526688\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.636983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 109\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.917685  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.265194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.751897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.879382\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 110\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.244738  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.750838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.029260\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.176018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 111\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.660906  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.705778\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.093122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.180391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 112\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.489885  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.414840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.133121\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.220732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 113\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.938931  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.127619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.616564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.737702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 114\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.007002  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.208659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.313109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.420791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 115\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.569562  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.943384\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.201117\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.298466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 116\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.645781  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.679917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.583783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.716635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 117\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.640393  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.814793\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.231292\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.341951\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 118\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.879306  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.115187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.442900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.561420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 119\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.822462  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.944179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.206588\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 1.302571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 120\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.720769  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.601674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.357333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.470692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 121\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.515251  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.848045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.266083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.374662\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 122\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.512430  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.777710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.194937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.305075\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 123\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.519452  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.878465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.511403\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.628597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 124\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.107523  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.325862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.284974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.398452\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 125\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.110688  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.273614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.573889\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.684431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 126\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.423746  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.904881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.186516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.281450\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 127\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.714736  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.163378\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.690812\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.809957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 128\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.477500  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.436392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.420584\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.537376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 129\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.444332  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.542295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.094879\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.185489\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 130\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.437511  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.747243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.168439\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.271420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 131\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.806852  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.689194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.353925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.470624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 132\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.866758  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.381494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.218508\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.319794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 133\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.428165  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.465333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.352614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.484565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 134\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.491306  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.478408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.345191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.471260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 135\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.797777  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.056614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.573468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.705060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 136\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.931537  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.462516\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.216932\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.331317\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 137\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.648531  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.904361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.100125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.202958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 138\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.253807  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.725131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.505882\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.647610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 139\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.237669  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.233259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.433334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.564152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 140\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.537472  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.303365\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.109392\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 1.206034\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 141\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.423684  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.280481\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.226698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.329693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 142\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.204494  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.145120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.334953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.451394\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 143\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.846103  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.829147\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.412623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.528685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 144\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.325071  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.684939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.172476\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.277230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 145\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.637350  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.199362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.204655\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.292011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 146\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.214624  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.601308\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.452838\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.561895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 147\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.310841  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.542206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.259632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.349909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 148\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.326184  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.333416\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.213632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.317365\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 149\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.573535  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.154665\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.978213\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.058205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 150\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.939390  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.903939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.188747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.289975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 151\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.884512  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.090560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.999989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.087084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 152\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.024242  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.589723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.157771\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.247272\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 153\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.314487  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.687422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.303720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.417388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 154\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.468624  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.751248\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.176381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.277262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 155\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.374207  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.416846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.980114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.048629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 156\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.412387  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.144496\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.487680\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.600752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 157\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.784755  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.468788\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.106408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.201635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 158\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.485064  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.729934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.894784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.969930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 159\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.059572  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.478138\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.217971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.331460\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 160\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.431616  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.094983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.415424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.532814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 161\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.276195  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.224340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.872527\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.946800\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 162\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.846202  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.817164\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.252272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.376868\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 163\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.130552  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.198132\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.366576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.476257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 164\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.134881  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.357111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.288989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.416092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 165\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.735958  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.675159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.387363\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.511406\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 166\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.504028  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.657477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.943360\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.036185\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 167\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.283396  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.604603\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.089098\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.189012\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 168\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.895168  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.217552\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.295062\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.413066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 169\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.207561  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.325485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.166472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.278192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 170\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.120775  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.520467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.966070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.058078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 171\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.563410  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.811121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.117179\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.227784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 172\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.699910  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.045737\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.181228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.279790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 173\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.117787  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.940257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.004286\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.098810\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 174\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.433372  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.021633\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.205843\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.312318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 175\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.047021  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.217400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.906772\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.990602\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 176\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.647611  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.199837\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.150098\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.259383\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 177\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.244299  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.849454\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.156127\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.256737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 178\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.442479  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.058389\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.035895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.132937\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 179\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.683494  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.139177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.228453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.330560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 180\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.576639  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.594112\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.196637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.298971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 181\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.656681  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.059440\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.954588\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.031526\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 182\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.949483  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.739394\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.134828\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 1.232165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 183\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.590795  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.709616\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.154944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.252289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 184\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.262102  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.368776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.319570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.433622\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 185\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.492126  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.356823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.048515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.138055\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 186\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.772453  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.683503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.201351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.294831\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 187\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.997321  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.226101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.214187\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.324427\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 188\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.524694  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.626881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.887970\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.947885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 189\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.824240  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.286160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.259805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.370055\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 190\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.722216  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.891295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.222817\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.330734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 191\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.463771  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.935307\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.032189\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.120215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 192\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.403557  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.802398\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.203027\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.302454\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 193\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.803584  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.307887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.331871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.431899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 194\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.269033  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.464635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.837104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.900793\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 195\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.281754  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.983456\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.902236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.966964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 196\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.998453  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.750550\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.165637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.263292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 197\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.961718  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.768435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.037046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.118276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 198\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.044243  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.020287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.853795\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.908276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 199\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.134239  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.176960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.953875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.030167\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 200\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.267218  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.972581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.934468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.003573\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 201\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.937307  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.311859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.153474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.250536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 202\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.979452  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.217204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.112709\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.207144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 203\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.567645  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.780132\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.830249\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.895381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 204\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.733505  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.368489\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.233238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.323742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 205\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.939410  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.711230\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.028876\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.112246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 206\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.115219  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.108542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.886988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.947725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 207\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.901820  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.152204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.151496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.232493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 208\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.547645  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.003033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.345395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.458868\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 209\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.191694  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.224201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.168283\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.267669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 210\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.605577  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.890856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.788553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.849401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 211\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.071260  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.893888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.161764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.252693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 212\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.045862  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.003831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.084263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.162626\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 213\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.813403  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.179434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.008877\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.085680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 214\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.759043  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.567642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.242226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.343998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 215\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.875933  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.717244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.112980\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.202957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 216\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.463036  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.658257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.919245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.985061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 217\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.765195  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.208236\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.228254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.330629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 218\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.364742  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.772992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.805402\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.861705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 219\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.171487  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.176755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.042333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.118233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 220\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.194659  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.415957\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.129574\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.212051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 221\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.987469  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.663292\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.994821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.076011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 222\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.329965  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.737025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.907747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.983019\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 223\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.530920  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.518697\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.937110\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.010944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 224\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.144386  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.492524\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.111345\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 1.209873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 225\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.459906  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.761903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.976607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.049467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 226\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.363456  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.310665\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.937158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.013781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 227\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.043701  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.374226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.195229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.295315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 228\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.058736  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.904245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.979974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.062015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 229\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.151069  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.422741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.986046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.061759\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 230\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.055386  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.560647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.821965\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.878160\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 231\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.151049  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.921874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.996788\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.070367\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 232\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.460245  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.675072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.299218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.397432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 233\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.896898  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.217683\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.936499\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.008085\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 234\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.633430  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.309666\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.174950\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.266382\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 235\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.452384  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.391084\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.208518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.316913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 236\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.299970  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.175264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.825038\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.897271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 237\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.770353  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.562755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.141649\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.238806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 238\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.645333  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.455938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.009287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.092787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 239\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.269505  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.418906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.820895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.892686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 240\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.114167  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.288162\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.733181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.795907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 241\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.678832  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.031173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.164426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.271821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 242\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.649863  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.940220\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.863119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.945791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 243\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.969730  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.862397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.747653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.801663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 244\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.281150  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.490847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.122149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.207178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 245\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.066265  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.057472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.840562\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.907266\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 246\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.880892  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.352581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.964885\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.036686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 247\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.837865  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.346849\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.918786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.985454\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 248\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.141824  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.259999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.061059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.136742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 249\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.816444  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.213607\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.984500\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.054463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 250\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.737272  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.159320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.869122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.929585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 251\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.949086  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.177458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.808467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.864177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 252\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.475189  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.802280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.378119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.471988\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 253\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.782914  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.923864\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.834131\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.902746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 254\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.796295  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.246564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.898303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.969605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 255\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.889211  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.766535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.806857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.870528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 256\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.053423  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.596084\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.970201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.047883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 257\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.960052  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.472767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.002071\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.072125\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 258\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.313824  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.844005\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.918207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.987213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 259\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.776481  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.084443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.152271\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.232717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 260\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.636601  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.295482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.922203\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.993757\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 261\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.096240  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.744319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.801260\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.866293\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 262\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.398749  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.947220\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.774641\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.828629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 263\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.543300  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.129342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.184111\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.268371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 264\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.715287  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.861575\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.787731\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.841939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 265\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.518098  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.503656\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.785421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.848009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 266\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.583898  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.753655\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.803465\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.863545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 267\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.560836  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.885537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.974482\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.043280\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 268\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.793053  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.185597\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.028926\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.101750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 269\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.087334  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.897431\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.767785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.814857\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 270\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.021793  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.638729\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.846684\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.902762\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 271\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.995872  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.697250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.945487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.011284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 272\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.205391  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.082579\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.991809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.057490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 273\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.279540  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.286667\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.882912\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.941875\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 274\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.506852  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.260366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.877578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.936894\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 275\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.192605  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.525881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.785547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.829447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 276\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.805222  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.647183\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.873200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.934352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 277\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.945995  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.633862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.014199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.067160\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 278\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.525634  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.704158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.691323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.728758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 279\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.390646  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.131421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.718860\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.759157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 280\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.217155  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.053761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.877560\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.928818\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 281\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.437142  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.609328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.929053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.985944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 282\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.271789  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.678580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.827944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.882274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 283\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.173766  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.593467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.989026\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.058376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 284\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.449701  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.779630\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.902412\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.967033\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 285\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.352976  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.327983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.696096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.747179\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 286\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.999670  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.666577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.956528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.021465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 287\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.173407  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.501992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.768732\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.818284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 288\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.074807  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.770893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.902614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.949844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 289\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.188336  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.267419\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.996733\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.049617\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 290\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.585631  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.875460\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.742635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.783907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 291\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.023521  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.327662\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.912034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.961029\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 292\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.878349  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.757209\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.736333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.776603\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 293\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.851301  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.228308\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.766960\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.818627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 294\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.502489  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.633456\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.137350\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.200099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 295\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.105957  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.281523\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.901676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.961229\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 296\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.088876  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.252329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.805079\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.857221\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 297\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.009158  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.380272\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.749082\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.792048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 298\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.546477  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.463355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.773660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.826612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 299\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.397310  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.385062\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.736286\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.781935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 300\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.082576  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.918102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.825234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.875174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 301\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.469553  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.136154\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.583902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.614621\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 302\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.176964  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.492702\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.989704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.058769\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 303\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.352589  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.346022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.728129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.773099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 304\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.139872  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.852274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.714982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.762770\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 305\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.331849  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.254592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.990510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.061352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 306\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.119258  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.880463\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.864125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.912750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 307\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.409461  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.248760\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.760828\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.809033\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 308\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.561385  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.816748\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.823692\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.875871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 309\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.741837  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.265307\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.773416\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.827194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 310\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.358337  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.346836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.685507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.727940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 311\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.190744  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.552865\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.790871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.848579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 312\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.565883  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.633654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.695969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.740145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 313\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.949692  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.929593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.864276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.922303\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 314\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.020078  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.196253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.793645\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.844718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 315\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.628448  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.686553\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.681137\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.730143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 316\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.383575  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.471941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.729552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.782401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 317\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.445081  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.369036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.647678\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.690739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 318\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.936052  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.973139\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.661083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.712799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 319\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.904396  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.110435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.903083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.963656\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 320\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.290277  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.231591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.834993\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.895075\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 321\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.573400  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.394861\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.918893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.979356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 322\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.605846  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.315216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.767627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.821074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 323\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.838688  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.022990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.863973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.927692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 324\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.738654  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.867274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.869398\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.937182\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 325\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.001508  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.136618\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.681212\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.726758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 326\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.538965  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.566356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.557358\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.591346\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 327\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.864742  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.300106\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.712390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.757313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 328\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.547347  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.195288\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.899463\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.957393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 329\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.963416  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.135670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.746554\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.789049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 330\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.898437  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.111423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.875743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.932691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 331\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.753538  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.493664\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.684993\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.729644\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 332\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.034060  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.781717\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.637184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.670067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 333\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.785590  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.616039\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.736049\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.784136\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 334\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.736344  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.977415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.852810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.907142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 335\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.091418  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.039607\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.619051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.660928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 336\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.597501  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.675372\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.937827\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.996298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 337\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.857146  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.705072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.539125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.569022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 338\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.883026  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.952986\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.699807\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.743243\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 339\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.677865  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.432226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.687431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.730271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 340\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.738389  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.938479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.962335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.019936\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 341\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.014719  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.623212\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.807160\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.861250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 342\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.209163  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.792767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.579541\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.618067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 343\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.681681  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.091750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.083518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.144332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 344\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.776984  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.865264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.765181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.813717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 345\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.726995  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.906266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.674076\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.713355\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 346\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.035022  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.235017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.745038\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.795402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 347\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.338770  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.115248\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.557214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.591596\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 348\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.779346  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.100805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.809651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.858991\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 349\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.657344  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.836977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.838512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.886974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 350\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.252898  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.508267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.777391\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.814375\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 351\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.483427  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.682452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.586735\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.607592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 352\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.233427  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.668181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.803052\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.843937\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 353\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.243340  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.275582\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.883660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.943177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 354\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.296228  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.613030\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.734733\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.787472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 355\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.629583  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.420644\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.749012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.797348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 356\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.675530  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.364996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.587932\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.620763\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 357\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.861861  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.916320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.628055\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.668922\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 358\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.364924  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.698581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.718907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.758386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 359\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.940060  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.898554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.633471\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.667534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 360\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.356610  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.643875\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.602125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.640726\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 361\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.769732  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.254938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.881604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.940010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 362\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.360641  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.846543\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.583733\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.612912\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 363\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.908491  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.518186\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.661658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.705855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 364\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.561623  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.272940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.561958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.605250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 365\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.721982  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.309265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.546113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.581117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 366\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.668905  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.673261\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.532162\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.563308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 367\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.773717  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.756333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.643759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.680134\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 368\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.710697  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.315491\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.664840\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.699374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 369\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.665417  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.320778\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.615153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652077\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 370\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.947145  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.769659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.717645\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.765544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 371\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.767799  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.231345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.704042\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.752597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 372\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.512459  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.406644\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.604270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.647345\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 373\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.566686  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.601929\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.734816\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.781172\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 374\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.293242  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.943605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.768091\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.813692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 375\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.989771  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.402295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.582241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.615321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 376\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.245075  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.457843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.675577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.721116\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 377\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.381758  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.867177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.667798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.712789\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 378\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.570900  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.970235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.891152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.952837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 379\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.726681  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.764663\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.580176\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.621901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 380\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.401727  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.482990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.673477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.721475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 381\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.013764  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.660802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.750232\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.805923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 382\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.518661  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.348786\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.610147\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.650361\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 383\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.932555  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.200896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.848403\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.896387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 384\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.036983  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.463483\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.634426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.676290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 385\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.559443  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.811150\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.622743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.665471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 386\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.335901  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.515624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.547637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.583032\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 387\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.579323  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.962177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.681467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.728154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 388\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.281501  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.172484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.469573\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.501968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 389\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.857908  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.325479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.678398\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.728275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 390\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.685088  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.518020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.795181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.851105\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 391\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.912336  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.276429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.545487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.588520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 392\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.023385  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.155215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.798934\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.854378\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 393\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.082448  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.212723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.675079\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.717775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 394\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.960745  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.266345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.735831\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.786058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 395\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.691506  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.302099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.521981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.554116\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 396\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.679996  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.142633\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.794022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.847451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 397\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.898026  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.193741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.695200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.740256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 398\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.714699  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.150878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.562474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.599000\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 399\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.561793  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.021173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.586619\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 400\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.375675  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.010434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.558615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.595709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 401\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.115643  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.166993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.568667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.610563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 402\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.271774  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.238824\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.701300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.752099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 403\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.268395  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.085216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632939\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.678517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 404\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.653605  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.064528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.680255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.731360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 405\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.234074  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.865835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.559977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.597509\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 406\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.607742  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.344705\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.717070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.762479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 407\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.898622  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.438198\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.648342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.696545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 408\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.706370  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.845189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.726134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.778182\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 409\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.086361  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.770780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.479574\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.515541\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 410\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.673534  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.996451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.485824\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.525710\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 411\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.546129  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.835366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.500661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.540287\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 412\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.847412  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.478362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.676056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.729360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 413\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.990405  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.076049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.440429\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.480366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 414\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.908317  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.423534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.790887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.851999\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 415\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.727215  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.547023\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.628291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.673215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 416\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.126944  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.335138\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.567809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.610190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 417\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.035698  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.207803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.619568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.662257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 418\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.736171  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.944733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.555468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.602493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 419\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.378397  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.519954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.537720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.577358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 420\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.151392  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.213249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.548767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.587284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 421\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.481263  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.449473\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.439426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.468469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 422\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.189477  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.522602\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.638936\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.685815\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 423\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.816864  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.032515\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.538216\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.575214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 424\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.811964  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.061258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.505029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.544699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 425\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.469708  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.844817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.654204\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.702373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 426\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.737774  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.131566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.573153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.610987\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 427\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.217825  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.988204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.686109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.730420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 428\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.739200  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.967115\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.658007\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.700566\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 429\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.649684  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.100612\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.714773\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.760377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 430\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.549163  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.881055\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.436222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.465587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 431\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.131762  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.895973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.583491\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.627772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 432\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.407405  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.269681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.622496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.664630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 433\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.068682  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.397513\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.603825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.650965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 434\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.894069  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.781923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.453017\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.488782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 435\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.413272  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.926647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.569736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.606624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 436\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.630381  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.557084\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.580297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.621955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 437\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.498704  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.148340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.461947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.488337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 438\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.155032  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.049417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.519590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.552283\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 439\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.536456  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.071996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.570134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.609516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 440\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.109266  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.734541\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.601837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.643591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 441\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.718923  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.111333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.473712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.502758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 442\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.833958  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.734668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.476502\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.509641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 443\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.637175  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.981631\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.502348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.542323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 444\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.647817  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.270298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.583753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.632785\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 445\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.316440  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.997345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.450496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.492811\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 446\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.542558  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.937246\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.472371\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.515506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 447\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.562444  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.030137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.524429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.571833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 448\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.695853  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.108019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.604210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.654152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 449\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.205946  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.433415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.495837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.536732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 450\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.233655  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.407888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.485565\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.525605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 451\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.249501  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.836827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.510596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.546924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 452\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.746733  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.999134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.365196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 453\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.438950  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.063512\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.569495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.617732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 454\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.145825  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.800978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.429069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.465289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 455\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.583637  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.879147\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.543869\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.590208\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 456\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.718687  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.183490\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.531498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.576057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 457\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.589828  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.884890\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.369210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 458\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.050099  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.962003\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.510150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.550834\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 459\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.122519  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.805345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.493119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.536624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 460\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.156865  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.974609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.556392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.605863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 461\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.719170  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.940994\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.567674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.610953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 462\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.278565  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.561844\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.503591\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.546060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 463\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.260684  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.872661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.440255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.479881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 464\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.275527  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.518480\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.488387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.532262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 465\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.657147  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.855202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.461774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.504660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 466\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.639338  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.494900\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.463123\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.506729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 467\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.163983  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.759819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.446641\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.487705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 468\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.753201  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.057842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.585452\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 469\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.928641  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.774284\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.377108\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411392\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 470\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.337890  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.581909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.563407\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.613572\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 471\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.830800  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.876162\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.530882\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.577210\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 472\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.989902  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.808063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.467500\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.512747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 473\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.651739  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.641008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 474\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.682608  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.561288\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.555210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.605790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 475\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.411312  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.919726\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.434443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.480904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 476\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.649961  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.046911\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.411074\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.453233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 477\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.196422  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.772956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 478\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.357956  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.387092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.445989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.491504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 479\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.056684  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.446785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.436610\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.483245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 480\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.384360  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.062671\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.447914\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.495149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 481\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.183582  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.498429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347456\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383210\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 482\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.169229  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.618878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.359261\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 483\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.246759  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.848643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.382113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.422608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 484\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.329133  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.521471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.410612\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.451846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 485\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.122503  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.504034\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.435894\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.477060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 486\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.136152  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.572068\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.487230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.530921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 487\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.992933  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.136791\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.438025\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.481332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 488\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.475268  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.699812\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.583341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 489\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.757715  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.244104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.432689\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.478431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 490\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.962562  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.052472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.466734\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.513943\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 491\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.348189  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.791829\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.440474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.483354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 492\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.484457  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.496338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.380725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.427651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 493\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.896393  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.357975\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.399923\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 494\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.034645  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.905211\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.444019\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.490173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 495\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.814670  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.354208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.421764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.465044\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 496\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.273469  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.610848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.551056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.601992\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 497\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.426430  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.136024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.479402\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.528609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 498\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.203076  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.742909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.576150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.635475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 499\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.766069  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.060262\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.388072\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.436022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 500\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.470559  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.297043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.581270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636573\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 501\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.197226  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.629365\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.444516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.490612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 502\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.638205  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.733176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.366332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 503\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.004462  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.437938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.357896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.402128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 504\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.357491  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.248171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.570328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.626647\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 505\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.829198  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.592776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 506\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.670181  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.727232\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.588267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.640796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 507\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.099332  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.599511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.412596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.458798\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 508\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.649851  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.838691\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.404232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 509\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.607994  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.417519\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.567474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.625768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 510\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.884318  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.406681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 511\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.541297  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.582494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.427917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.477226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 512\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.022309  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.416609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.464313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.515854\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 513\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.526547  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.427933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.418443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.465567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 514\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.760237  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.355934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.476442\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.526338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 515\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.185641  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.535708\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.625907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.681850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 516\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.394136  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.519384\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.406853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 517\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.740609  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.310753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375649\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.418935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 518\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.572195  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.910466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632632\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.687353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 519\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.709743  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.589301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.444421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.488358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 520\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.509191  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.119797\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.486736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.534505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 521\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.003021  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.715874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344414\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.388040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 522\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.578038  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.248551\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389483\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 523\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.856804  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.346499\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.411031\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.457413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 524\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.133042  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.650983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.455240\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.501582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 525\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.368655  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.142513\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.423093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.467307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 526\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.806450  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.367515\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.559073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.612742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 527\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.507503  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.330054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.352566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399830\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 528\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.499863  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.731244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.483632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.533851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 529\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.304598  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.323950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.421605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.471509\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 530\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.395142  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.514969\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.319239\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 531\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.089635  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.640664\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.426696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.475283\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 532\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.294634  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.292364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.438224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.489177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 533\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.998343  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.449592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.320069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 534\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.973233  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.365130\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.489827\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.544842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 535\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.522429  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.216896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.372739\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419031\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 536\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.928867  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.711269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.296774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339617\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 537\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.340283  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.280223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.497731\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.552702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 538\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.046094  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.462935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.473355\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.529457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 539\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.983186  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.467946\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.532784\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.595275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 540\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.909943  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.600789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304939\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 541\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.066278  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.401275\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.335477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380959\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 542\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.658946  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.037215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.398604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446927\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 543\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.289359  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.865084\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.410443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.460441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 544\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.142241  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.576925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.377704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431426\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 545\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.379015  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.611633\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.584797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.647867\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 546\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.724133  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.123260\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299037\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344619\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 547\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.730050  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.557310\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.470660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.523939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 548\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.922403  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.095259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.425655\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.473759\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 549\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.210772  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.554419\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.353579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.403868\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 550\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.371973  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.530064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.398669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.447082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 551\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.164501  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.482020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.460331\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.516333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 552\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.705330  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.428733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.403178\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.456553\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 553\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.238592  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.589966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 554\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.873634  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.369313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.603536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.668643\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 555\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.024762  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.137284\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340317\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 556\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.461105  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.161978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 557\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.023405  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.215305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.489046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.544669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 558\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.563156  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.542716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.345166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 559\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.604269  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.426784\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.417053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.466567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 560\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.538797  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.325529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363870\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.411062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 561\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.533112  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.257039\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391931\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 562\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.220689  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.681268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.429048\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.482062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 563\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.769905  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.317976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.400620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 564\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.818197  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.462657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.421086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.474405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 565\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.132406  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.226266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.511685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.565935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 566\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.944399  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.153267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.510425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.567051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 567\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.926387  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.333315\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.417693\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.462841\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 568\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.704317  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.188232\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.407289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.458813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 569\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.137855  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.267913\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.332744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.381365\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 570\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.181126  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.508434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 571\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.285376  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.494262\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.524755\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.586227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 572\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.404542  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.457286\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.397781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 573\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.389236  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.297158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299602\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 574\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.532241  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.181516\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.451511\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.508628\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 575\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.329222  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.941622\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.399150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.457980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 576\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.404106  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.297557\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.394810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 577\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.816422  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.336696\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336108\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384778\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 578\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.679743  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.332853\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 579\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.011477  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.537267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.383525\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.434392\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 580\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.294377  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.258720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 581\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.767866  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.185289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.394709\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.440797\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 582\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.336685  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.375449\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.395861\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.448265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 583\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.537233  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.179530\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.368510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 584\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.575360  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.168544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.439338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.490665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 585\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.912697  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.147873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.367822\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.413406\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 586\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.369267  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.277153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.425020\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.467563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 587\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.506496  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.100502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.302605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 588\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.999709  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.568693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.410411\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.455473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 589\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.744359  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.100283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 590\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.262424  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.231264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.411462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.463582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 591\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.997676  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.083930\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.487974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.547081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 592\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.362746  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.352623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362121\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.416966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 593\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.993779  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.099840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.403461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.455458\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 594\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.629761  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.260902\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.512019\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.566507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 595\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.823023  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.129130\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.403439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 596\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.257060  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.172194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.413640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.460598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 597\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.360174  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.074926\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 598\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.626236  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.151119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.323102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 599\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.914584  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.044733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.572445\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 600\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.663060  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.439936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.423450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.475850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 601\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.024514  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.093347\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.373637\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 602\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.862837  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.949356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.367801\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.420302\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 603\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.030769  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.765037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.364664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419334\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 604\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.592351  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.144627\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.410549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.467537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 605\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.012664  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.360218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 606\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.035333  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.458338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.415234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.470065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 607\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.855949  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.056569\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.372191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.423752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 608\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.587097  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.135095\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 609\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.620023  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.838691\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.341673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 610\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.318574  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.163552\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.352335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 611\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.289872  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.175807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.404347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.451615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 612\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.043516  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.364948\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352492\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 613\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.102208  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.241065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419334\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 614\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.966696  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.003948\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.392775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.437300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 615\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.694187  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.152209\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.401168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.451725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 616\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.082096  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.003897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.372293\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.415936\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 617\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.105597  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.097371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.463450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.513921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 618\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.866620  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.200819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.428861\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.478423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 619\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.250762  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.110572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.545925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.602109\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 620\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.368131  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.039742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361455\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 621\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.747730  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.107638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.401669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.453809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 622\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.933398  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.025028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.360694\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.410506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 623\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.467629  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.155904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.418642\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.476601\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 624\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.666242  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.959298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311702\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 625\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.255669  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.996350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.416863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 626\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.762732  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.932135\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.408967\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.456747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 627\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.442218  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.127086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.398028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.450179\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 628\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.893585  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.918324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.378371\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.428614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 629\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.981329  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.943494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.408292\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.457178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 630\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.524661  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.867448\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.381484\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431253\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 631\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.640553  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.093446\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.302764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 632\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.696059  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.098795\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347500\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392408\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 633\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.502845  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.605451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.352796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.400399\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 634\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.333108  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.190500\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.421169\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.470708\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 635\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.978921  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.816686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 636\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.547895  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.170589\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.364687\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.409627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 637\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.128914  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.128444\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.359842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.406408\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 638\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.735406  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.775442\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.360823\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 639\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.575148  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.024194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.386202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.436965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 640\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.377358  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.810841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.512034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.569353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 641\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.338014  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.947029\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314373\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 642\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.116345  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.021664\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.322367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.374224\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 643\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.260088  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.908031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.415866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.473644\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 644\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.485595  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.020529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.431152\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.488881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 645\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.967847  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.986285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.478471\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.538018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 646\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.568016  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.831779\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363481\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.414920\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 647\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.711343  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.911584\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.354327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.404315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 648\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.281805  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.897859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.516454\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.573497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 649\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.435257  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.679968\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.378646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.427824\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 650\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.291598  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.089565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.352294\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.405614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 651\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.994824  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.656101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 652\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.806784  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.178933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.508644\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.565429\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 653\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.001495  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.932435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 654\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.331129  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.994789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.332067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.379483\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 655\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.773973  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.947753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.398874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.448341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 656\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.121351  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.755047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340763\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 657\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.859628  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.781486\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.357008\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.401550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 658\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.608255  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.323552\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.416074\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.466106\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 659\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.372705  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.872731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270931\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 660\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.603874  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.113832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.429772\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.483685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 661\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.876112  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.796184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.436683\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.491477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 662\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.136352  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.620042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.244984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 663\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.052400  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.815276\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 664\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.688331  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.975941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.447999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.500635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 665\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.548352  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.856314\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290156\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.334168\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 666\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.478460  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.045888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375017\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 667\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.163082  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.808573\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328009\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.377365\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 668\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.709592  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.922933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.416684\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.473721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 669\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.578668  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.825857\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.487086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.542381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 670\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.154941  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.133311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.426953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.482214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 671\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.834612  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.788551\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.396786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 672\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.511569  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.056990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.342523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 673\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.580327  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.936344\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348709\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.401427\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 674\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.483794  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.950434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 675\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.799698  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.930459\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 676\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.238951  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.041390\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.430808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.486015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 677\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.921217  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.812146\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288082\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 678\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.195829  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.951069\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.408946\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.464780\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 679\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.467455  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.860511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292260\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 680\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.964730  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.710311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387461\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 681\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.486741  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.196635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.381960\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.437736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 682\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.094427  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.668151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.320985\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 683\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.022519  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.811783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.390747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 684\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.723377  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.776922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.368715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.420175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 685\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.134128  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.847683\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.355099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.405848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 686\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.663562  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.724860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.370152\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.420686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 687\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.755551  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.745522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.445546\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.505925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 688\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.574211  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.789193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.402061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.456241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 689\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.616515  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.879839\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.455341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.511208\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 690\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.332608  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.801411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 691\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.289825  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.866709\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293580\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339412\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 692\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.960577  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.673405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.446961\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.497275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 693\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.052944  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.540476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 694\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.690664  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.737352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 695\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.124735  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.653850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291817\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 696\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.041863  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.517070\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.317286\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 697\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.879288  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.971657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.378503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.432362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 698\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.679289  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.873994\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.358406\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.413141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 699\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.684265  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.678746\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.376849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.435837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 700\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.816586  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.798162\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246162\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293765\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 701\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.663467  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.681476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.445870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.509744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 702\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.798858  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.863754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.343378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.395753\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 703\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.383471  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.744233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339942\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 704\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.511816  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.770506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.385694\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.439729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 705\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.020103  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.812303\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.463392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.524906\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 706\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.045659  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.717106\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.405988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.464704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 707\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.498728  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.792834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282558\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.327616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 708\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.963921  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.809713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.376532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.434363\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 709\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.857142  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.506813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335862\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 710\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.754824  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.863757\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.450610\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.510414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 711\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.484871  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.815763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.361963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.414428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 712\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.073596  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.434021\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.323581\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.373226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 713\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.056396  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.759321\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.439550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.496442\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 714\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.896836  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.917778\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.407399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.462081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 715\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.024440  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.870848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347521\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 716\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.955544  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.733752\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340085\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 717\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.256332  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.565288\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.368545\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.418899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 718\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.330121  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.697820\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385770\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 719\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.735984  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.646745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.404295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 720\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.307291  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.744324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301085\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 721\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.488522  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.739227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328699\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 722\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.920556  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.877676\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.360764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.415478\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 723\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.665863  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.602454\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.306394\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 724\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.758569  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.674938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.343715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 725\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.201506  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.504163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.441682\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.502146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 726\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.768976  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.655698\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 727\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.162857  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.578183\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.395127\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 728\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.623642  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.694619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.417379\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.474929\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 729\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.932278  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.609549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.425972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 730\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.282456  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.738641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 731\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.061892  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.670007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 732\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.776491  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.634201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.366260\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.418374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 733\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.996072  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.664484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 734\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.900964  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.805260\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314380\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 735\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.834543  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.569065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.326407\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 736\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.135765  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.560028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.379120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.435781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 737\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.266001  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.517065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347854\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.401520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 738\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.849883  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.506161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 739\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.527539  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.525313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.353975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.409194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 740\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.661011  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.508614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.354219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.410081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 741\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.165264  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.700197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232189\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284041\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 742\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.670006  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.442604\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 743\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.277831  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.394089\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.289335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 744\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.678734  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.755414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.453462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.513490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 745\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.706465  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.491222\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 746\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.070799  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.986029\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.309288\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367602\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 747\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.565153  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.626506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.407745\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.472071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 748\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.985946  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.516439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.309513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 749\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.535095  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.713308\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.354446\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.411462\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 750\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.476874  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.697875\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.324031\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385287\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 751\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.263016  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.311845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 752\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.223538  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.610790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.364156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.421865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 753\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.567954  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.444590\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.403197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.458086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 754\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.195072  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.520475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246405\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 755\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.725605  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.355179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.391906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 756\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.355557  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.600397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375839\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.428295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 757\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.920378  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.835266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.429498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.492675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 758\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.100636  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.591881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 759\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.789872  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.401129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241522\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 760\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.115417  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.281254\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.401366\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.463587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 761\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.236926  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.711418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.357801\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.410248\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 762\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.266280  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.519233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.295322\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345970\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 763\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.395898  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.757670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.395397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 764\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.164847  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.747430\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.321731\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380647\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 765\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.550873  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.391040\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248071\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 766\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.524546  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.716557\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.281981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327162\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 767\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.605206  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.430452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.495920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.564050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 768\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.232636  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.479629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.309834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.365008\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 769\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.710931  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.514766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277836\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 770\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.240019  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.566804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312889\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.374501\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 771\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.247899  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.443652\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270930\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 772\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.327672  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.354949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.369202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.427590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 773\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.140782  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.275301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 774\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.943221  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.479919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 775\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.163895  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.826756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315789\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 776\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.525298  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.470015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356525\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.418589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 777\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.769641  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.567167\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348315\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.409259\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 778\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.780132  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.206621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300748\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356370\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 779\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.979238  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.623302\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344977\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 780\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.451866  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.396973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331914\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 781\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.500849  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.537450\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 782\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.140893  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.345502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291189\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 783\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.874665  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.489806\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.302284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 784\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.150054  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.348491\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.418749\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.485522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 785\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.492367  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.209931\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265922\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 786\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.150573  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.145985\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.369874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.435399\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 787\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.828668  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.441761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336345\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 788\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.665654  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.613623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.370768\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 789\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.268875  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.481682\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389135\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 790\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.898694  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.313395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331921\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 791\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.390934  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.240137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.243690\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.291065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 792\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.468655  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.577162\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.366139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 793\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.120380  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.362755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285691\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 794\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.837877  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.358341\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.374834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.434530\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 795\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.214044  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.644817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286830\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 796\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.632605  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.620588\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 797\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.953866  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.388818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331927\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 798\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.169871  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.286140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 799\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.968302  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.308388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.342054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 800\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.670196  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.628280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 801\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.578413  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.272863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285476\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 802\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.206547  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.362777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.465830\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.532981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 803\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.606745  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.304798\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431795\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 804\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.318040  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.234953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 805\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.755144  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.327116\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.318214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.377835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 806\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.677503  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.203009\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.391516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.456839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 807\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.108173  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.236307\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.403954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.476021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 808\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.656878  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.621893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 809\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.331486  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.363902\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.415235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.477733\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 810\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.837103  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.131981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297545\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353186\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 811\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.526698  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.308897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356884\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411670\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 812\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.130250  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.336774\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.354822\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.410293\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 813\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.956910  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.149468\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.418823\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 814\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.167334  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.134642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 815\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.142084  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.442607\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.320533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.374671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 816\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.322296  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.620573\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279105\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333508\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 817\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.101586  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.392882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.385809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 818\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.296697  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.187908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 819\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.147261  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.284144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.359021\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 820\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.174223  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.425407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310852\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 821\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.964345  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.251177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 822\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.566308  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.219841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316469\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.374401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 823\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.490127  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.337815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.321716\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380008\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 824\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.479708  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.464190\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 825\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.357654  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.432968\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294464\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350836\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 826\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.349697  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.261754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 827\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.357265  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.205405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358327\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 828\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.133928  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.240763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.322548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 829\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.781190  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.268782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.370122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.433020\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 830\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.712512  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.292328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.359461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 831\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.537514  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.326296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 832\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.588849  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.189689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.341109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 833\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.121606  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.440153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.408846\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.467972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 834\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.417392  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.416927\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.381743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.441039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 835\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.568444  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.457946\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312755\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 836\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.617537  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.084357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.376888\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 837\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.423029  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.137272\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375017\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 838\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.307184  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.493002\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389327\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 839\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.322196  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.305414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359112\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 840\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.415416  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.400945\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312446\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 841\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.445918  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.229792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.368494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.427621\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 842\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.264981  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.198463\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.400676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.463974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 843\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.078814  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.334768\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 844\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.976249  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.374546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252162\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 845\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.054851  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.554823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.445236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.511502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 846\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.006037  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.400148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.422876\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.488595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 847\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.070177  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.215745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329248\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.388402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 848\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.501340  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.298678\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 849\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.384803  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.390467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316758\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 850\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.404971  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.587655\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.341894\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.404801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 851\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.167886  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.221170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 852\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.250997  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.088922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.306562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371887\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 853\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.723445  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.316297\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.324814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380677\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 854\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.005073  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.859384\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273151\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.327929\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 855\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.158223  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.158833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.320521\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 856\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.714636  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.113182\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 857\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.307670  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.340906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.342877\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.403975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 858\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.093182  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.096896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 859\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.682648  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.231573\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 860\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.983345  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.150028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.410757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.475338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 861\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.256993  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.222542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234011\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 862\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.850997  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.266594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315980\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 863\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.297854  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.256329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315717\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 864\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.786172  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.878567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.295180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 865\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.531162  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.240757\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337213\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.403007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 866\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.753038  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.948545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.320313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 867\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.997878  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.100052\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291677\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 868\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.173970  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.345871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.384280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.447850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 869\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.677436  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.261581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.444217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.511170\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 870\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.461605  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.227588\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247705\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304677\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 871\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.220351  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.270590\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.323884\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 872\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.958572  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.180872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355056\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 873\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.738479  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.164869\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 874\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.689420  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.140609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330729\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 875\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.315793  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.454085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285013\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.340967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 876\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.787178  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.159501\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.439124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.505665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 877\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.865103  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.074030\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 878\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.706212  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.106223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312371\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 879\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.023578  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.400791\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.358808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.421999\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 880\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.096106  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.054552\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.332794\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389486\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 881\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.932929  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.133202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.390050\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.449420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 882\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.577510  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.113983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.387938\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.453441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 883\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.068475  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.156180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.351465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419208\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 884\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.369976  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.131528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 885\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.010100  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.954797\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.298298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 886\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.680398  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.176768\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.376889\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 887\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.404583  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.070383\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278350\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 888\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.460102  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.071088\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 889\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.413181  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.302444\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329204\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 890\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.344572  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.173443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.358254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.415126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 891\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.834752  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.099991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334851\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 892\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.493086  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.235742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368580\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 893\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.713253  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.051401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 894\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.521073  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.064576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386726\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 895\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.679716  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.919527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 896\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.309381  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.259955\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.335292\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.389267\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 897\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.842518  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.171934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.367390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.425082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 898\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.754843  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.859559\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.365172\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 899\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.426536  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.244865\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.351429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.414115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 900\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.073129  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.851756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339312\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 901\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.962886  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.129754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.350837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 902\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.471686  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.050027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.298814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 903\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.022622  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.108723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.358433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 904\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.611840  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.875256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.364006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.424728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 905\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.512897  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.009750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.397666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.465288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 906\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.727766  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.772503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.379554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 907\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.841505  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.777592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.321864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386888\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 908\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.036642  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.068316\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363401\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.428789\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 909\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.069679  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.069116\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.332708\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 910\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.270493  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.893773\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264863\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 911\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.997219  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.960656\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286193\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 912\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.123644  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.745046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.305795\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363069\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 913\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.381912  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.151699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299080\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 914\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.684049  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.239150\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329688\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 915\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.489146  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.958117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 916\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.666607  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.940549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.380182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.442288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 917\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.049727  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.073155\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339147\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.398775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 918\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.287611  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.791962\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.436113\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 919\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.903673  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.919137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312502\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 920\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.281930  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.063707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 921\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.733342  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.890103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340986\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 922\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.720063  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.005144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 923\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.629472  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.833275\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.373624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 924\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.877354  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.907412\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.382754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 925\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.437501  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.071047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.324958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 926\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.295064  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.439649\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.442706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.515556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 927\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.272009  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.970342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.379898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 928\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.530269  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.803039\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 929\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.088999  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.109480\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280355\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336492\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 930\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.085128  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.031325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.326661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386090\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 931\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.017007  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.732637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408990\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 932\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.591083  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.792476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 933\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.267074  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.977137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371034\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 934\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.735346  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.889607\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343903\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 935\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.720150  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.128723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.353835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.416655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 936\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.015224  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.861584\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339247\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399539\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 937\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.734597  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.095657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326786\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 938\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.079643  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.050235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272204\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.325751\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 939\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.423846  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.046103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.379427\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 940\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.206631  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.958086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 941\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.468405  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.780254\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264480\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 942\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.106484  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.813310\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 943\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.549863  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.783423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 944\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.392172  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.850355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.367569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.432724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 945\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.657717  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.923141\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356509\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.423271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 946\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.956741  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.981298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363481\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.432966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 947\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.107539  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.953872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362346\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 948\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.777275  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.924438\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 949\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.758114  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.875954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.379030\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 950\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.304387  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.900684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 951\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.321340  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.028795\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 952\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.198282  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.064233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345826\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 953\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.127520  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.008079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.355539\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.422535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 954\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.554823  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.007953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.426432\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.503503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 955\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.044978  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.887964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279530\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 956\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.875095  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.966608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.213400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267656\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 957\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.164729  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.782831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 958\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.739415  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.702867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 959\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.467958  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.943333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277697\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.333565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 960\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.601667  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.826103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310322\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 961\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.791793  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.567326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265603\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 962\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.798010  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.851273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 963\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.066130  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.760774\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.379191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 964\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.944266  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.968246\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 965\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.772437  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.465919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280178\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344069\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 966\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.725244  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.847837\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251997\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307897\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 967\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.399554  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.863944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 968\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.878386  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.732115\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.303907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.364832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 969\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.098565  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.874293\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 970\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.290254  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.872836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267427\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 971\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.995291  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.744914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225436\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 972\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.155075  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.775521\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.335681\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 973\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.448681  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.706574\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.296035\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 974\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.506478  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.709414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 975\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.506297  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.885351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348862\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.412198\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 976\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.132484  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.446899\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 977\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.849119  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.763681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.401326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 978\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.647423  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.961614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313879\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 979\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.838499  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.638843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.206372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257044\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 980\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.809270  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.966639\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255053\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.312747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 981\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.591931  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.948402\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.376275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.441413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 982\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.563246  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.714254\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 983\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.520294  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.699357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331048\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 984\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.078178  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.817196\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310239\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.369004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 985\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.867710  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.962888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266876\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325892\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 986\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.516317  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.809963\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.224514\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 987\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.693049  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.913665\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.392236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.462651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 988\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.537773  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.840958\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238852\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 989\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.244660  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.852789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.419676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.488292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 990\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.320174  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.749206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393777\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 991\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.049426  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.735726\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 992\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.156233  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.809982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.374946\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.441517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 993\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.553739  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.750916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.338555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.400921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 994\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.309591  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.502479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232174\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288703\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 995\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.775626  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.911189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286572\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347098\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 996\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.992728  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.591189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.318532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 997\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.382840  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.929643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344195\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.409842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 998\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.504497  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.764120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.306095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.364277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 999\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.624514  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.505781\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.393819\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.460047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1000\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.498910  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.746273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1001\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.844759  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.582934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277724\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.334263\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1002\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.815535  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.600935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.351532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1003\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.679952  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.844851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1004\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.254644  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.613299\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260437\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1005\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.244162  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.543012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.335224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397364\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1006\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.916144  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.490110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1007\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.517485  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.653053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344956\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1008\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.488837  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.743745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251613\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308898\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1009\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.125115  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.844017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1010\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.550006  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.977526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353861\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1011\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.802850  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.776945\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315544\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.379698\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1012\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.675548  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.650805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.382847\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.448964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1013\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.859081  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.818024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1014\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.660930  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.809498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231233\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286170\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1015\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.749697  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.586827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330657\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1016\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.574479  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.612002\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1017\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.690591  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.625847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263865\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1018\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.311971  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.501095\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1019\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.729221  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.502283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408676\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1020\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.300652  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.420940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268488\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1021\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.357248  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.650935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.323741\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.385432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1022\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.871794  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.594840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1023\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.394582  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.797937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278322\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1024\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.954162  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.749264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.364438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1025\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.068283  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.603446\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316591\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.374213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1026\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.276028  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.499691\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280155\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336820\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1027\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.382144  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.747541\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.333635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1028\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.928277  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.684331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1029\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.895517  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.737392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325937\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1030\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.556132  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.580103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.449443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.523100\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1031\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.439705  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.575388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277161\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333830\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1032\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.085789  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.564031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234420\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1033\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.224630  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.745937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1034\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.992985  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.625882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.393969\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1035\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.334494  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.756392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311745\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.374811\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1036\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.428286  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.636035\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.281617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340450\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1037\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.268359  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.741593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310087\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1038\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.475431  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.628098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352727\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1039\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.503770  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.814170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.366505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429297\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1040\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.158716  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.675807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.360766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1041\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.536541  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.723519\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255526\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.316757\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1042\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.366352  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.549277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372395\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1043\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.525693  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.788005\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.283023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1044\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.449600  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.861492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240545\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303191\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1045\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.328969  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.717766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237648\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1046\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.120090  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.780291\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.338053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.409189\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1047\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.452004  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.579840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.333446\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.396916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1048\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.672262  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.558143\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254539\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1049\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.825243  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.739075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1050\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.112540  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.582523\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1051\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.867555  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.617519\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.269155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1052\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.507338  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.608296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1053\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.867445  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.585858\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.289729\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343122\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1054\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.601968  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.651343\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1055\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.213219  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.208086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.305458\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1056\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.009228  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.426196\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1057\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.465150  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.731999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1058\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.584630  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.566887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.351433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.407154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1059\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.013863  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.426405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248488\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1060\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.655919  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.501564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.322499\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1061\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.713491  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.218076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340493\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.405491\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1062\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.656916  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.482324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.319225\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1063\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.426092  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.562953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.320751\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383912\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1064\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.304798  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.613645\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1065\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.273834  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.488340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322942\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1066\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.839613  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.633073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1067\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.203690  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.701785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.319686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1068\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.056585  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.520045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1069\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.901990  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.459197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1070\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.229152  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.653904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301643\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1071\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.051314  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.494690\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285678\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1072\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.123852  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.375006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290647\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1073\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.978493  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.606609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.306106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1074\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.175435  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.521071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235904\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1075\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.303898  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.662770\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366458\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1076\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.565508  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.675666\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1077\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.050350  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.501204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.296802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358596\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1078\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.906856  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.554969\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1079\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.491323  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.619629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.364157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1080\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.930236  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.581213\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.324680\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.395189\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1081\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.406216  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.598537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264333\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.329724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1082\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.384029  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.566469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.381050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1083\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.884124  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.650403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.371270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.438179\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1084\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.697881  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.349787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1085\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.197421  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.284840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1086\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.308123  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.551743\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329400\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1087\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.586435  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.173504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1088\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.265586  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.491405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1089\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.927199  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.301465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.242532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1090\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.583881  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.340166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.296308\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358911\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1091\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.211938  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.541786\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.405659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1092\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.087548  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.354878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1093\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.470648  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.501814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318823\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1094\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.338687  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.494856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1095\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.135621  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.462641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1096\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.584277  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.513139\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.283994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1097\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.989453  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.628573\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.211473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262168\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1098\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.895378  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.374371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254009\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307862\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1099\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.640550  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.496239\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.439711\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1100\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.293204  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.431335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345959\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1101\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.891151  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.524275\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.295600\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.345979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1102\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.811126  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.435737\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1103\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.304313  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.547238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246109\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1104\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.893962  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.435812\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313347\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1105\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.720207  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.375345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251296\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303613\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1106\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.673328  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.372425\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.225007\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1107\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.306147  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.353087\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.202130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255390\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1108\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.921659  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.426034\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315025\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.374438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1109\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.811137  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.425268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.321408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1110\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.362229  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.585629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1111\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.940757  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.356090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248690\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1112\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.763222  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.282523\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1113\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.498032  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.481181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226541\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1114\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.620474  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.547521\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354632\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1115\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.601848  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.447720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349098\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1116\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.484982  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.476061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.364606\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1117\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.126416  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.580715\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.305531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1118\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.946114  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.294761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1119\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.082315  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.410564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276063\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335797\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1120\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.321837  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.662382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1121\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.548492  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.555956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276324\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.333905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1122\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.931860  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.612596\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.307699\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.369609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1123\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.336800  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.575278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273101\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1124\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.274258  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.512851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1125\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.417372  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.337091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1126\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.658019  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.456108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276950\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1127\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.216988  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.495362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291695\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1128\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.564361  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.360972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356943\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1129\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.118061  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.161969\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270344\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1130\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.552868  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.276100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293539\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1131\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.765569  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.324743\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335346\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1132\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.118218  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.341716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1133\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.206343  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.235059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.341926\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1134\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.954861  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.485108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.394655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1135\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.515125  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.342226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274927\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1136\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.758660  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.492754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1137\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.412109  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.330944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.217300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1138\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.128059  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.277180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326270\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1139\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.533780  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.288832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305757\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1140\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.157364  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.359226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.222473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276105\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1141\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.847361  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.260414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251106\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.310255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1142\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.339890  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.267784\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1143\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.152489  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.323219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277514\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1144\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.981296  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.246252\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1145\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.432547  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.284691\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.295768\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1146\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.388805  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.483323\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356122\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1147\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.981273  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.320879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1148\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.067609  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.251855\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1149\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.778901  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.316908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.449294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1150\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.291248  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.228343\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375859\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1151\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.469120  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.293059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1152\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.584044  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.250434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313422\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1153\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.996682  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.315114\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.377178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1154\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.485500  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.338049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262362\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321347\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1155\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.988102  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.320295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.218076\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273607\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1156\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.378686  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.340401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1157\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.789183  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.056182\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.281519\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1158\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.678355  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.253185\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262358\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.324867\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1159\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.710542  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.358871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.318274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386251\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1160\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.247600  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.415993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239845\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1161\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.279461  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.384011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.281811\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.341401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1162\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.745703  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.366488\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1163\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.560485  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.173555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240131\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296462\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1164\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.980294  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.115765\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301670\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1165\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.436842  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.124320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1166\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.343190  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.267285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1167\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.759558  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.335266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1168\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.130319  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.226315\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1169\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.858687  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.141814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276416\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335389\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1170\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.130724  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.317287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1171\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.417788  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.189128\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262935\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1172\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.506455  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.347862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1173\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.313618  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.197249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1174\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.923072  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.319169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.318276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.381591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1175\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.919008  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.149764\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1176\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.475341  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.364695\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1177\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.781997  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.269481\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1178\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.816509  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.250520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.243107\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1179\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.509641  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.298704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256268\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319135\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1180\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.665417  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.138139\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.287687\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1181\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.374343  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.366225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253184\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.314689\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1182\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.375737  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.062174\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258565\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1183\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.056384  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.312251\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279442\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1184\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.410720  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.371223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296206\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1185\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.775620  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.099159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363906\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1186\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.683773  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.158580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.364159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1187\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.998258  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.065629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1188\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.963053  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.207694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329390\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1189\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.796266  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.146539\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397558\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1190\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.582557  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.091318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1191\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.594071  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.235265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259414\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1192\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.921071  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.001704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.211120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1193\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.423658  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.242386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1194\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.551437  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.247288\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1195\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.535842  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.196594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1196\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.472897  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.098642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1197\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.285945  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.174589\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290950\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1198\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.686618  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.440141\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431555\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1199\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.383414  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.312653\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1200\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.599354  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.264131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.376540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1201\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.630419  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.146208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220927\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.275402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1202\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.557487  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.096221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258713\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1203\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.131627  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.122537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.325430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1204\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.123269  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.188894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1205\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.316313  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.170621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254212\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1206\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.012800  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.031808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1207\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.004658  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.007074\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.321415\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.381430\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1208\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.671314  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.148925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261316\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313970\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1209\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.833321  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.087485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.345588\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1210\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.232497  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.354394\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234116\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1211\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.551789  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.109033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270511\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1212\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.613568  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.194687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246556\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1213\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.002361  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.112069\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1214\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.249125  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.228310\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293740\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351530\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1215\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.029383  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.941677\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1216\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.976633  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.089831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265036\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.320537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1217\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.313242  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.257860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.346959\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1218\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.147380  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.130298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238827\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282976\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1219\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.174571  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.132781\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1220\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.153105  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.168128\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1221\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.729970  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.081317\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220471\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.271314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1222\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.652396  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.166249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1223\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.811845  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.174136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1224\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.984813  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.127333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247075\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1225\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.027429  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.099238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1226\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.839027  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.021096\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300771\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1227\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.160131  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.023479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330433\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1228\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.829769  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.048819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240401\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1229\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.771250  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.852942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1230\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.751439  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.280383\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1231\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.034102  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.221681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1232\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.253589  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.065168\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306415\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1233\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.024288  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.100746\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1234\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.790187  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.995824\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1235\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.254591  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.267501\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1236\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.375028  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.233273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328557\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1237\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.234440  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.162065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314304\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1238\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.036749  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.153401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321259\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1239\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.982989  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.260774\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362027\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1240\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.039001  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.136225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236689\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288404\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1241\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.666597  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.148517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291288\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.345640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1242\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.820855  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.078434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397266\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1243\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.136760  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.086726\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294019\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344963\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1244\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.652745  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.262742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315069\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1245\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.213152  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.964004\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.281467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341943\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1246\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.739645  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.951128\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1247\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.803666  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.943738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.322066\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1248\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.108905  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.060615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264769\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327252\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1249\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.778627  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.005731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1250\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.929161  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.012734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.307278\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1251\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.582023  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.986080\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.369924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1252\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.510508  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.991020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.213142\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265999\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1253\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.805927  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.217359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.338627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.406713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1254\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.099241  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.073704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251444\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1255\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.811868  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.035401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228545\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1256\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.076604  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.006059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1257\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.083697  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.088526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.284071\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1258\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.216743  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.104717\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265294\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332527\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1259\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.127238  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.038913\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207162\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1260\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.028614  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.013630\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327652\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1261\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.200804  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.194131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.322313\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.386833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1262\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.099151  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.056993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1263\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.197811  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.154823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237285\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1264\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.672073  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.864000\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1265\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.877700  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.019376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234990\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296069\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1266\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.497939  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.904609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299412\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1267\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.632542  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.022447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.289848\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359101\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1268\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.791132  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.109964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.221477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1269\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.498947  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.967572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307678\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1270\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.408492  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.089699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291967\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1271\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.281898  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.976481\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231779\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1272\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.456855  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.072764\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1273\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.185756  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.163342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1274\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.484362  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.196400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288629\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1275\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.724189  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.156227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1276\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.716185  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.009901\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1277\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.839340  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.887648\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269145\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338036\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1278\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.556817  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.024158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1279\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.268533  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.986056\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.218662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1280\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.509954  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.019285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1281\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.744070  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.981770\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237735\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.302070\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1282\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.468953  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.759766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255746\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323088\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1283\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.757180  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.981039\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269010\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336985\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1284\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.615765  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.084121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1285\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.736301  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.993608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1286\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.072791  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.957615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269349\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1287\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.807975  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.786090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.289535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1288\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.011050  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.903203\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.216332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1289\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.473178  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.789806\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259681\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1290\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.558096  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.920577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.317195\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.387528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1291\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.900860  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.859382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1292\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.519471  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.978730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1293\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.553247  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.894927\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354406\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1294\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.456595  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.912714\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.223724\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282764\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1295\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.127027  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.796804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.284000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1296\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.550031  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.149618\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275992\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345005\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1297\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.410671  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.926704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253260\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1298\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.351875  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.814020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271380\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338703\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1299\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.300104  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.981655\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1300\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.002185  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.852951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1301\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.497653  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.044914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276198\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.341906\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1302\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.748253  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.939752\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293283\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361706\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1303\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.832219  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.057326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1304\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.529981  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.724169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386161\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1305\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.187522  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.795477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256648\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1306\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.530831  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.836885\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256961\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315261\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1307\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.980419  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.897674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.301418\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1308\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.724898  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.829811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1309\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.321060  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.979089\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1310\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.183600  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.902703\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236609\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1311\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.432534  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.016451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1312\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.438555  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.890811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1313\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.401466  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.804454\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.224143\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284824\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1314\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.658361  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.959394\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1315\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.071761  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.845323\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.307849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378512\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1316\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.781506  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.888340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259060\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325316\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1317\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.939958  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.808166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239174\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1318\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.132043  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.959146\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1319\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.146747  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.857912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1320\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.879880  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.979293\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.308084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1321\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.917335  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.879146\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257903\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.325604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1322\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.885120  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.854944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1323\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.269203  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.926572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251456\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316436\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1324\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.859556  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.804631\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268406\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.333958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1325\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.844725  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.014735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.289113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1326\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.612708  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.902103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.223601\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1327\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.717834  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.794517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1328\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.912938  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.669076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.244094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1329\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.046262  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.824593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.222722\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282941\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1330\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.857177  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.947756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253499\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314670\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1331\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.022077  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.869717\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268860\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1332\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.352644  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.700406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371166\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1333\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.686144  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.904586\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1334\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.191951  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.860051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1335\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.904751  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.821606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286571\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1336\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.721794  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.781658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329148\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1337\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.939182  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.850335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219710\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277778\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1338\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.813797  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.886785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.198197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1339\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.006207  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.780553\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.283279\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342036\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1340\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.245591  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.932404\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1341\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.816313  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.903922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266429\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.321478\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1342\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.841248  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.850465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323309\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1343\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.427688  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.669866\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286538\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1344\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.408082  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.702050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1345\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.771072  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.742846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1346\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.022186  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.767941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.224946\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1347\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.761644  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.866719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1348\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.802534  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.863587\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.229237\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286478\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1349\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.079498  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.797035\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1350\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.540122  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.824418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.283683\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342818\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1351\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.293001  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.953838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239909\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301172\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1352\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.396733  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.736831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1353\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.600062  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.851193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351044\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1354\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.848562  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.782893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1355\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.620294  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.777222\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.243248\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307697\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1356\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.919642  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.807917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1357\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.592445  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.642459\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266666\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1358\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.935060  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.915595\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.216716\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1359\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.038446  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.756023\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1360\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.800571  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.771018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1361\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.181504  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.813754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255106\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.312340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1362\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.820643  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.821373\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313993\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1363\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.261076  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.772338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253962\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1364\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.168074  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.929587\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.227314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286827\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1365\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.036762  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.783683\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339395\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1366\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.618151  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.677998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266813\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1367\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.814204  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.710785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270913\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1368\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.328509  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.747659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238193\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1369\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.113123  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.668719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1370\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.795644  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.680843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.324398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1371\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.368143  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.726863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279137\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1372\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.063624  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.773121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271267\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1373\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.385220  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.814987\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1374\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.965575  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.723951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1375\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.976521  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.739858\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1376\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.483039  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.659160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251493\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306586\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1377\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.814894  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.689924\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.223462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277384\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1378\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.690772  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.588954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220648\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271184\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1379\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.807436  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.673401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311632\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1380\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.407686  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.742163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.225179\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1381\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.741664  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.662514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270052\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.324843\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1382\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.700215  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.718802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.287507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347593\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1383\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.378346  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.695207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245020\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301032\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1384\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.239138  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.629368\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300732\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.360927\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1385\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.136928  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.770193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285529\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341828\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1386\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.367496  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.666120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349334\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1387\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.431558  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.791108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259918\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.315004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1388\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.730508  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.849872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1389\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.477730  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.733733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1390\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.244177  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.670730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283696\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1391\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.681310  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.684463\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256631\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1392\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.283039  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.742886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201176\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1393\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.384558  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.693079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239960\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293351\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1394\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.873236  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.610701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.224664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1395\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.981367  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.745823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1396\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.778197  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.637301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.290283\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1397\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.874606  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.774363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.242342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297657\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1398\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.782336  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.604302\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.229655\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.287612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1399\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.694245  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.714279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.196405\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1400\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.521451  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.604402\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1401\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.845139  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.697482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250977\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.305984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1402\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.768746  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.589872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257852\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1403\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.997549  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.755470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.269300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1404\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.303729  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.780099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1405\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.560827  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.702175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1406\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.825584  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.784680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1407\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.132131  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.565638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.243410\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295817\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1408\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.577543  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.649707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1409\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.525455  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.607465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1410\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.646441  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.689517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1411\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.775788  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.623805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1412\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.710253  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.615134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1413\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.441375  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.646007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233641\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1414\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.742369  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.809282\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312942\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1415\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.301187  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.712339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1416\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.679498  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.645382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.206388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1417\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.531451  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.811410\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273000\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1418\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.701016  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.524832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1419\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.748587  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.566822\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.196356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1420\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.791429  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.543081\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293620\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1421\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.395202  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.577608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241138\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.299110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1422\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.483132  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.652835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265105\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1423\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.538941  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.695446\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236812\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296242\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1424\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.513649  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.713775\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215522\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1425\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.634114  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.582175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.373789\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1426\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.665711  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.602979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1427\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.446650  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.679328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1428\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.465937  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.612679\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219883\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280346\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1429\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.067942  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.561894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1430\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.727261  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.707019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.346401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1431\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.464828  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.649071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.225604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1432\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.586116  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.754860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1433\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.599391  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.636208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1434\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.880561  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.624037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.227171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1435\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.471420  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.632225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254447\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1436\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.149258  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.594051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1437\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.500231  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.660771\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1438\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.899086  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.607491\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.225185\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275045\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1439\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.915213  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.700615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306707\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1440\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.648865  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.588817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.217853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1441\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.929944  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.674182\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199097\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.248429\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1442\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.265154  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.708386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194949\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1443\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.392290  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.583942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1444\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.924722  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.603754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1445\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.568955  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.588335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1446\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.879074  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.547972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210770\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1447\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.594980  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.486388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.243369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1448\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.403714  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.677277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311622\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1449\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.491151  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.693231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1450\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.331028  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.557909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304094\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1451\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.880267  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.596873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233110\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1452\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.620792  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.675240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249546\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307593\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1453\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.928644  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.494380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.196749\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247575\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1454\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.505207  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.486171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241148\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1455\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.395291  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.492376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199717\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249710\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1456\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.249719  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.629109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226471\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1457\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.562590  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.712496\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325760\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1458\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.798921  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.622268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233337\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1459\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.541117  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.715617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.211770\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1460\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.550210  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.619872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207225\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259379\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1461\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.045365  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.637926\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274733\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.338021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1462\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.947631  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.695283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302823\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1463\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.671153  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.555982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.223808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1464\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.242250  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.581415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.227022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280754\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1465\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.393429  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.603498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262106\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1466\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.758899  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.666469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.202872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1467\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.303632  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.488346\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215652\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1468\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.235670  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.566201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245237\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1469\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.363878  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.487307\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.225987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1470\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.413931  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.507481\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1471\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.423610  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.523936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.287569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1472\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.267195  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.547140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297529\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358643\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1473\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.677816  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.563351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1474\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.222642  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.712152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1475\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.683187  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.587304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1476\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.235824  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.568205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245377\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.880273  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.456739\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272804\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1478\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.111370  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.633826\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1479\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.435665  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.614917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233075\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1480\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.337798  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.417357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271708\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1481\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.348470  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.600724\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210629\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.260068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1482\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.626831  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.500422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.222978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1483\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.869237  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.551652\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278416\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1484\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.734126  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.602672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.264899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325253\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1485\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.458350  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.462180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1486\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.206181  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.669485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273985\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1487\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.464298  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.523304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245128\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1488\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.056452  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.379186\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1489\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.280991  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.444864\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1490\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.620528  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.631685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1491\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.693222  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.608297\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1492\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.512251  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.594263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1493\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.440855  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.480780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.213371\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1494\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.511878  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.642903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.229851\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1495\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.539588  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.481207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205501\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1496\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.526602  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.594366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.217747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276161\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1497\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.850488  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.644380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1498\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.667063  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.425590\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286349\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1499\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.789374  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.532208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.251706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1500\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.287110  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.510153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294148\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# a useful function to present things clearer\n",
    "def seperator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "# define parameters\n",
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50\n",
    "batch_size = 64\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp.to(device), trn_amplp.unsqueeze(-1).to(device))\n",
    "val_dataset = amp_dataset(val_datfp.to(device), val_amplp.unsqueeze(-1).to(device))\n",
    "tst_dataset = amp_dataset(tst_datfp.to(device), tst_amplp.unsqueeze(-1).to(device))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)\n",
    "epochs = 1500\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "hdn_dim = 50\n",
    "model_dr = amp_net_dr(hdn_dim=hdn_dim).to(device)\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(model_dr.parameters(), lr=learning_rate)\n",
    "seperator()\n",
    "print(\"model architecture\")\n",
    "seperator()\n",
    "print(model_dr)\n",
    "total_parameters = sum(p.numel() for p in model_dr.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses_live_dr = []\n",
    "trn_losses_dr = []\n",
    "val_losses_dr = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    seperator()\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    seperator()\n",
    "    trn_loss_live = train_epoch(trn_dataloader, model_dr, loss_fn, optimizer)\n",
    "    trn_losses_live_dr.append(trn_loss_live)\n",
    "    seperator()\n",
    "    model_dr.eval()\n",
    "    trn_loss = trn_pass(trn_dataloader, model_dr, loss_fn)\n",
    "    trn_losses_dr.append(trn_loss)\n",
    "    seperator()\n",
    "    val_loss = val_pass(val_dataloader, model_dr, loss_fn)\n",
    "    val_losses_dr.append(val_loss)\n",
    "    model_dr.train()\n",
    "    seperator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "c3 = 'tab:orange'\n",
    "\n",
    "axs.plot(trn_losses_dr, label=\"train loss\", color=c1)\n",
    "axs.plot(val_losses_dr, label=\"val   loss\", color=c2)\n",
    "axs.plot(trn_losses_live_dr, label=\"live train loss\", color=c3)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"epoch\")\n",
    "axs.set_ylabel(\"MSE\")\n",
    "\n",
    "axs.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can see that dropout has completely prevented over-training! We also see that the 'live' loss is higher do to the dropout used in training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularisation\n",
    "\n",
    "Similar to dropout L2 regularisation is a method which can be used to reduce overfitting.\n",
    "\n",
    "It simply works by adding a term of the form: ~$\\lambda|\\omega|^2 $\n",
    "to the loss function. In our case we get:\n",
    "\n",
    "$$L = MSE + \\lambda|\\omega|^2$$\n",
    "\n",
    "where $\\lambda$ is an additional hyperparameter.\n",
    "\n",
    "One possible interpretation of L2-regularisation is having a normal prior over weight space where $\\lambda$ is related to the width of the prior and the central value is set to 0. This normal prior resctricts the model complexity by punishing models with a weight configuration far away from its central value of 0.\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's just copy paste everything from before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a useful function to present things clearer\n",
    "def seperator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "# define parameters\n",
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50\n",
    "batch_size = 64\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp.to(device), trn_amplp.unsqueeze(-1).to(device))\n",
    "val_dataset = amp_dataset(val_datfp.to(device), val_amplp.unsqueeze(-1).to(device))\n",
    "tst_dataset = amp_dataset(tst_datfp.to(device), tst_amplp.unsqueeze(-1).to(device))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)\n",
    "epochs = 1500\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "hdn_dim = 50\n",
    "model = amp_net(hdn_dim=hdn_dim).to(device) # model without Dropout!\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch L2 regularisation can be added via the argument 'weight_decay' of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note:\n",
    "\n",
    "An alternative explicit implementation would be:\n",
    "\n",
    "```\n",
    "weight_decay = 1e-2\n",
    "l2_norm = sum(torch.linalg.norm(p, 2) for p in model.parameters())\n",
    "\n",
    "loss = loss + weight_decay * l2_norm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture\n",
      "-----------------------------------------------\n",
      "amp_net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 8751 trainable parameters\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 215.510025  [    0/ 1000]\n",
      "avg train loss per batch in training: 213.827185\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 210.149334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 211.982016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 203.697769  [    0/ 1000]\n",
      "avg train loss per batch in training: 203.288278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 192.792691\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 194.248711\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 196.490219  [    0/ 1000]\n",
      "avg train loss per batch in training: 175.179067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 147.815821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 148.163631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 146.297607  [    0/ 1000]\n",
      "avg train loss per batch in training: 108.765290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 61.280390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 61.236669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 57.639748  [    0/ 1000]\n",
      "avg train loss per batch in training: 43.720009\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 43.470731\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 49.068373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 48.226639  [    0/ 1000]\n",
      "avg train loss per batch in training: 39.108218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 36.249311\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 37.996541\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 48.109173  [    0/ 1000]\n",
      "avg train loss per batch in training: 35.758659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 33.406069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 36.008030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 28.408659  [    0/ 1000]\n",
      "avg train loss per batch in training: 32.917050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 31.612536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 33.741502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 32.860794  [    0/ 1000]\n",
      "avg train loss per batch in training: 31.341733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 29.671619\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 32.063855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 29.595182  [    0/ 1000]\n",
      "avg train loss per batch in training: 29.603939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 28.497802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 30.508292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 29.482378  [    0/ 1000]\n",
      "avg train loss per batch in training: 27.875879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 26.937648\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 28.783778\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 35.446987  [    0/ 1000]\n",
      "avg train loss per batch in training: 26.115344\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 24.938623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 27.160112\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 20.330017  [    0/ 1000]\n",
      "avg train loss per batch in training: 24.141137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 23.132744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 25.050194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 26.868034  [    0/ 1000]\n",
      "avg train loss per batch in training: 22.326141\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 21.179163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 22.897548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 23.083549  [    0/ 1000]\n",
      "avg train loss per batch in training: 20.037888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 18.500156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 20.243468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 19.590378  [    0/ 1000]\n",
      "avg train loss per batch in training: 17.280364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 15.578029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 16.783971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.366457  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.998727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 11.929970\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 12.959829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 11.963364  [    0/ 1000]\n",
      "avg train loss per batch in training: 10.386493\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 8.268174\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 8.868952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.693984  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.716829\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.822334\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 5.378506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.309978  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.025351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 3.247159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.780132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.548410  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.385393\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.648482\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.024832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.207107  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.466904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.135488\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.387314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.369847  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.060521\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.007555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.186346\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.678793  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.886127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.758832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.941599\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.366032  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.766322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.686000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.890320\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.549219  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.654148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.642435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.786772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.575158  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.616727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.538818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.708330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.359168  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.544305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.506660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.670715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.522353  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.517313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.482442\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.628833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.390262  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.473086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.461008\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.594695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.591095  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.467574\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.441229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.588798\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.375532  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.450318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.428479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.552133\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.437841  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.445466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.424810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.567652\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.388166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.430614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.434479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.552668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.594025  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.411876\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.396474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.530748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.350750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.404110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.414790\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.519753\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.454190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.395382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.365970\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.493967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.424682  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.395099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.404878\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.554053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.243014  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.386836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.368490\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.477396\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.375831  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.398227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.347402\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.462446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.305370  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.359075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.342321\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.470621\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.332326  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.356364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327203\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.447324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.427558  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.349096\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334509\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.472249\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.520701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.358992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.338833\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443147\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.289831  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.329809\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.326222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.334690  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.319152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431949\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.239538  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.318854\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431034\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.257379  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.313476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.311269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.430864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.210530  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.311956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.305313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.361676  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.305831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.317388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.442464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 51\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.316210  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.307291\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.299869\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 52\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.359510  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.305332\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.295783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.420446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 53\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.329190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.303731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.303271\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.415979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 54\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.361111  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.308941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.403640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 55\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.305093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.300695\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.294925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.410699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 56\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226662  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.307654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297831\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.405363\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 57\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.313806  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.311995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293556\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.428230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 58\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.293303  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.308172\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.316479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.423042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 59\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.374354  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.298350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288311\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.401362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 60\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.244097  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.301305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286380\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.413370\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 61\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.335155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.284603\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286171\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.407339\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 62\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.311989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.286635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.402546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 63\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214071  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.293245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.279937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.396050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 64\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266736  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.294789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.446141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 65\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.410666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.288071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.281369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.396957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 66\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.230523  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.286618\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274742\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389820\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 67\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.330099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.291608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 68\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.258266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.307064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.415720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 69\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.238143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.307930\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.423497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 70\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.213745  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.317359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.345901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 71\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.363940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.310759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.286168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 72\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.370633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.288365\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.381581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 73\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.213870  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.285976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275177\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 74\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.284997  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.279168\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.280228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.408322\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 75\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.331061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.287806\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389662\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 76\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.242278  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.290693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.303847\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.442794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 77\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.331786  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.327176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.313421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.415321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 78\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.358852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.303071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.395614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 79\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214249  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.272158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 80\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.245955  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.279886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270194\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 81\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.212635  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.273441\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.381636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 82\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.246268  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.271274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263153\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.376173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 83\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.349528  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.269079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.402099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 84\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.321051  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.271765\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.377597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 85\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.314562  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.272164\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 86\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.211025  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.271561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 87\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.383738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.269976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277089\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.401278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 88\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.183218  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.289760\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260880\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 89\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.267066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.263933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.369661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 90\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.223426  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.264694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.373953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 91\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.354878  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.273689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272765\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.397653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 92\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.246636  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.294503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.300601\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398170\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 93\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266772  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.282066\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398603\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 94\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.202002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.268953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 95\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.258985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.263137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 96\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.312685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.261658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 97\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203820  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.259314\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372411\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 98\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.257978  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.263412\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260976\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380267\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 99\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.275376  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.265072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261454\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370125\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.307892  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.268327\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269100\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.333100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.269499\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.267204\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.276696  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.258832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.183976  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.257522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254905\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.372693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190364  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.256134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.245096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.256076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.256277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.199087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.260567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.375357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.349007  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.259470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245002\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357161\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224289  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.257866\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.258231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384576\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203414  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.265432\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363911\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.188660  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.257439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.283543  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.255908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.247983  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262247\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.244803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.251556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363849\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.241607  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.257742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.314196  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.276976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.244136\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.242533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.295997\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.383104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.338114  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.248939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361000\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.229596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.248602\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358201\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197212  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.250698\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.334714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.245689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.258909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.260783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.250834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239406\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.275206  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.250064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.242474\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.351050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.227989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.254712\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250040\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175461  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.275710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.243950\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.250946\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.245802  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.253542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236353\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147000  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.252199\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.280403  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.251571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.239880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.241914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353404\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.289905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.252413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232668\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345134\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.282988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.244228\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238804\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221720  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.244925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.265288\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224450  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.249670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.229263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.346649\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.264856  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.248549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236306\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161894  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.257325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342602\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.248313  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.242592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.189833  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339335\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226763  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236004\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.244719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.260630  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.244839\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.198504  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.238322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167450  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.249196\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169159  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.289014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337916\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.419699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.349704  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.268966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352886\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.301529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.271723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.247050\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344617\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.280389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.211949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.235580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.355405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.306698  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.241336\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236373\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341937\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.216820  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.245451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.229984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266520  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.230075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.245313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371903\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.288358  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.228558\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231195\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329196\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.253309  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.230454\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.222706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208556  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.240371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343200\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.258508  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.237007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.221071\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.324887\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.239629  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.221552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.262490  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.224059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.186362  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.364701\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.290914  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.351665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176638  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.230443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214279\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.211392  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.221839\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.259464  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.230764\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322124\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.200022  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263908\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197422  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.227556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.212872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.202183  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.221906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.218563\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.331126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232249  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.229841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385745\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.232071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.200589  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.220938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.244959  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.239996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.312334  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312336\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226522  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.220091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208828\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.245626  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.225492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.363715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.238812  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.240049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.191222  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.235505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.308372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214464\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332041\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143599  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.228654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.211918\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210135\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309843\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.215092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215484\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.233362  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.231189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.353453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.315206  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.233943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.284364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.422634\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.315748  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.229388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.279205  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.194378  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.228833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232727\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.271935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.215841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.204587  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.202797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.305074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152146  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.219338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231377\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.355712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.317405  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.219097\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205376\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301731\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203517  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.213506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 190\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140204  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.216364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.202535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206375  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204216\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.309465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.327419  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.204900\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 193\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.228110  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.225646\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197279\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 194\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.189128  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.224031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200777\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299800\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 195\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190330  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.204304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 196\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.235778  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.205013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197671\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 197\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156655  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.209435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.195881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 198\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.243361  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.208904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311237\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 199\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187777  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.204371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358436\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 200\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.227594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.242535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.285906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.413042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 201\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.246893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.233473\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 202\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.242866  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203363\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.313660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 203\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150220  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.213042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.198477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 204\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.306069  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.208247\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.212301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327248\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 205\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266797  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210354\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 206\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.210929  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.205992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339361\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.200391\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.191607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293351\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 208\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136255  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210271\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193116\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.290858\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 209\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164151  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.206635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 210\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208563  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209527\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 211\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210211\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197098\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300102\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 212\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.204690  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 213\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.351252  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.255334\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.195061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 214\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.303462  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.211631\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 215\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318682\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 216\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.202799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204983\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226002\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.311167\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 218\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.255361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260545\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 219\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.340263  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207855\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.186877\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 220\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161124  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.202257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.244567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.326867\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 221\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208259  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.201595\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 222\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.201827  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.197045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 223\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.213139  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.197166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 224\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.215835  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.203154\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193406\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 225\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179954  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.194826\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.186533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 226\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.192684  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.194291\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.189267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 227\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164869  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.206636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200175\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310650\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 228\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.259584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.191022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 229\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.227209  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.209577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.186945\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.282326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.199344  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199891\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.186580\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 231\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.205577  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.192553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 232\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128269  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.195869\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183481\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.287348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 233\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171768  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.195475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.189115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 234\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.194891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183757\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331356\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 235\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224612  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.202972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.211011\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 236\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198379\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207622\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 237\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.186287  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318280\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 238\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195597  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192612\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 239\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.242132  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191611\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.182440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 240\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144917  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189162\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 241\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176690  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 242\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.193407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.187550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 243\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.360802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 244\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171250  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.228510\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 245\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219409  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 246\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138051  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.186503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185652\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280908\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 247\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.194173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275575\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 248\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188706\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 249\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.264153  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196048\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 250\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175279  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.184569\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194916\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.307061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 251\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.173775  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.186387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 252\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232398  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.180259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288336\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 253\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161112  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183000\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285072\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 254\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108107  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 255\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138459  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191844\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.242455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 256\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175868  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.208934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.189569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178581  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.186733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303035\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154233  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.179346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277672\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 259\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.176188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267583\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.186019  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 261\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189975\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 262\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175963  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.173563\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 263\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.204971  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.214962\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 264\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.213615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198901\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300859\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 265\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.215860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.175526\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 266\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196641  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189130\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280533\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 267\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153476  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263166\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 268\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149397  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185470\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.192627  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282000\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.200278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340696\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 271\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.322667  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.182271\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.287022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 272\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169384  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177612\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178458\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 273\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187920  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182332\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209788\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.285180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 274\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181242\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.176026\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270498\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 275\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.184099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.320796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 276\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208478  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.204562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.211951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 277\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182039  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.203816\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.212869\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 278\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195602  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.194821\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 279\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.238403  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.236785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.307294\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378404\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.302447  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.196833\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 281\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165826  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271912\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 282\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.211602  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173771\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164885\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258127\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 283\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155426  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181575\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236575\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.371120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 284\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208939  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.200253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 285\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.194362  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228294\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 286\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.207572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.211825\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.180750\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 287\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.297070  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.174324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250745\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 288\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 289\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146076  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167446\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 290\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 291\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.173615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 292\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176782  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203651\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.285221\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 293\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.173904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187118\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.182208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 294\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174201  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.224636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.276808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343309\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 295\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266027  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.216400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.184624\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 296\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165069  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.165308\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268070\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 297\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219349  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172004\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256270\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 298\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.223100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183786\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.233494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 299\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.272333  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210209\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240292\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310601\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 300\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.230214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237009\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 301\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208335  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 302\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188612\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188265\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303395\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 303\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165780  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179010\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317106\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 304\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.229074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 305\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182581  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180958\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.173095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 306\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141239  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.174692\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.165727\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 307\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139089  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177870\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 308\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137693  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183618\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.226552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348674\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 309\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.228441\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.302345\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.449758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 310\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.312063  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226463\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 311\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187569\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171010\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259090\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 312\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164427  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.200101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.218507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.296254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 313\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167711  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.209273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203924\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.278878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 314\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190220  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162460\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 315\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176579\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207112\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 316\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203310  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156385  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 318\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131415  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175688\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163697\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 319\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138252  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.166808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207033\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 320\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157703  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173299\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164469\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252892\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 321\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175495\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188366\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.294539\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 322\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.210362  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 323\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141170  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.180650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260408\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 324\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.193807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171021\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.161944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 325\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 326\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138046  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172711\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 327\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155218  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266248\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 328\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116505  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.170089\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164377\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 329\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.265812  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163967\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.166782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 330\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258153\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 331\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.168066\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 332\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.189351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159931\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 333\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167398  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.174058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 334\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179841  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197719\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.274724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 335\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224486  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 336\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167831  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.174271\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 337\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163652  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.174348\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.179532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 338\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.233760  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167890\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246977\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 339\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.200107  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167128\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152401  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167336\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163732\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248777\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 341\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142654  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 342\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162448\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251037\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 343\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.227408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161213\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 344\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 345\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168446\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250678\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 346\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137673  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245427\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 347\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.161661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 348\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102357  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171883\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 349\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184274  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251359\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 350\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141133  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 351\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160181  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169048\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 352\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152683  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159117\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 353\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162026  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.318592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 354\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147323  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176751\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161387  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161772\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158125\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.246099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 356\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142732  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.165503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 357\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.207779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171764\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273408\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 358\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.233308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176510\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162081\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261628\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 359\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158348  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187052\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.166001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 360\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154516  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.155981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 361\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156320\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248107\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 362\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095124  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.173336\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 363\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177283  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 364\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164428\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246800\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 365\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159510\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 366\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169149  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 367\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146247  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 368\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094841  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 369\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.165390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255189\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 370\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.199972  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162337\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 371\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.259838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164743\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 372\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.204926\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.224913\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 373\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.241637  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.155987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243311\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 374\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.192214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267124\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 375\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.217351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196440\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.173919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252530\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 376\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164535  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200829\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.276893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 377\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146141  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.217997\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 378\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 379\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.216579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169974\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153432\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 380\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149014  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171709\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 381\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160525  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 382\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.170551\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 383\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.185173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157591\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241193\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 384\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256303\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 385\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164632\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159283\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264336\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 386\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158055\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 387\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132098  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 388\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.200238  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 389\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 390\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.215660  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.166418\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260519\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 391\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182979  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172369\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.168537\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 392\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128469  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159367\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 393\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187130  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164666\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.150605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 394\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.235440  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158137\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 395\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134180  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185266\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289824\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 396\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.186761  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164648\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 397\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.191223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151090\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.243062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 398\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.194989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160009\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.165453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 399\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122454  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.289865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 400\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.193573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173588\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157949\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 401\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094475  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172394\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 402\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109577  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188303\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 403\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177820\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234248\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 404\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178232  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173798\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241026\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 405\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195946  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.237792\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 406\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240171\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 407\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.185940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157762\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154867\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242920\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 408\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179995  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155660\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.149578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238859\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 409\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102987  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159105\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 410\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214541  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177762\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236993\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 411\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196401  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.161655\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 412\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147532  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154155\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 413\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155400  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153278\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 414\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154037\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230786\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 415\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122715  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 416\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128758  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158211\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 417\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153145\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.149403\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235620\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 418\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169491  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.165782\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.251242\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 419\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203041  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239184\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 420\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184569  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245733\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 421\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.216865\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.324164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 422\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219205  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 423\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121230  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172711\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231922\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 424\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123502  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.295603\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 425\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.188402\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 426\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.204465  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.161759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 427\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166932  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245915\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 428\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147290\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234755\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 429\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131126  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164847\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 430\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167740\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 431\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.176705\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 432\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131123  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.158426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 433\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.183470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153853\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 434\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.150598\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 435\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125952  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.170964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.275175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 436\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.259007  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183087\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.309336\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370100\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 437\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.332471  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.217972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.211579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290207\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 438\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.193282  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230995\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 439\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195517  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.146912\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.232680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 440\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152706  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154524\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231529\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 441\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169637  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160118\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 442\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146876  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153927\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232279\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 443\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133128  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172829\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 444\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152409  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168551\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347771\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 445\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.170731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.165388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 446\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149116  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154239\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 447\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144859  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 448\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148769  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 449\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153441\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 450\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 451\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.195465  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171820\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.187946\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 452\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149663  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233008\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 453\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224207  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.146669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 454\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128878  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 455\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187033  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230101\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 456\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.155461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 457\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159646  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145299\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224985\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 458\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164872  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142040\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 459\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144814  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148367\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.174614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 460\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137095  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.146021\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.240681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 461\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163303\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140801\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 462\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115082  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 463\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.168829  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.155738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 464\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151762  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151155\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 465\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.202293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 466\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142117\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 467\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.202645  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 468\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140519\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 469\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164952  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150928\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.146024\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 470\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137645  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151312\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218125\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 471\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149616\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143779\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 472\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162364  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141080\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219088\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 473\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122326  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 474\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114499  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 475\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146102  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 476\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139562  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 478\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137192  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.144526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139689\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226816\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 479\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126970  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228969\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 480\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.157828\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 481\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170332\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.237240\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 482\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 483\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159132\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144365\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215026\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 484\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160990  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148150\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145352\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236055\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 485\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 486\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.217593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 487\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.227949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224169\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 488\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214950  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148358\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.206752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 489\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165894  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.170005\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 490\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.242233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 491\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226460\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 492\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169182  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 493\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153371  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228783\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 494\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174030  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140955\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 495\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117635  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137014\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224500\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 496\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 497\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163595  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143796\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 498\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160430  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 499\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119742  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143410\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 500\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137978  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152146\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217915\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 501\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.189509\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260237\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 502\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141399  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158963\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137952\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.219819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 503\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196031  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183879\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 504\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155683  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156830\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 505\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174015  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168533\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145638\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 506\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119756  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148600\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217264\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 507\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145700\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134418\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 508\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155943  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 509\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.200094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170256\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 510\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155500  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216985\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 511\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120959  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139441\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 512\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154080  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143420\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233249\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 513\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083474  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143868\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 514\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149251  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.155341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.246779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 515\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.184874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.191096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 516\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.263660  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142847\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216920\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 517\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147917  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143075\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 518\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127017  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.150888\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 519\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157392  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270869\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 520\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170875  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 521\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181202  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140667\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 522\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.198745  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 523\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160987\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.230906\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 524\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.180805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255094\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 525\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221335  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182228\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.184450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280036\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 526\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138845  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.201250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 527\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206825  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.168253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.239692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 528\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136528  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216785\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 529\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141515  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142844\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.149600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 530\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 531\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145626  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135047\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 532\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176835  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215309\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 533\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108722  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136344\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 534\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113784  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146697\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 535\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184600  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143445\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234309\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 536\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156857\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241096\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 537\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178780  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172690\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241652\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 538\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137614  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148192\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 539\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176140  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149891\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 540\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 541\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 542\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141742\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 543\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181969  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140491\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132517\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209304\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 544\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136958  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.144984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132883\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.206278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 545\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145145\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 546\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136362  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139524\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 547\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 548\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177705  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133251\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204557\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 549\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106192  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 550\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109656  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 551\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138604\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156260\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226368\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 552\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163280  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228880\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 553\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153739  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136824\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 554\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178266\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240926\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 555\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.216367  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143536\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129554\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213934\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 556\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104540  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.149906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 557\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134803  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131423\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 558\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149168  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139607\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213626\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 559\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146452  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 560\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166962  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130019\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 561\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148413  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.165561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.172993\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270888\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 562\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.213888  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 563\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171257  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150317\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143990\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 564\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115129  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 565\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150457  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135589\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148800\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.215343\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 566\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.221130\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 567\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 568\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.281377  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163852\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215606\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 569\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177332  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 570\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114795  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 571\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144332  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 572\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.171753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.264304\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 573\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159171  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 574\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126080  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205347\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 575\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137652\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.166676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 576\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129308\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 577\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151985\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 578\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127883  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133713\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 579\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.187415  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147928\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 580\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149486\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 581\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181621  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142294\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203403\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 582\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142675  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136315\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 583\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.192749  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231351\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 584\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.183472  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 585\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078324  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126352\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 586\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127937\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.206430\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 587\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.143610\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 588\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131416  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150869\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257059\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 589\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122175  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126742\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 590\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127722\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201868\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 591\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123969  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132248\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211726\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 592\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105070  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 593\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147499\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148600\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 594\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110602  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 595\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149062\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131644\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201291\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 596\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121315  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133497\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210421\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 597\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139236\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126377\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 598\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174542  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128683\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199489\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 599\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147221  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125290\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197573\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 600\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128496  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215869\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 601\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088936  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140430\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220827\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 602\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151102  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160889\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 603\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132635  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161717\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 604\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116895  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140908\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209411\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 605\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.166064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139151\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 606\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111482  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131432\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198583\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 607\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134348\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128131\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.198747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 608\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138597\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219990\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 609\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154070  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 610\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139929\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160337\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.222451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 611\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160752  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.162505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.182095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 612\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181121  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124544\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194108\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 613\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 614\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112718  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141663\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202607\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 615\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161539  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 616\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134394  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 617\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117077  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130385\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 618\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184625  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144931\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 619\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206768  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138239\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123176\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 620\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130537\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209195\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 621\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112923  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134980\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193572\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 622\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131211\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 623\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095025  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234879\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 624\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116769  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198558\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 625\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 626\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130390  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132330\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 627\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153392  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127748\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 628\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157495  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123802\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.200865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 629\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109561  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 630\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127735\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 631\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112222  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120807\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196601\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 632\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103566  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144777\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 633\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131264  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193948\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 634\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.235933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192241\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 635\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116708  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121629\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 636\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093423  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191492\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 637\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139407  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163602\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249759\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 638\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138518\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 639\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113734  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147918\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 640\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148422  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148628\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 641\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096479  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135500\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219900\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 642\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.185975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135300\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124014\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 643\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129159  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.150200\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 644\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170501  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122011\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 645\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167257  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122922\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 646\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202754\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 647\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 648\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 649\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122421  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169066\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.227246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 650\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164718\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254929\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 651\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.188498  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159931\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.175174\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 652\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155842  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 653\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 654\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182112  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130208\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 655\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190648\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 656\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 657\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106793  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127747\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199785\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 658\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137894  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128746\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 659\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142884  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142688\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191601\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 660\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110894  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150901\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133178\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 661\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142681  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121321\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196158\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 662\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108095  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 663\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084386  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122469\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 664\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.168924  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149945\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.174595\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236019\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 665\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.230211  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121081\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190212\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 666\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114817  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131533\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 667\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150540  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206311\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 668\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121404  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118457\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 669\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094008  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118770\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 670\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119247  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119085\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.195926\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 671\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081677  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 672\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 673\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103535  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122600\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 674\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116406  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 675\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125626  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 676\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179479  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119758\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186196\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 677\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122583\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 678\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095118  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134195\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 679\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151551  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123302\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 680\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 681\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 682\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112890  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.168060\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 683\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129676\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133680\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 684\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.183544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116734\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188488\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 685\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116633\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 686\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087039  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 687\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105224  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 688\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108997  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 689\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132038\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 690\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106746  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126128\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125458\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 691\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133719  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118164\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.192219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 692\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085412  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121784\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 693\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137818  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 694\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142394  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125970\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 695\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123363  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 696\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125818  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.175637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268484\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 697\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139686  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259599\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 698\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.188359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 699\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190154  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 700\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112084  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 701\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115252\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 702\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107837  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128573\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147033\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 703\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158618  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134074\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118758\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 704\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 705\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112075  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126775\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 706\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117391\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 707\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158170  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 708\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100773  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 709\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093551  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 710\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111543  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124788\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122649\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 711\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129965  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.144931\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 712\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103510  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.175270\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.266881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 713\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133076  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121203\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 714\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087139  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120597\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 715\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098340  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 716\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170598  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130483\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 717\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115530  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125698\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 718\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126597\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192840\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 719\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120936  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185198\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 720\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088489  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121145\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202247\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 721\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082432  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116470\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 722\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094341  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 723\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105994  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118024\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 724\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158970  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145584\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.213297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 725\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.198627  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150069\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127370\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 726\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110050  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136678\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131965\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213480\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 727\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080059  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128147\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 728\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121675\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 729\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113502  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122858\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 730\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100217  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179455\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 731\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097312  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121293\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 732\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099634  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113584\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 733\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113603  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124044\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112368\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.182591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 734\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183247\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 735\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132222\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139413\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 736\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156004  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 737\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126254\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 738\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 739\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150955\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 740\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147434  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125959\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202089\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 741\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099536  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124585\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184542\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 742\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101675  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 743\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134244  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 744\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 745\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144645\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 746\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121401  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147800\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 747\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130516  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115957\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 748\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132932  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 749\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184770\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 750\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101268  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 751\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090318  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 752\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130564  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122384\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 753\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137813  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 754\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109968  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123835\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.203199\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 755\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126810\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 756\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129097  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 757\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184639\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 758\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130125  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114920\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137852\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219584\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 759\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133026  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110319\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 760\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140334  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130514\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 761\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114090  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215033\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 762\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 763\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091833  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131165\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 764\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076739  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 765\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122751\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201912\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 766\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117783  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 767\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116421  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119481\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 768\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124667  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110710\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 769\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129390  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112631\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 770\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164333  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179438\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 771\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115420  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 772\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116285\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181429\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 773\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134251  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 774\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098169  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114005\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185811\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 775\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090783  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113033\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.183620\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 776\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149732  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110353\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 777\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112481  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120353\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 778\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109414  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 779\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130603  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 780\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128388\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127714\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 781\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119709\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118049\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198131\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 782\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120235  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 783\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 784\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101088  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123797\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 785\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076755  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199755\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 786\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113739\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178312\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 787\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090916  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 788\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121608  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123613\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203017\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 789\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140423  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 790\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133687  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137586\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114574\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 791\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093480  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109616\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182673\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 792\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110002\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 793\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128286  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112310\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 794\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117435  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 795\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127332  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113344\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179188\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 796\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135342  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112641\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.179351\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 797\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115778\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 798\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100309  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121602\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172853\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 799\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108684\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 800\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128962  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 801\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111101  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 802\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124448\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 803\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143332  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 804\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 805\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112522  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112186\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 806\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108820  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129777\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204576\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 807\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145672  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 808\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105895  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116870\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106009\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 809\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124080  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 810\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128634  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 811\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 812\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157814  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184127\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 813\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099095  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 814\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 815\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156229  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182820\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 816\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084077  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114087\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189706\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 817\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078304  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115744\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107218\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.176298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 818\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112346\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133265\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 819\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175090  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 820\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133713\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194140\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 821\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 822\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110628\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107522\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175927\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 823\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116335  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114048\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175011\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 824\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114489\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108151\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180988\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 825\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 826\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104730  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 827\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087364  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 828\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198090\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 829\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129974  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108273\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 830\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123647  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108813\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 831\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106747  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 832\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130847  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108021\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181541\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 833\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111743  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108735\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 834\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084147  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 835\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088680  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194501\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.286451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 836\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176185\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 837\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174628\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 838\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131829  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119528\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.194518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 839\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130463  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126135\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 840\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145737\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 841\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122870  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123192\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193854\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 842\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115575\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106526\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 843\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 844\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129639  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108397\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176875\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 845\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103618\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 846\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109764\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197888\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 847\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089959  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109376\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172961\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 848\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100071  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106432\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 849\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083751  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172853\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 850\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118397\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 851\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123410\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 852\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128607  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115449\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 853\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110223\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 854\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141438  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105992\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 855\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085021  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135111\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190804\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 856\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154239  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 857\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110791  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183041\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 858\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127603  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108460\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180380\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 859\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109253\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.186122\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 860\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118911  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 861\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134353  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 862\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095747  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 863\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107247\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 864\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 865\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123679\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112010\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 866\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111024  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172783\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 867\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088346  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103992\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 868\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096504  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106369\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 869\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135533  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121057\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167063\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 870\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086385  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 871\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114145\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 872\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118071\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 873\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091463  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169795\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257647\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 874\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104516  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124948\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 875\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103724  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128781\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169917\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 876\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094527  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 877\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105978  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108809\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183365\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 878\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116143\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 879\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144294  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114706\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 880\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114154  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105710\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.171953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 881\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118062\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179525\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 882\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107152  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166455\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 883\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129973  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181129\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 884\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124411  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116712\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108221\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170820\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 885\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077488  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 886\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142406  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118347\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206347\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 887\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102634\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 888\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121524  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110579\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117608\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 889\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138091  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117213\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 890\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106307  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119105\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 891\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114045  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104418\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 892\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078488  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173745\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 893\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067196  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104136\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166436\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 894\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087413  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113860\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176879\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 895\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074961  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175020\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 896\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098064  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 897\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073022  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 898\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118941\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 899\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081947  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207912\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 900\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098134  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.233735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 901\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123163  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130916\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.209768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 902\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126963  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106402\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 903\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121618  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117292\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 904\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152599  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 905\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140786  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107681\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 906\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081005  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 907\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 908\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119930  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109308\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104320\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 909\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105600  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 910\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106353\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102813\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 911\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162576  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196483\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 912\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138549  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 913\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072681  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112024\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 914\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127368\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102315\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167491\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 915\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109231  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110246\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168676\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 916\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074336  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 917\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127801\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126047\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 918\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079445  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105940\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 919\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138071  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103801\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 920\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099675  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113880\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175240\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 921\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092433  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108696\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165168\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 922\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101955\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.168233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 923\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081349  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180810\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 924\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079543  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172632\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 925\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091064  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 926\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100959\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 927\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110090  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190508\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 928\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115650  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 929\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115317\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102277\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 930\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083724  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112833\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 931\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110178\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105588\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167355\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 932\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 933\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113242\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168887\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 934\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105004  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180976\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 935\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124687  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 936\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107203\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101746\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 937\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102585\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 938\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111795\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164948\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 939\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 940\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110676  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114695\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.111304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 941\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170558\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 942\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104282  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.135769\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.212312\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 943\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104901  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114666\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110710\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.183595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 944\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086259  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127652\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.212922\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 945\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160357  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141877\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 946\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133912  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148355\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228220\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 947\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116973  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123347\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103088\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 948\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109744\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173938\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 949\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118797  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113183\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107441\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 950\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120355  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113583\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 951\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091979  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103354\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 952\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114844  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166538\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 953\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080442  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105844\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 954\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101755  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099840\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163125\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 955\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 956\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 957\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105424  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174599\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 958\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134012  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109260\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102882\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 959\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133304  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 960\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090407  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108105\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166259\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 961\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161593\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 962\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089098  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176378\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 963\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 964\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097244  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105512\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101557\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.167585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 965\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 966\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091953  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099693\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165089\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 967\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170402  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117578\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 968\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162124\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 969\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 970\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111671  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106168\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 971\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096583\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162991\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 972\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089242  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 973\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103760\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163166\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 974\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099429  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109087\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099962\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 975\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 976\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 977\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099454  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102582\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096613\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157136\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 978\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092299  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 979\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100611\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168342\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 980\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084171  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107278\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099912\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 981\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125411  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103476\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163987\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 982\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119326  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168529\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 983\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073552  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108697\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169689\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 984\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114473\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 985\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158432  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110627\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.170348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 986\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 987\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174399  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 988\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117564\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 989\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106358\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 990\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085577  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119456\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166426\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 991\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103312  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109772\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108350\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 992\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110048\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 993\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134895  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 994\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089923  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 995\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089853  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 996\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104639\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108428\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 997\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105843  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105512\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 998\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130312  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 999\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1000\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102287  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119002\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1001\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116041\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171359\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1002\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186992\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1003\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106404\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097847\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1004\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096715  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097304\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161906\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1005\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071047  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165287\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1006\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124032  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099363\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.158219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1007\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072335  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165193\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1008\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170676\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1009\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111996  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1010\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143817  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110692\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1011\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096231  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125997\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1012\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089872  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109199\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.161064\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1013\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115326\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1014\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110718\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1015\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087532  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162498\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1016\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102603\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100007\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164867\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1017\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124524  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105491\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105423\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168532\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1018\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088471  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1019\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093829  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104779\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1020\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110014  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098647\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165707\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1021\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091514  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168098\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1022\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093052  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105732\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1023\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120693  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151155\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1024\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133916  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1025\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059068  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103000\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158450\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1026\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093445  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102085\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.165081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1027\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122880\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179613\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1028\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097953  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1029\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106471  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098913\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1030\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115606  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125739\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198512\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1031\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1032\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102375  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106114\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095889\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1033\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100564  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164217\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1034\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123674  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1035\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114676  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162619\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1036\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064232  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1037\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088303  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1038\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1039\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079026  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102612\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1040\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153867  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1041\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095682  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099478\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1042\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134459  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1043\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101969\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1044\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123522  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099740\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161836\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1045\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068082  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100444\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097502\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1046\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105708\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107181\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.163072\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1047\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138333  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096323\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1048\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120010  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1049\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161148\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1050\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079368  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110752\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1051\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136280  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112281\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1052\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159883  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095643\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1053\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102981  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101748\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1054\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1055\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124019  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106055\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1056\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125398\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1057\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102813  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166538\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1058\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114752\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170229\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1059\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1060\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1061\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093587  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107190\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105926\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1062\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067394  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1063\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116898  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114968\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094522\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1064\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1065\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1066\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099519\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109095\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.180936\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1067\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1068\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091498  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114397\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171708\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1069\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097718  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105683\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176220\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1070\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110206  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102143\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102551\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167843\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1071\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142312  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1072\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095782  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110368\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1073\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144687  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1074\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100796\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157378\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1075\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114769  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103905\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106287\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175379\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1076\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104496\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1077\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1078\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093309  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100074\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093647\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1079\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079538  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101584\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1080\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094848  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107751\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1081\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091432  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097469\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1082\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099920\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093934\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1083\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081780  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1084\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164834\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1085\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108911\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1086\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116882  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102411\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.162451\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1087\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1088\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156012\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1089\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089856  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093363\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1090\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093839\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1091\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157392\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1092\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104183  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100786\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093663\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1093\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123572\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1094\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125647  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102676\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169012\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1095\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150704  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108349\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095850\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1096\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108178\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1097\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077405  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164927\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1098\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1099\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119924\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175603\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111962\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101157\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115445  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105112\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108904\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095254  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101074\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156520\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113673  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094645  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111654  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098746\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096327\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.160330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116403  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104928  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101807\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165425  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102286\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082001  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098066\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126990  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098001\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095619\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156108\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089683  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100868\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097751\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162754\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068326  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101633\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100427\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159186\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075114  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107679\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109493\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168827\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135923  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100817\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060663  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095900\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156450\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072494  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096967  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099066\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154894\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095410\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091378  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094762\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097595\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157486\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156324  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152525\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116334\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.185736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122144  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171359\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123657  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105585\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078826  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131436  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110248\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184188\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096705\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113882  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104512\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092769\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158094\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079383  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095538\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072612  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103966\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102746\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095811\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163317\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106384  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161328\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149423  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092670\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079624  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098835  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132125  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133928\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.187169\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119941  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100203\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152919\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096365\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065917  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079121  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156461\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082484  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098360\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124720\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196745\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081448  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091319\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072958  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111096\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106748\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080622  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092616\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122773  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092504\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117584\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145212  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089862  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097213\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097690\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156096\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094451\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193158\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162688\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143886  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.149616\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.207340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155725  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.195122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247151\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.251965  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127739\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095014\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093562  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099584\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083999  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099089\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154644\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095826\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103290\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075104  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106021\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156073\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078829  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091101\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098128\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099008  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135392  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099596\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113500\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186988\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100582\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094446\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159754\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098192  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098484\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092283\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065195  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096645\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094187\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156820\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091570  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159105\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113010  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152333  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106261\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106116  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109830\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130647\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103653  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158572\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065871  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118910\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.173589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093499\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109952  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086058  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108230\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098504\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157406\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1190\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106297  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154600\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076654  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180986\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1193\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121479  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161559\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1194\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131393  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1195\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114353\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100921\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1196\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114681  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107905\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132318\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1197\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126722  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1198\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1199\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098834  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.125365\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1200\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165358  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105501\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100592\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158003\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1201\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105600\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1202\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147513  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1203\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106939\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1204\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141792  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090737\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1205\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100946\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100446\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165383\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1206\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091088  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107299\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104772\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.163762\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136694  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099234\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095642\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1208\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088680  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098097\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1209\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157860\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1210\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097713  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105547\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1211\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095902  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095379\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1212\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097819  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091959\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1213\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1214\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1215\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107973  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105291\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093537\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1216\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100261\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104846  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126992\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1218\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104292\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165440\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1219\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153770\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1220\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070279  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094652\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1221\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092799  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092376\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1222\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097481  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157378\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1223\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125033  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1224\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079560  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108611\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176404\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1225\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097986\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1226\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114850  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090326\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.152564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1227\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077128  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1228\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090391\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149581\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1229\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107050  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097147\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1231\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124361  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098675\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.138518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192440\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1232\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162984  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153133\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1233\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107810  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154266\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1234\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093816\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1235\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096612  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095020\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1236\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113117  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1237\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137496  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1238\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081850  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096750\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155305\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1239\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091984  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097758\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093322\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1240\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095276  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1241\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080693  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097251\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100824\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1242\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095183\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1243\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115917  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092600\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1244\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093627\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090689\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1245\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098952\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101428\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1246\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101739  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101433\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.158003\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1247\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069109  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096829\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147769\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1248\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109309  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1249\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083693  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098768\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091734\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1250\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108650  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089471\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154361\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1251\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124971  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169170\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1252\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132349  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130580\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186888\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1253\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125368  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1254\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107367  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118766\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1255\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1256\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183498\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144110  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158696\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102105\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165316\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1259\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107755  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091702\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103938  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103349\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162639\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1261\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115447  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104224\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159979\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1262\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075058  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1263\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062667  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1264\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111776  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109622\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175482\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1265\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092306\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1266\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069107  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094263\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.150809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1267\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094986\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1268\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128514  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156392\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132859  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089622\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078038  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150951\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1271\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148412\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1272\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063637  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096729\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1273\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133607  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1274\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080899  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096611\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1275\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115089  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092860\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153670\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1276\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095357  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152961\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1277\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1278\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167686  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099670\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172925\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1279\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099125  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099313  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107892\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160196\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1281\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1282\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123711  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1283\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125568  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130430\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.153889\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208102\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1284\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136880\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120311\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1285\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101999  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153269\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1286\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068345  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099822\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093222\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.150947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1287\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099844  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093969\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156384\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1288\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104845  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088647\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1289\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079839  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095646\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156287\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1290\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096011\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090007\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1291\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1292\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124876  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091141\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1293\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1294\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076873  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1295\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087715  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095224\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1296\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1297\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097326  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110326\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1298\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079560  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099433\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090804\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1299\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096369  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088411\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1300\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085495  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1301\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091657  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1302\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142743  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093756\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088962\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149269\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1303\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108759  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098176\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1304\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092204\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1305\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066161  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108412\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155184\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1306\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103410  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092095\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.158547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1307\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097255  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106495\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1308\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127029  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1309\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099947\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155731\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1310\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100769\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1311\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095495  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099890\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093049\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1312\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091419  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156920\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1313\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117298  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1314\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117459  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108690\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1315\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091412  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101048\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1316\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126445  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164106\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133372  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100132\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108529\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1318\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096711\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150200\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1319\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139482  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094515\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1320\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.152928\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1321\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.163846  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102224\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092358\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157148\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1322\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095893  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159445\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.234250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1323\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133659  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094240\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1324\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069987  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096988\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1325\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096981  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1326\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099910\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090247\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.151211\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1327\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099315\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1328\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1329\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1330\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098972\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1331\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101234\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100830\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160650\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1332\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141863  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098946\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089908\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1333\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100870\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092801\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1334\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065955  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182156\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1335\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104348  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104183\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1336\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092145\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1337\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103690  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118524\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188343\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1338\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132381  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106875\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089799\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147125\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1339\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158379\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108429  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100376\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157380\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1341\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091653  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109132\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1342\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.125134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152279\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1343\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090950\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1344\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088158  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1345\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082932  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102907\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093663\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1346\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091323\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.157178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1347\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167867\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1348\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112480  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108529\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174440\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1349\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097831\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1350\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103347\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1351\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153836  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095589\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1352\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110888  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1353\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104387\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156462\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1354\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105678\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107717\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154088\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1356\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098496  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095427\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1357\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092448  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167599\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1358\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153095\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1359\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127278  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1360\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148023\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1361\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082871  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092941\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1362\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113868  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1363\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089915\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152111\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1364\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063852  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096074\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152186\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1365\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111125  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1366\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096593  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089164\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.148216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1367\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087774  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088949\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151023\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1368\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049491  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112092\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1369\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124837  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1370\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1371\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122699\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1372\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094233\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1373\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063476  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151395\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1374\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110546  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1375\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160154  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153206\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1376\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141226  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089252\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1377\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068652  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092378\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150733\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1378\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076959  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149584\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1379\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093558\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1380\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099556\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1381\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094330\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089142\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145918\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1382\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112555  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154985\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1383\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120437  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1384\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120300  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094397\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152949\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1385\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107261\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1386\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113678  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089192\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.149964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1387\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1388\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082485  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1389\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090014  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1390\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092273  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149303\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1391\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145860\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1392\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.193027  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1393\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1394\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078215  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165433\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1395\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099765\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1396\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106060  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116186\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1397\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090850  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117770\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1398\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165410  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1399\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086396  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093867\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1400\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067595  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096616\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097633\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1401\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094402  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097416\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158390\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1402\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070449  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1403\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128147  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098602\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1404\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073822  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092967\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154486\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1405\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178498\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1406\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089461\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.149561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1407\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092385\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1408\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073469  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088211\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1409\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093582  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095952\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.113171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1410\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119330  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120613\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1411\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117162\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091923\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1412\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096120  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100913\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114278\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1413\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102855\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094111\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1414\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087674  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098644\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102587\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1415\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087078  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090778\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147804\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1416\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094087\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1417\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101128  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179524\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1418\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152916  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149364\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1419\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110987  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090423\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149088\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1420\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075873  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1421\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078718  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1422\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167103\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1423\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098021\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1424\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1425\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089253\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1426\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083574  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088381\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.147737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1427\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071932  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110009\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156410\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1428\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116877  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1429\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097783  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092622\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102522\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1430\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097150  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098407\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1431\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121329  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089172\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1432\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093196\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102851\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1433\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113232  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094931\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150679\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1434\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066586  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092788\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152036\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1435\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113238  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094688\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1436\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1437\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099225  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095816\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1438\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105499  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090493\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148416\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1439\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093640\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151931\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1440\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076790  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092052\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1441\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071344  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097552\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.106706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1442\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086986  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1443\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093418  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087428\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1444\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079763  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094499\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147250\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1445\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157343\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1446\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068340  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102351\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.161117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1447\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102175  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100779\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164793\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1448\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097670  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097319\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1449\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162879\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1450\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107706\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160830\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1451\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072534  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174394\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1452\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108391  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1453\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063117  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096430\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1454\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095849\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1455\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072914  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090930\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1456\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1457\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140286  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090848\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153886\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1458\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066919  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095112\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089189\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151483\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1459\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091150\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1460\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092646  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1461\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074106  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1462\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082602  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097650\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.123743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194961\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1463\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103663  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1464\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149371\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1465\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108855  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093655\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088609\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149854\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1466\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061391  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093358\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102536\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.166387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1467\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098177\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1468\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096179\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1469\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063486  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1470\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.168858  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1471\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125334  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177551\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1472\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105746  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111167\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.119234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1473\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134647  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119493\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1474\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109580  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102892\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088950\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147134\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1475\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094861\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149478\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1476\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064819  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066868  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1478\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093491  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149430\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1479\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098662  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147818\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1480\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061611  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096216\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1481\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130751  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1482\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077447  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1483\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093319  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089665\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088092\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1484\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062195  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1485\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114191  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.094415\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156116\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1486\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128140  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120143\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.184869\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1487\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1488\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103862\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1489\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119971  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1490\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1491\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083475  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.116103\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1492\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141000  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1493\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085460  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1494\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072816  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087515\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1495\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101374  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150124\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1496\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082426  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159185\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1497\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073376  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1498\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078526  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146442\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1499\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084761  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089865\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.089478\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1500\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151227\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seperator()\n",
    "print(\"model architecture\")\n",
    "seperator()\n",
    "print(model)\n",
    "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses_live_l2 = []\n",
    "trn_losses_l2 = []\n",
    "val_losses_l2 = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    seperator()\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    seperator()\n",
    "    trn_loss_live = train_epoch(trn_dataloader, model, loss_fn, optimizer)\n",
    "    trn_losses_live_l2.append(trn_loss_live)\n",
    "    seperator()\n",
    "    trn_loss = trn_pass(trn_dataloader, model, loss_fn)\n",
    "    trn_losses_l2.append(trn_loss)\n",
    "    seperator()\n",
    "    val_loss = val_pass(val_dataloader, model, loss_fn)\n",
    "    val_losses_l2.append(val_loss)\n",
    "    seperator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "c3 = 'tab:orange'\n",
    "\n",
    "axs.plot(trn_losses_l2, label=\"train loss\", color=c1)\n",
    "axs.plot(val_losses_l2, label=\"val   loss\", color=c2)\n",
    "axs.plot(trn_losses_live_l2, label=\"live train loss\", color=c3)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"epoch\")\n",
    "axs.set_ylabel(\"MSE\")\n",
    "\n",
    "axs.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the L2 regularisation has worked in reducing the over-fitting!\n",
    "\n",
    "L1 regularisation should have a similar effect, try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "As we saw in one of the previous plots in a training without any regularization the validation loss typically stops decreasing after a while (or in the worst case even starts increasing) while the training loss keeps on decreasining. The idea of Early Stopping is to simply stop the training when this happens.\n",
    "\n",
    "------------\n",
    "Copy paste again.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a useful function to present things clearer\n",
    "def seperator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "# define parameters\n",
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50\n",
    "batch_size = 64\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp.to(device), trn_amplp.unsqueeze(-1).to(device))\n",
    "val_dataset = amp_dataset(val_datfp.to(device), val_amplp.unsqueeze(-1).to(device))\n",
    "tst_dataset = amp_dataset(tst_datfp.to(device), tst_amplp.unsqueeze(-1).to(device))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 1500\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "hdn_dim = 50\n",
    "model = amp_net(hdn_dim=hdn_dim).to(device) # model without Dropout!\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trigger condition can be arbitrarily complicated but in its simpliest version we just compare the current value of the validation loss with the previous value. However, because the validation dataset is typically small and the validation loss therefore prone to statistical fluctuations, we should not immediately stop the training the first time the breaking condition is triggered. Instead, we introduce the new hyperparameter 'patience' which tells us how many epochs we want to wait before stopping the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture\n",
      "-----------------------------------------------\n",
      "amp_net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 8751 trainable parameters\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 215.709625  [    0/ 1000]\n",
      "avg train loss per batch in training: 215.415174\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 213.748549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 215.699629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 213.350677  [    0/ 1000]\n",
      "avg train loss per batch in training: 212.066108\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 208.935191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 210.739919\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 208.247589  [    0/ 1000]\n",
      "avg train loss per batch in training: 203.307487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 193.663724\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 195.010308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 197.185837  [    0/ 1000]\n",
      "avg train loss per batch in training: 174.717442\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 143.467213\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 143.511059\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 142.403824  [    0/ 1000]\n",
      "avg train loss per batch in training: 100.174035\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 50.856661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 50.878791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 45.056538  [    0/ 1000]\n",
      "avg train loss per batch in training: 46.621802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 40.734594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 45.686957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 71.709679  [    0/ 1000]\n",
      "avg train loss per batch in training: 38.041324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 36.109696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 37.674430\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 29.245129  [    0/ 1000]\n",
      "avg train loss per batch in training: 34.109169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 32.804916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 35.261358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 35.690392  [    0/ 1000]\n",
      "avg train loss per batch in training: 31.563272\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 30.533057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 33.140580\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 38.401939  [    0/ 1000]\n",
      "avg train loss per batch in training: 30.177028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 28.897558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 31.561220\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 23.698305  [    0/ 1000]\n",
      "avg train loss per batch in training: 28.272331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 27.284870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 29.326274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 30.328203  [    0/ 1000]\n",
      "avg train loss per batch in training: 26.472935\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 25.608381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 27.784788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 29.216511  [    0/ 1000]\n",
      "avg train loss per batch in training: 25.322155\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 24.276171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 26.510799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 21.176123  [    0/ 1000]\n",
      "avg train loss per batch in training: 23.797689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 22.637823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 24.523533\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 24.356115  [    0/ 1000]\n",
      "avg train loss per batch in training: 22.067733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 20.913759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 23.037453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 22.628561  [    0/ 1000]\n",
      "avg train loss per batch in training: 20.576287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 19.248072\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 20.877908\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.063210  [    0/ 1000]\n",
      "avg train loss per batch in training: 18.708606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 17.323388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 18.960075\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 13.826763  [    0/ 1000]\n",
      "avg train loss per batch in training: 16.880054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 15.460407\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 16.602522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 17.224266  [    0/ 1000]\n",
      "avg train loss per batch in training: 14.357862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 13.013869\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 14.352424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 9.037001  [    0/ 1000]\n",
      "avg train loss per batch in training: 12.038972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 10.952696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 11.907240\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 16.831869  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.777106\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 8.443712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 9.380938\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.121973  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.465512\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.230661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 6.758007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.411791  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.427986\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.377830\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.710998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 7.261413  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.770676\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.947820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.335120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.840941  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.601503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.198701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.472415\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.979059  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.960571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.700754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.979399\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.118496  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.721472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.486106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.653176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.507142  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.373257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.250084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.442380\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.156131  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.184860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.064197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.234398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.242609  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.028311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.961373\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.113062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.994503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.955672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.851632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.012597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.601035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.843727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.783192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.940460\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.979063  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.770072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.742926\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.896829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.978066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.719599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.690078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.823026\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.650291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.668635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.647171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.818194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.684153  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.625209\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.625355\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.759678\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.550368  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.608282\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.584706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.733387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.511842  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.587270\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.586691\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.721133\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.648695  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.554122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.544114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.689303\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.540479  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.533034\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.543291\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.689518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.965675  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.541159\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.493169\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.660178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.552223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.514829\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.477960\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.667571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.662960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.487861\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.463723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.645218\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.443779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.464361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.450566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633302\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.458274  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.456982\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.439948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.392643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.456054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.433489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.622310\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.520933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.441198\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.430793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.623592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.526607  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.447463\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.422758\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.622828\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.304945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.426998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.427590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636760\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.489240  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.418779\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.428339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.593425\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 51\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.517061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.426845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.403968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.584314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 52\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.452581  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.409591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.400541\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.603502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 53\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.519406  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.429384\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.466294\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.609966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 54\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.342005  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.415358\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.398026\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.572628\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 55\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.462371  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.398657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.383026\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.577716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 56\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.341217  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.389198\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.385828\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.603369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 57\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.377386  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.421273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.527140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.794910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 58\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.438700  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.506547\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375768\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.585396\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 59\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.586325  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.404603\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.374581\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.552313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 60\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.323554  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.370027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.561187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 61\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.363136  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.365784\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.357072\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.562667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 62\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.403430  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.372850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.358225\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.549484\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 63\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.439587  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.363843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.356998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.588485\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 64\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.261162  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.364430\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.350307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.547885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 65\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.398093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.358601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.374790\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.545324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 66\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.375190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.376990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348467\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.565907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 67\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.487346  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.375324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.375492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.550090\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 68\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.383532  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.373642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.376949\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.627980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 69\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.324022  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.351417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331622\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.541240\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 70\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.329278  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.345022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334294\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.539428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 71\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.311996  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.339688\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.555030\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 72\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.392803  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.338684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.532503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 73\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.415022  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.341645\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.524417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 74\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.380006  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.331561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.550614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 75\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.228147  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.332099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.321130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.533417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 76\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.298311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.331758\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.318251\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.529821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 77\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.239781  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.325176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.317531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.522741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 78\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.292710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.323187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314185\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.548871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 79\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.238625  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.323648\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312223\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.519340\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 80\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.217880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.316787\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.314177\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.551336\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 81\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.236263  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.313580\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.309776\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.539860\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 82\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.328100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.315152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.310784\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.508461\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 83\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.333167  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.319992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.304882\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.554421\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 84\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.263034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.310157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.298930\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.511463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 85\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.426781  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.311402\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.516516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 86\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.360008  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.300643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.517233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 87\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.317042  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.297702\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.291639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.511189\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 88\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.255579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.295800\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.293140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.503145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 89\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.325925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.290363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.284796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.512113\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 90\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.254991  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.292781\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.315506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.576059\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 91\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.304309  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.310115\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.292155\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.503705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 92\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152313  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.303677\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288464\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.499446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 93\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.261048  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.293880\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.288419\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.536401\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 94\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.223662  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.292884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.302017\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.511282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 95\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.371995  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.297604\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.297291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.553465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 96\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.387290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.284899\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.278241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.497421\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 97\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.243080  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.284471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.275893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.514893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 98\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.250902  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.279917\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.296085\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.552904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 99\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.273703  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.288154\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.496721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.227775  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.271741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.274270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.494052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.275714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.279357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.269219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.493552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.255920  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.271177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.507907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.270422  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.272742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273112\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.485997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.346981  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.282323\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262432\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.492152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.284109  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.273240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.287139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.556702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219149  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.268431\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.260395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.504675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.386428  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.268367\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.277753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.493935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.209732  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.274431\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257369\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.496258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.279536  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.256839\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.488725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.240383  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.266587\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.489031\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.223253  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.277648\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.255626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.485479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.336762  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.261770\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250640\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.484833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.235739  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.258570\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.484035\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.225556  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.256118\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.492348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.281896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.255257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.259314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.476178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.250927\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249850\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.499466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.252017  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.267694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.270916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.486277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.269153  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.253349\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.248453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.500746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.252543  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.254347\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.474960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.249053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.244572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.239231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.485307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174651  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.251131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266656\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.536360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.240460  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.260342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.241158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.471117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.377826  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.241975\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.238653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.487024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.217364  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.243859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235598\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.488620\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.247492  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.242249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.240181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.474793\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.209977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.252850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.488256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.284682  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.245165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.471636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.254066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.240699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.477788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.180206  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.239973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.231066\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.477442\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.245679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.247992\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.485743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.243939  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.248061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232787\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.480299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.235454\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.478638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206698  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.235237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.227715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.469307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.298184  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.238362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.249621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.466194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.257571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.243268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.236405\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.495889\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155395  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.240577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.224188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.460219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145874  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.240065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.228654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.469463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.236861  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.232196\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.223167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.459390\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226434  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.232445\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220683\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.462238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.231486\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.221952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.461876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.281885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.257281\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.540078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232411  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.251995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.219199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.468710\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.230823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.216438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169316  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.219094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.450849\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.257605  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.222840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.212260\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.458768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.244481  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.226263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210909\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.454514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.217749\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.461902\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.288175  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.214604\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209165\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.457387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139150  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.216099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.210521\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.474216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.272672  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212155\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.207634\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.462069\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.202927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.214666\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.206096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.451370\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.194629  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205957\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.463552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197784  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.221464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.202395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444972\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125762  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.221152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.206497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.450497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.223869  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.213792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.456289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152181  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.198218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444787\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219314  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.213417\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.273298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.476822\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.235518\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203714\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.444751\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158599  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.213750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.466937\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.440159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158856  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.202175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.447718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149941  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.209119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.439045\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.168105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.195164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.445349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157706  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.220985\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.440476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.239517  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.203871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.203130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.443255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.269458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.208102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193268\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.445974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.197544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.189226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.428591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197019  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188063\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.447556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.211981  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.195660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.475630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.193813  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196570\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.186555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.452942\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.195312\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.441625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159019  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.195827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.439001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.193563\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181609\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.194159  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.185142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.180552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.426479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.186269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429875\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.199165  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183660\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.436607\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197433  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.181234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.425161\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.192927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.204327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429220\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.200462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.179086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431954\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203312  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.184995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.192359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.460721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170039  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187937\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.184503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.457373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171946  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183892\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.193061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.466589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.253935  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179780\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.428922\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.238768  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.173164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.427691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152986  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177092\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.417647\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157512  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170940\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419224\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.192679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181125\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178847\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.421500\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.170460\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.433837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128552  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.170125\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.178917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.450055\n",
      "EarlyStopping got triggered! Stop training!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seperator()\n",
    "print(\"model architecture\")\n",
    "seperator()\n",
    "print(model)\n",
    "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses_live_es = []\n",
    "trn_losses_es = []\n",
    "val_losses_es = []\n",
    "\n",
    "# patience of EarlyStopping in number of epochs\n",
    "patience = 4\n",
    "\n",
    "# set counter to 0 for early stopping\n",
    "trigger_counter = 0\n",
    "val_loss_last = 1e10\n",
    "\n",
    "for t in range(epochs):\n",
    "    seperator()\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    seperator()\n",
    "    trn_loss_live = train_epoch(trn_dataloader, model, loss_fn, optimizer)\n",
    "    trn_losses_live_es.append(trn_loss_live)\n",
    "    seperator()\n",
    "    trn_loss = trn_pass(trn_dataloader, model, loss_fn)\n",
    "    trn_losses_es.append(trn_loss)\n",
    "    seperator()\n",
    "    val_loss = val_pass(val_dataloader, model, loss_fn)\n",
    "    val_losses_es.append(val_loss)\n",
    "    \n",
    "    #######################\n",
    "    # early stopping\n",
    "    if val_loss > val_loss_last:\n",
    "        trigger_counter += 1\n",
    "    else:\n",
    "        trigger_counter = 0\n",
    "        \n",
    "    if trigger_counter >= patience:\n",
    "        print(\"EarlyStopping got triggered! Stop training!\")\n",
    "        break\n",
    "    \n",
    "    val_loss_last = val_loss\n",
    "    #######################\n",
    "    \n",
    "    seperator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "c3 = 'tab:orange'\n",
    "\n",
    "axs.plot(trn_losses_es, label=\"train loss\", color=c1)\n",
    "axs.plot(val_losses_es, label=\"val   loss\", color=c2)\n",
    "axs.plot(trn_losses_live_es, label=\"live train loss\", color=c3)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"epoch\")\n",
    "axs.set_ylabel(\"MSE\")\n",
    "\n",
    "axs.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the network has stopped training before the training loss diverges too far from the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalisation is not a method to prevent over-fitting, so we do not expect it to do that.  The point of batch normalisation is to make the training more efficient.  It's difficult to see the benefits of this regularisation method with the dataset we're using here, although it is still useful to learn how to implement it.  It works by normalising the data as it passes between the layers, keeping the distribution (i.e. mean and variance) of the data constant throughout training.  This means that the weights in a network layer do not have to adapt to simple changes in the distribution of the data in a previous layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class amp_net_bn(nn.Module):\n",
    "\n",
    "    # default hdn_dim is 30, but can be changed upon initialisation\n",
    "    def __init__(self, hdn_dim=30):\n",
    "\n",
    "        super(amp_net_bn, self).__init__()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ipt_dim, hdn_dim),\n",
    "            nn.BatchNorm1d(hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.BatchNorm1d(hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.BatchNorm1d(hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, hdn_dim),\n",
    "            nn.BatchNorm1d(hdn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hdn_dim, opt_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a useful function to present things clearer\n",
    "def seperator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "# define parameters\n",
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50\n",
    "batch_size = 64\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp.to(device), trn_amplp.unsqueeze(-1).to(device))\n",
    "val_dataset = amp_dataset(val_datfp.to(device), val_amplp.unsqueeze(-1).to(device))\n",
    "tst_dataset = amp_dataset(tst_datfp.to(device), tst_amplp.unsqueeze(-1).to(device))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)\n",
    "epochs = 1500\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "hdn_dim = 50\n",
    "model_bn = amp_net_bn(hdn_dim=hdn_dim).to(device)\n",
    "learning_rate = 5e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model_bn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture\n",
      "-----------------------------------------------\n",
      "amp_net_bn(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=50, bias=True)\n",
      "    (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 9151 trainable parameters\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 212.375000  [    0/ 1000]\n",
      "avg train loss per batch in training: 214.656414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 214.174937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 216.414281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 201.443359  [    0/ 1000]\n",
      "avg train loss per batch in training: 207.566393\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 207.252019\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 209.330189\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 205.033386  [    0/ 1000]\n",
      "avg train loss per batch in training: 200.942711\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 199.095222\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 201.015245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 200.655487  [    0/ 1000]\n",
      "avg train loss per batch in training: 194.809007\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 192.119080\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 193.811966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 186.308716  [    0/ 1000]\n",
      "avg train loss per batch in training: 188.916632\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 186.704892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 188.507913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 186.095459  [    0/ 1000]\n",
      "avg train loss per batch in training: 182.938658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 180.629340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 182.592252\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 177.967621  [    0/ 1000]\n",
      "avg train loss per batch in training: 176.892238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 174.230875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 175.954898\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 173.682556  [    0/ 1000]\n",
      "avg train loss per batch in training: 171.175824\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 169.238008\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 170.995496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 167.088165  [    0/ 1000]\n",
      "avg train loss per batch in training: 164.826267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 162.640244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 164.424003\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 157.042297  [    0/ 1000]\n",
      "avg train loss per batch in training: 158.778770\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 156.220053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 157.908097\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 157.440613  [    0/ 1000]\n",
      "avg train loss per batch in training: 152.170043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 149.109534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 150.902314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 143.439316  [    0/ 1000]\n",
      "avg train loss per batch in training: 145.838230\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 143.743244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 145.752336\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 146.887726  [    0/ 1000]\n",
      "avg train loss per batch in training: 139.444184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 136.879259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 138.720978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 141.002167  [    0/ 1000]\n",
      "avg train loss per batch in training: 132.666523\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 131.032904\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 132.709511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 130.604324  [    0/ 1000]\n",
      "avg train loss per batch in training: 126.168179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 124.389987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 126.291485\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 126.535126  [    0/ 1000]\n",
      "avg train loss per batch in training: 119.416173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 117.346505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 119.403815\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 114.386139  [    0/ 1000]\n",
      "avg train loss per batch in training: 112.885238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 111.129472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 113.077749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 109.727570  [    0/ 1000]\n",
      "avg train loss per batch in training: 106.337462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 103.572207\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 105.630210\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 107.316338  [    0/ 1000]\n",
      "avg train loss per batch in training: 99.966396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 97.524246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 99.481631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 99.696457  [    0/ 1000]\n",
      "avg train loss per batch in training: 93.581184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 90.606887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 92.452055\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 88.602585  [    0/ 1000]\n",
      "avg train loss per batch in training: 86.966983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 84.833097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 86.746109\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 84.123245  [    0/ 1000]\n",
      "avg train loss per batch in training: 80.525957\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 78.842703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 80.630784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 77.632919  [    0/ 1000]\n",
      "avg train loss per batch in training: 74.592962\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 73.065943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 75.059608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 70.439346  [    0/ 1000]\n",
      "avg train loss per batch in training: 69.150951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 66.680806\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 68.543654\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 67.577553  [    0/ 1000]\n",
      "avg train loss per batch in training: 63.049380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 61.319259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 63.203504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 62.611115  [    0/ 1000]\n",
      "avg train loss per batch in training: 57.810529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 57.026157\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 58.820299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 54.747665  [    0/ 1000]\n",
      "avg train loss per batch in training: 52.504648\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 51.425534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 53.095048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 50.771400  [    0/ 1000]\n",
      "avg train loss per batch in training: 47.393322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 46.719301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 48.589115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 47.284485  [    0/ 1000]\n",
      "avg train loss per batch in training: 43.041693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 42.797585\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 44.450977\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 39.337090  [    0/ 1000]\n",
      "avg train loss per batch in training: 38.553283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 36.777570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 38.283791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 37.784187  [    0/ 1000]\n",
      "avg train loss per batch in training: 34.444596\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 32.827551\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 34.459878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 33.952927  [    0/ 1000]\n",
      "avg train loss per batch in training: 30.638502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 30.432919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 31.753209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 27.278065  [    0/ 1000]\n",
      "avg train loss per batch in training: 27.074849\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 25.907313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 27.303638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 24.787130  [    0/ 1000]\n",
      "avg train loss per batch in training: 23.920966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 23.426270\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 24.749858\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 26.062931  [    0/ 1000]\n",
      "avg train loss per batch in training: 21.181525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 20.217594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 21.321157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 19.680855  [    0/ 1000]\n",
      "avg train loss per batch in training: 18.182327\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 17.609426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 18.521286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 17.782093  [    0/ 1000]\n",
      "avg train loss per batch in training: 15.644755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 15.091974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 16.090004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 14.568785  [    0/ 1000]\n",
      "avg train loss per batch in training: 13.635225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 13.415904\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 14.291013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 12.206768  [    0/ 1000]\n",
      "avg train loss per batch in training: 11.624485\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 11.068156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 11.876040\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 10.274746  [    0/ 1000]\n",
      "avg train loss per batch in training: 9.762814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 9.633220\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 10.444033\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.758709  [    0/ 1000]\n",
      "avg train loss per batch in training: 8.589790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 8.518238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 9.208690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 8.362661  [    0/ 1000]\n",
      "avg train loss per batch in training: 7.392637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 7.335387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 8.080165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.164795  [    0/ 1000]\n",
      "avg train loss per batch in training: 6.051859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 6.143224\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 6.824519\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 5.482969  [    0/ 1000]\n",
      "avg train loss per batch in training: 5.550859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 5.336508\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.966569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.890069  [    0/ 1000]\n",
      "avg train loss per batch in training: 4.729035\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.771856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 5.413531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.202340  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.879626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.347889\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.825656\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.657777  [    0/ 1000]\n",
      "avg train loss per batch in training: 3.131698\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.802132\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 3.214592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.316290  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.762014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.387700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.838187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.280061  [    0/ 1000]\n",
      "avg train loss per batch in training: 2.305056\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.325302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.771824\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.892733  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.966461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.610995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.021721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 51\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.300776  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.790105\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.691433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.041362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 52\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.839099  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.342975\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.307651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.668243\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 53\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.281525  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.161792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.067698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.354555\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 54\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.649861  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.271576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.011552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.378835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 55\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.928932  [    0/ 1000]\n",
      "avg train loss per batch in training: 1.033895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.031435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.337971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 56\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.661238  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.897429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.679561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.935132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 57\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.666984  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.853297\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.886657\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 58\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.883578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.967762\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.679164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.994794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 59\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.735687  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.706936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.514524\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.748343\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 60\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.656488  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.663489\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.473078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.688467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 61\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.603215  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.758721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.448212\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.678170\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 62\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.543408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.634017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.411715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.646081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 63\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.481186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.596102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348565\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.586352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 64\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.597459  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.637040\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.409289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.642704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 65\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.480369  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.615728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.379141\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.622681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 66\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.275140  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.474277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346983\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.562796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 67\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.794363  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.547738\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328383\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.542990\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 68\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.713051  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.640848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.305564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.522766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 69\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.535377  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.482728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.272190\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.494313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 70\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.652719  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.539087\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.325436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.546960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 71\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.503083  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.479210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.282924\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.497205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 72\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.292414  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.492701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.359116\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.577197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 73\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.569584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.451865\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.262539\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.500073\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 74\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.381869  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.499352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.246874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.479244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 75\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.450174  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.432742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253038\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.486987\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 76\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.597575  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.415698\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.449249\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 77\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.288696  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.465361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.305991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.533181\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 78\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.235636  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.499709\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.584702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 79\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.359974  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.434699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.312481\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.493885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 80\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.386350  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.444520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.230118\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.438629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 81\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.260974  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.505199\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.261721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.453511\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 82\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.264705  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.463022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.234574\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.441141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 83\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.248570  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.411131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.213505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.421409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 84\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.377123  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.430717\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.212810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.413360\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 85\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.538472  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.348819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.419890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 86\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.312897  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.403894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.254067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.454571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 87\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.375318  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.372014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.237319\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429836\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 88\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.358753  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.372342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.197620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 89\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.247836  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.345425\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.216682\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.433982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 90\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.344771  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.393048\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.232456\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.427020\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 91\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.346303  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.356965\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.192464\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.398845\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 92\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.215232  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.365067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.200348\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399063\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 93\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.386768  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.370054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.222261\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.432504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 94\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.283740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.308912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.192736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.382424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 95\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.225592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.334260\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 96\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.292710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.355572\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.188575\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.406490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 97\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.350752  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.319346\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 98\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.262432  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.318135\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.208493\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411678\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 99\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.320281  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.339036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.196792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.396499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.375664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.333067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.205987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.400639\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.537542  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.371249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.185360\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.395730\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.411018  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.328201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.148288\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.340256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.322380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.176710\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.389402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224384  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.342808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.370504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.173093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.332624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.163465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.225912  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.358184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.213674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.411578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.310240  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.305901\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.365386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.335951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.334321\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.146405\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.307909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.320400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.365658\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.526127  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.306535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.176161\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.385393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.246282  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.299803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.244487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.308919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.139139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340239\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.330202  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.310189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.169044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.317626  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.283320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.183212\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.376843\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.212808  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.274996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.162901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.244417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.254907\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.145122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343982\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.311063  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.254722\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.151870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.271415  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262776\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.131520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.319761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.258809  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.261152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.136669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327547\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.329887  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.295771\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.147774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338622\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.234112  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.309569\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142151\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329770\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.189815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.259175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.144838\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.336246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.229425  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.264959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.146130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.356674\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.281217  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.254659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.124503\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.322984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.226586  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.243171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127652\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.302161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.160734\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.433912  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.260004\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.261173  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.303102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167002\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.367952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.318719  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.276441\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306908\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.249118\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.122900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.310406\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179381  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.295731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.194768\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.378398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.267025  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.285290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.134518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.321740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.217033  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.257545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.128027\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152917  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.262317\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.130069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.314394\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.251928  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.249230\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.357804  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.244226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.132078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325029\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.242747  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.222118\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114141\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.302666\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.236017  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.222674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.109993\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.304744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155435  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212141\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.177583\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.392535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.220451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.252181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133320\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.349274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.252253  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.291050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114131\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.214366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.229538\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117819\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.279121  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.233352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104817\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.294186\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.609196  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.269492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.140774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343789\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102202  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.204036\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110216\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154817  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.214527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.299230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.294068  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.115611\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.307991\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.259767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.233153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.141551\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.330476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.330144  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.208158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110110\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.292280\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.229091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.127665\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.312470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.430731  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.206129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.316380\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219805  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105374  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.247061\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.121719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.303584\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151037  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.234257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.129280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.301138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.235359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.227148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.117798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293798\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.436880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.258434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114649\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300103\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.175807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.225175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.189699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.223214\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114990\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.300836  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.245545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114524\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112824  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.202692\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.293391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232721  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.223272\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104822\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281977\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124165  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.193680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.298290\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.139092  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.281279\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.311771  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.239311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.120701\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.301324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.305640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.247354\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133674\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.317169\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.218891  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.195042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096164\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.270130\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.242712  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.217537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158463  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.208942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.210069\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.108320\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.282388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.317108  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119480  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.101396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.284123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.189318  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218123\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291632\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.206757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.297115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157548  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.227151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.110876\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.291517\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158610  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.196667\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151475  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.209529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100794\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132565  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.246853\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.278323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184582  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.213702\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.126902\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.300618\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144560  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096098\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154049  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092130\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133500  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.230201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.102182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272648\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.207260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.205701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.263816\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.186067  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.195464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103176\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.178630  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.165815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087308\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.263735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169376  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198115\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.258606\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.221282  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.212114\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087033\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.253784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232632  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084890\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.257322\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.107431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.276625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 190\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140153  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154204\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259109\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.285622  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.205240\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.105372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.272966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137458  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251988\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 193\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093331\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.273872\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 194\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.224686  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.199227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097080\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.266586\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 195\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.201761  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.197977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 196\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140335  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207468\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.118324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.283159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 197\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.251231  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.192237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.100910\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280458\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 198\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.244534  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.183187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.255337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 199\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.188028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.084487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.249913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 200\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.237086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.198562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.097119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.274227\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 201\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.236736  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.218069\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.112020\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.271779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 202\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.085259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 203\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078211\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.245245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 204\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164536  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.197605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.133621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.306858\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 205\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.213072  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.085125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254574\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 206\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.176632  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082643\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.243443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182668  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.209814\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.259064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 208\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117650  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.190895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 209\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172631  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177010\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 210\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089231  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.225625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086722\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250252\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 211\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.225288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.186881\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.277468\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 212\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.229423  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169900\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091780\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.260271\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 213\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121201  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.163914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.240414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 214\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162278  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182154\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083852\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252005\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 215\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.191831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.086030\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.256265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 216\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172452  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.176439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087694\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.248092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.212800  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 218\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110389  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071256\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 219\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.216682  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.268640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 220\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117698  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.167755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065160\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224430\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 221\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147182  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.144113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093011\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 222\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.155059  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173741\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.095247\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.279467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 223\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114167  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155436\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099135\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.280734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 224\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138595  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226172\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 225\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068040  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153334\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076699\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231880\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 226\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.315974  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182809\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.088941\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 227\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141818  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160693\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.085990\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.251833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 228\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119471  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.186347\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.083650\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 229\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118117  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.241024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208789  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064714\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226532\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 231\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.286267  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.230459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 232\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203157  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070156\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 233\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157570  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189192\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073876\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 234\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.296013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.184874\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064739\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 235\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112017  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226136\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 236\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150550  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.250196\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 237\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094196  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075289\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 238\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.210766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.173933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.232435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 239\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174641  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.180838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072683\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 240\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162327  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.205139\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.104856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.261313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 241\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.287507  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.187750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.247557\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 242\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.209982  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.181014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.096104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.254666\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 243\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103375  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137816\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.077312\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 244\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.196525  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.186074\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 245\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.166148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.091590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.243611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 246\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.280113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.175681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072437\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 247\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128576  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 248\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.169151  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.174109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.085397\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.242074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 249\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.158271  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.189950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.075485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227541\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 250\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068203\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 251\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.161789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 252\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112001  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071800\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 253\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.141036  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 254\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131887  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143901\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218562\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 255\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147697  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146220\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.229176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 256\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153253  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154711\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081936\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.251214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.178267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 259\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.174393  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156439\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.238294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167882  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.174043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.082339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.236165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 261\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.107365  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156450\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 262\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.125754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140234\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070068\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 263\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146983  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079773\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235188\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 264\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.179004\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 265\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144418  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.156506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067952\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 266\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146920  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142203\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209385\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 267\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155653\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059175\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207767\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 268\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161481  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.207256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.248860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.076891\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.230732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.181405  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.182836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066966\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 271\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172197  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.144175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.227178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 272\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.188166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157334\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200959\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 273\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077270  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066256\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 274\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119222  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.158396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068415\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 275\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091426  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081741\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.235885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 276\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.164703\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055332\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 277\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.182750  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 278\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134489  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 279\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.210861  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.191101  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.228502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 281\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095732  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209153\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 282\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114448  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205032\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 283\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.245411  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153463\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 284\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164320  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148125\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.219977\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 285\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167757  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.171037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 286\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.288258  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145553\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 287\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164001  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.087516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.244092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 288\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.179717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154585\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062500\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 289\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.205294  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060437\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 290\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112130  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.157569\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064072\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.214432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 291\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154260\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.224123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 292\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146760\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058011\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 293\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.156579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.137440\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213830\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 294\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203956  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149868\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 295\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162581  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060448\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 296\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.230295  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149486\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 297\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069839\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 298\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124747  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 299\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124594  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.174210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073928\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211945\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 300\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.205825  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.172422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063752\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200624\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 301\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093068  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124228\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 302\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143050  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.159044\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231035\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 303\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.188265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.160422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079432\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223532\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 304\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078073  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.073987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 305\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161653  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 306\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128354  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199680\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 307\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086941  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132081\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 308\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.207832  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.155464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 309\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144360  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 310\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120051  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.151053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 311\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152172  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.081978\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.230221\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 312\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106388  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 313\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150910  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189512\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 314\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062150  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.092731\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.252968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 315\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131958  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.169497\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070278\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 316\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.212560  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057535\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154654  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152689\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062301\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211453\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 318\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097643  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050082\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 319\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080537  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.139179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 320\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111984  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136853\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061595\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.211960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 321\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092501  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138810\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 322\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052656  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121394\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 323\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.157927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145070\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061670\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 324\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.219408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.152461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069365\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 325\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.153116\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058491\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202927\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 326\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092599  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 327\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136680  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071712\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 328\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116937  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130438\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 329\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.228304  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058482\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202273\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 330\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101554  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.134266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 331\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142709\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.070046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.217698\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 332\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097873  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.177650\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058755\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.202193\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 333\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117971  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055141\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 334\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080634  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.150314\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057643\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184056\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 335\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134570  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145252\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058808\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 336\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112049  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052771\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 337\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114073  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058318\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201626\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 338\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132859\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184242\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 339\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205488\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.217160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.145169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 341\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110704  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051239\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198722\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 342\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051127\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 343\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167792  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143139\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.074482\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.220362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 344\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109494\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.068871\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214816\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 345\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063098\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 346\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.147052  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.149443\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200335\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 347\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154211  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143541\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056580\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 348\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101822  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049872\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191657\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 349\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102879  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119360\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 350\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058006  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123604\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 351\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 352\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177731  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126433\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 353\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124118  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.147772\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052812\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.185086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 354\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074776  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.141220\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070433  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059134\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.202614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 356\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114390\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063601\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 357\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094253  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 358\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092634  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122796\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047428\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 359\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.186707  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118353\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 360\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072563\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 361\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.248001  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.146308\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 362\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091519  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 363\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150824  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106714\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183677\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 364\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 365\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084735  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051087\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 366\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100645  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060363\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.199496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 367\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073285  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 368\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087043  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041350\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 369\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130269  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187671\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 370\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051051  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103415\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049299\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 371\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060705  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054365\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193941\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 372\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.165059  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115247\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.064774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 373\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117201  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185607\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 374\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094557  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.154437\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044403\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.185863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 375\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112241\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182355\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 376\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103590  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042587\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188211\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 377\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.190984  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204847\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 378\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.138847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 379\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086067  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189102\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 380\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108979  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.143055\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056133\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 381\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132956  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.129498\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063457\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.215303\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 382\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.124948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109563\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 383\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.149236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.133998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044251\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186243\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 384\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078120  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042760\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 385\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085452  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050671\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190818\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 386\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111500\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.079697\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.225523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 387\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131188\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.093005\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.226875\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 388\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087862  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 389\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.159208  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.142024\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 390\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065583  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.135117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055840\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 391\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086217  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117391\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 392\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084439  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186195\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 393\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.136623  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.127371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 394\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065991  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103931\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037128\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 395\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087526  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096241\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042448\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.183039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 396\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091142  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100907\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 397\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091078  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.131853\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 398\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105036  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 399\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067365  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124084\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049939\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185673\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 400\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088009  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048649\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188697\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 401\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083924  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111032\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 402\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118892  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.140163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048967\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183721\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 403\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047372  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.132707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 404\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090171  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042079\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178603\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 405\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142705  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 406\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053443  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108883\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 407\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092936  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115135\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 408\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.208191  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.118423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054921\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186156\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 409\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092505  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124341\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061970\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196559\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 410\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.166993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.128072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200258\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 411\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.146810  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.148466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054909\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193778\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 412\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073262  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 413\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108127  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 414\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105819  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 415\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120192  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197755\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 416\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077238  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.071486\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.212549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 417\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194697\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 418\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047439\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 419\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180732\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 420\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112920  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108013\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041976\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173808\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 421\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 422\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090979  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055830\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 423\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.126627\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 424\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094047  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036612\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169870\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 425\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 426\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094676  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 427\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057447  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175970\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 428\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080656  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.136663\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192510\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 429\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185162\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 430\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111020  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183938\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 431\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123932  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046652\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184407\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 432\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045939\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189919\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 433\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123062  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052627\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200513\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 434\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066768  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105069\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041909\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 435\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 436\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092467  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 437\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148501  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109598\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039222\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.169872\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 438\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102430  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196243\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 439\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061875  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 440\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.233541  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055780\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198991\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 441\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.232242  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 442\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091217  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 443\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062500  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.115183\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 444\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.171762  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110268\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058611\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198478\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 445\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058924  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 446\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037039\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171150\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 447\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110067\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192045\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 448\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070653  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044267\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 449\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025706  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181056\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 450\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076973  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 451\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142030  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181986\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 452\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073859  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108800\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039524\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169601\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 453\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079647  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 454\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047789\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 455\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065687  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 456\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118024  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104212\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 457\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.172571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.123372\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055329\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 458\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055768\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.207208\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 459\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144902  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060181\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204338\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 460\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 461\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072439  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.144469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047072\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 462\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064444  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098737\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185352\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 463\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.122102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171336\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 464\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075478  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034414\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170383\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 465\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065054  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099068\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049692\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187932\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 466\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068400  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 467\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.209993  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.130221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 468\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036390\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 469\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 470\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.153474  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057231\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.192622\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 471\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089258  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048572\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 472\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100129  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165070\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 473\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082195  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094002\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 474\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041270  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120624\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039153\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165060\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 475\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087721  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054859\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 476\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102800\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039629\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170478\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114007  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095341\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051101\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181963\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 478\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044179\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 479\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.132966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040799\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.174123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 480\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108954  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104294\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181198\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 481\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.134487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.116181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052870\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 482\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 483\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.111352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206992\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 484\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 485\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058860  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041210\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173012\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 486\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.185889  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 487\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154288  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 488\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094371  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.121359\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035401\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 489\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077870  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 490\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048489  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045512\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.185119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 491\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065938  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104490\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042730\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 492\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.162257  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112170\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042190\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171220\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 493\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064175  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093841\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177483\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 494\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088652  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157108\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 495\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098554\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038101\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 496\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081743\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041410\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 497\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092183  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 498\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098284\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189203\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 499\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.101025  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036785\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167825\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 500\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052010  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042592\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.177323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 501\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120182  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091293\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 502\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090229  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090372\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046451\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 503\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 504\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.236045  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106939\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163836\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 505\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096441  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 506\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037831\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 507\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036219\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167065\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 508\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100203  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052482\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191221\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 509\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.184948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.117379\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053203\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 510\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.152357  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.119236\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.056056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 511\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051005  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106300\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049460\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 512\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 513\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081583  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159990\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 514\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064083  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.124376\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049589\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172797\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 515\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053817  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096242\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 516\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052517  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083910\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 517\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143004  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041983\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178602\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 518\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098251\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178697\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 519\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080433\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032655\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168470\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 520\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117737  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 521\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164305  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044708\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.175505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 522\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.151272  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032281\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 523\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144298  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102015\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049713\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188938\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 524\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056230  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048447\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 525\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083546  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193190\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 526\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066467  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 527\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175617\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 528\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096697  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104281\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.065106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201568\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 529\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093786  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107818\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059609\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198052\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 530\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105061  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 531\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103566  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 532\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069847  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092412\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037601\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 533\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102475  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044761\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 534\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126298  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097575\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 535\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104094  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041765\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 536\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037422\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 537\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099398  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.114265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042433\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 538\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100371  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163391\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 539\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085022  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187095\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 540\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.144659  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.120117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051484\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 541\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105001  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.109976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197488\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 542\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.107792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038876\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.170768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 543\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111868  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112022\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048838\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184037\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 544\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.143451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043008\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 545\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097901\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 546\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092341  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108562\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 547\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.201547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.113193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 548\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070990  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034708\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161117\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 549\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061438  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087010\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038766\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166760\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 550\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086147  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084199\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168590\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 551\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035716  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 552\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035656\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 553\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.161660  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035961\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 554\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148584  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 555\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050746  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047492\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176266\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 556\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115723  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035299\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155440\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 557\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097547\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150632\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 558\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040005\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 559\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.105730  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041263\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 560\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071069  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082378\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037972\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 561\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061648  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040290\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167690\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 562\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053873  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039416\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 563\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046432\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.179028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 564\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076404  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090533\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174098\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 565\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083377\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 566\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087156\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 567\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078092  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104093\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166809\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 568\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 569\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063715  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168415\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 570\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077278  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087078\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049268\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 571\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066522\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 572\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092582\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 573\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112495  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086090\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043829\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181894\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 574\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043615  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042380\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 575\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.140732  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098343\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 576\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065721  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038050\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163620\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 577\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.101304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049038\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 578\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078582  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100375\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049514\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 579\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076177  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099360\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179921\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 580\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056394  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.110694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040228\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 581\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056498  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 582\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119867  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079618\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164302\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 583\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090742  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035100\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165223\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 584\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033009  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044078\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.176201\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 585\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097212\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 586\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075003  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041190\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177780\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 587\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086613\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 588\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.102291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035829\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 589\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048297  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 590\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045460  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 591\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148677  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.108055\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041612\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 592\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076360  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175857\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 593\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080593  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046020\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 594\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052079  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086622\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156797\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 595\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048677  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081720\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032257\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166073\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 596\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035541  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165429\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 597\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068330  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 598\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083791\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034452\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167606\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 599\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064171  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081656\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168698\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 600\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130872  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090375\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047239\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179440\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 601\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081609  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089084\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031123\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 602\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051417  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.098996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 603\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099509  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045939\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177196\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 604\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062741  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 605\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046225  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040003\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.164594\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 606\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 607\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070875  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042525\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 608\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064701  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 609\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052836  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036478\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170748\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 610\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097911  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034143\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168100\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 611\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044301  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 612\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.114806  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038663\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 613\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106114  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097057\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040763\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 614\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095109  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097515\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 615\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123136  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 616\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117632  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 617\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063194  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085482\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031064\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 618\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181834\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 619\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076933  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189722\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 620\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133218  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 621\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061276  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085056\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 622\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104199  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090081\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172555\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 623\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 624\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055503  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074253\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041520\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173659\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 625\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058370  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097966\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043128\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 626\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034388  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042276\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.176276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 627\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041433  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171800\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 628\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076940\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171343\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 629\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086287  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080479\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042278\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183036\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 630\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060245  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187498\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 631\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076782  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039550\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168849\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 632\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081606  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093670\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 633\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090921  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.102907\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 634\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070113  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088001\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043404\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 635\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.145272  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094126\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048706\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 636\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046307  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163934\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 637\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086865\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047933\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 638\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037182  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 639\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066015  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093703\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051105\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182127\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 640\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.110372  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092750\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 641\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.138980  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035202\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159637\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 642\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117988  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051018\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182648\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 643\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058229  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039085\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 644\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120273  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 645\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071776  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035316\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169948\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 646\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058876  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154760\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 647\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053836  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041552\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.175660\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 648\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054753  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076314\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046817\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 649\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064370  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036612\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 650\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043122  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033470\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161403\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 651\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072230  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044074\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 652\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.104275  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044774\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177558\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 653\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069152  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080095\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044760\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 654\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080016  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068930\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162902\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 655\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065576  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067496\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039678\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 656\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046388  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068885\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033782\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 657\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066853  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079775\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190155\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 658\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064065  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086534\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042410\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 659\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064469  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096979\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167851\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 660\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067367  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039281\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 661\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079135  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.106037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.060399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 662\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116612  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087672\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030742\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 663\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054327  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 664\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049505  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165784\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 665\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092153  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046624\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169103\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 666\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086437\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150879\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 667\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058916  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079888\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174679\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 668\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080304  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.112464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036019\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.172061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 669\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.154115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.104557\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.205214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 670\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091386  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044144\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184270\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 671\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093379  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039919\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 672\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088727  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038194\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171436\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 673\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069972  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029760\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 674\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076180  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079038\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 675\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.115291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170996\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 676\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064186  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080565\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034004\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 677\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031306\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 678\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085870  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077271\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032990\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 679\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026896\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154706\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 680\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035525\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 681\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084967  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034959\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 682\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165841\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 683\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063650  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.095541\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 684\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089796  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087175\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028991\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 685\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094268  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155245\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 686\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041946  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035510\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 687\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063396  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085686\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031703\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 688\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027501\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154358\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 689\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091283  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069298\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033814\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.164180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 690\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074179  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158675\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 691\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058691  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071489\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 692\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070141  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.090690\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044464\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 693\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077540\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 694\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049482  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066558\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169771\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 695\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064000  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071451\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170849\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 696\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056835  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080242\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032509\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 697\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076008  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 698\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054188  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.194177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 699\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084426  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078281\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 700\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089526\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038996\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 701\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116299  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.099202\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039062\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 702\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093425  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.091233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045116\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 703\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095363  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059874\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186911\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 704\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067528  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032690\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156509\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 705\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.160785  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081326\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 706\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109630  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075416\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037677\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 707\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048909  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030300\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157786\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 708\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070188\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038937\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 709\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088391  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084631\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034928\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161058\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 710\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053857  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083842\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030299\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.161613\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 711\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067783  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041337\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 712\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070763  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083961\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 713\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080101  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078445\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039035\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 714\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042733\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 715\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126986  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 716\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055774  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069551\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045588\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 717\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108135  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079652\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039186\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 718\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048613  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034322\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 719\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053875  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073692\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027252\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 720\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088936  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072957\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157491\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 721\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026412  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028877\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158092\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 722\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078598\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173344\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 723\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071300  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.096242\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 724\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080304  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.103046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154114\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 725\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084789  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.105571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 726\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078267\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029574\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155228\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 727\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035097  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093921\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193887\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 728\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.089838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167400\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 729\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070399  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073632\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 730\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068388  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024537\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146107\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 731\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050712  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068081\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032398\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.156885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 732\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095112  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 733\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087026\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040584\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167626\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 734\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072982  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082245\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036136\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165532\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 735\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093695  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030723\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158237\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 736\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032809\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159334\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 737\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168769\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 738\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077104  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.094768\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179259\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 739\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067357\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 740\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044491  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074926\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040402\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 741\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039866  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087375\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161243\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 742\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054032  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.097680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158752\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 743\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181913\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 744\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054311  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063994\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 745\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162980\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 746\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077134  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030441\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154572\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 747\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077849\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.067533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.213924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 748\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042380\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 749\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034758  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058519\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029879\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154969\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 750\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042090  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069212\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041672\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 751\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068391  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 752\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040183\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.169255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 753\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050601  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035730\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 754\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060716  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071433\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173201\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 755\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046656  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026935\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151233\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 756\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058043  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083659\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 757\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032588  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168908\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 758\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061924  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.093964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045624\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 759\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057057  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073539\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034276\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 760\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067352\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171028\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 761\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063396  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034179\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158310\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 762\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056363  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070825\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160731\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 763\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106110  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071227\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171189\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 764\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045103  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069397\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172031\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 765\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091204  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036769\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 766\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081183  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067209\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173159\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 767\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047930  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071455\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 768\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116840  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177767\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 769\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090126  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084523\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172125\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 770\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057151  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069857\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175266\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 771\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086971  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025476\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152029\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 772\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068985  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070201\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 773\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060688  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040001\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.183951\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 774\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169669\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 775\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166240\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 776\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.135553  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037265\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 777\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067969\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025581\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 778\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053565  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 779\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092870  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068369\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038232\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 780\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.137919  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076866\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 781\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054846  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071907\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 782\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.094968  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179737\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 783\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092765\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 784\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054670  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153710\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 785\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068303  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075590\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 786\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088231  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078819\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 787\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071634  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070531\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 788\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088292  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024075\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 789\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060324  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072393\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160757\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 790\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037220\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 791\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076357  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030163\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 792\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077969  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063994\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 793\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070078\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 794\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.142856  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068751\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029992\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.165838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 795\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111305  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078737\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029150\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 796\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085667  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083301\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024668\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 797\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064544\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035766\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 798\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083722\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054844\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.191445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 799\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.112578  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 800\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034241  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.084810\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165879\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 801\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073157  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045148\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 802\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075161  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066538\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.218087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 803\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053908  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183277\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 804\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111914  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073513\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042631\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 805\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120644  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070967\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028004\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 806\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076765\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036787\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161483\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 807\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045025  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068340\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 808\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037816\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168797\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 809\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061145  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046284\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181156\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 810\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055751  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083921\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062966\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 811\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087455  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164687\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 812\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.097637  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042722\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 813\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038000  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 814\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096066  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.092163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187354\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 815\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056093  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043396\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.182230\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 816\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059101  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.198267\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 817\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045074  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083868\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182614\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 818\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025555  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031621\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 819\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059099  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 820\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041529  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031394\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 821\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044681  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063806\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 822\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 823\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040906  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 824\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052150  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070617\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169765\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 825\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053790  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072360\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 826\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030898\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 827\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087666  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080312\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036688\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 828\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069478  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 829\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079309\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159252\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 830\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109059  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 831\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099048  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063643\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 832\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 833\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.111672  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072663\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036238\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 834\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042934  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179868\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 835\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117940  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.085045\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158383\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 836\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053467  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032682\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.157995\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 837\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087710  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 838\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054156  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151416\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 839\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034264  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 840\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041414  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029068\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161898\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 841\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043766  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167088\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 842\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071114\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038283\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 843\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038197  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.078033\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.216589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 844\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053196  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068106\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037243\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171010\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 845\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030655  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156405\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 846\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 847\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046987  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035708\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 848\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089286  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025878\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157153\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 849\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058791\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028340\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 850\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093145  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065187\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036062\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 851\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.100594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182437\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 852\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086486  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081623\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181825\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 853\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033723  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069481\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039719\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176880\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 854\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065980\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026986\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 855\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057272  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062924\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024630\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 856\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080360  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065332\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166978\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 857\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037840  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039472\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.162091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 858\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118752  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078142\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042185\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163654\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 859\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082483  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032162\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 860\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086295  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080178\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168668\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 861\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055807  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.088962\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043628\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172293\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 862\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047114  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065087\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 863\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.131593  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.081176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042771\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167551\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 864\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057797  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045813\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166952\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 865\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073360\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039057\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 866\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075461  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062355\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 867\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050380  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065003\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 868\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090484  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074801\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 869\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043783  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060905\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028955\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156776\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 870\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053139  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064148\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026068\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149302\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 871\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 872\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030998  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 873\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047084  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067797\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 874\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068283  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076401\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161552\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 875\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024825  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059374\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 876\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054222  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.080171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039873\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173308\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 877\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109143  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 878\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070000  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043326\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.187873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 879\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035936  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 880\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.128118  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 881\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025889  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 882\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053867  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190177\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 883\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070961  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062902\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 884\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121968  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070809\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031893\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170648\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 885\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076626  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070583\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 886\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050972  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066535\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.190928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 887\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033002  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054985\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023890\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 888\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071021  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169337\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 889\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043049  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 890\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.081129  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066427\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 891\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085366  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.082808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028296\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166382\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 892\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038826  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073413\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159384\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 893\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066134  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067005\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031258\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 894\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063848  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067286\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037041\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170712\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 895\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070425  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059560\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034968\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167128\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 896\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034745  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 897\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046655  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074305\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039769\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 898\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023420\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148900\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 899\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099449  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041998\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.180880\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 900\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119033  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070729\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030540\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 901\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.086545\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025908\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146688\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 902\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076704  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068123\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029330\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156380\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 903\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071976\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046620\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176664\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 904\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073616  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061392\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168610\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 905\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022945\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150899\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 906\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038472  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067734\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033860\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168539\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 907\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046669  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061549\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 908\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033677\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 909\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059214  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063046\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 910\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056473  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061016\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178101\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 911\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044266  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036887\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173443\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 912\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034098\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172656\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 913\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056937  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 914\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.206260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 915\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062196\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 916\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045921  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076004\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049103\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.195184\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 917\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.116793  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038644\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169810\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 918\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036945  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165455\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 919\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037383  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069033\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035317\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 920\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041518  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053211\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043541\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.174548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 921\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077223  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076973\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034278\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158503\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 922\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051038  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071497\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032917\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 923\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.103039  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036940\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170406\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 924\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046356  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042362\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 925\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054552  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.069569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 926\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060640  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026475\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156359\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 927\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057699  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072602\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 928\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050946  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074655\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034511\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164788\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 929\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038565  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183212\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 930\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056303\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039327\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169433\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 931\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041879  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060676\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155716\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 932\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085846  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036193\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 933\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050609  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069632\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031951\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 934\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056367  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 935\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059887  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073561\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.197842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 936\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026834  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024159\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 937\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042934  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 938\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043890  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 939\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050435  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070023\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036415\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172526\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 940\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036264  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061431\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164342\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 941\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058838\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038334\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.172757\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 942\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041966  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181082\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 943\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070742\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162642\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 944\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039031  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051912\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 945\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075118  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027566\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 946\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029951  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062785\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165061\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 947\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057772  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038032\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177594\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 948\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095023  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067826\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165817\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 949\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061418  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061591\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041732\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 950\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082574  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072997\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167754\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 951\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063343  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058570\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029082\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 952\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056989  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 953\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086718  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070652\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164319\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 954\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041800  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042811\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177858\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 955\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098563  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035490\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 956\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072656  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069047\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173436\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 957\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049019  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025678\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152368\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 958\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077629  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068716\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037089\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170003\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 959\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059302  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181832\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 960\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083536  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171545\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 961\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058378  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063422\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168881\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 962\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058239\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038280\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.167513\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 963\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.133123  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043737\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166147\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 964\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066187  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 965\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034034  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151528\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 966\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039738  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 967\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066761  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036087\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163683\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 968\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066124\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 969\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089038  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025421\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 970\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059303  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035053\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 971\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.109949  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 972\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064194  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061118\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 973\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068126  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073371\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161397\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 974\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049396  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062391\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187857\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 975\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051185  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169792\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 976\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056216  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070065\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164932\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 977\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046736  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048986\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186378\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 978\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040662  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066891\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167549\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 979\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074767  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.079774\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048607\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.187430\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 980\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037362  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060393\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030934\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158919\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 981\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047129  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041094\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174798\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 982\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047822  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066070\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049597\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 983\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031773\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.157706\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 984\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063901  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065963\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048050\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175819\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 985\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041048  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 986\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031744  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059151\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157622\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 987\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055248  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044633\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189890\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 988\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049524  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057908\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062750\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.204118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 989\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 990\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074090  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160493\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 991\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063026  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078723\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040373\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 992\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083796  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030868\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 993\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.207186\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 994\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063925  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.074446\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 995\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043011  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023356\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149278\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 996\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077941  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066356\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 997\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071815  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063977\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 998\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063718\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046407\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 999\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050195  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053551\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159908\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1000\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061624  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065822\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040406\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172265\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1001\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030799  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063918\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1002\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020070  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054995\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1003\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067698  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027120\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155425\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1004\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073445  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029041\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.154254\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1005\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089784  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1006\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046540  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033025\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1007\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059119  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063628\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171329\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1008\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049148  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032645\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1009\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062299  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065747\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.057895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.208350\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1010\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048325  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069419\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021070\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144666\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1011\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061834  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064079\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042027\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166477\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1012\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088628  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147263\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1013\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074569  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043080\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179318\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1014\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057234\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028326\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1015\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042470  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056503\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1016\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082789  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172783\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1017\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048302  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040072\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169303\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1018\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117129  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036442\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155778\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1019\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039343  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033347\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1020\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045511\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030365\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159112\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1021\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070036  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062585\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036114\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1022\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053089  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159909\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1023\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069319  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050055\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173711\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1024\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042015  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043365\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.182869\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1025\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039217\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166589\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1026\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060689  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055786\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174710\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1027\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.083375  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059559\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172928\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1028\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046471  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065812\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1029\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061571  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1030\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062577  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048811\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026625\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155800\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1031\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046787  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1032\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077469  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026165\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1033\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048246  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056306\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.203007\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1034\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029162  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061130\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039660\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167863\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1035\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061653  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061035\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156995\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1036\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032463  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054924\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1037\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057501\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035493\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158635\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1038\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070596\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163963\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1039\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030153  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057104\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029027\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152772\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1040\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.118684  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157750\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1041\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074749  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066674\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025480\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1042\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069281\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025451\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1043\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036748  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060363\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024530\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146526\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1044\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040785  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068412\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025117\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.149604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1045\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044256  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069761\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028833\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160192\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1046\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032838  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029486\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160362\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1047\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.117231  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063850\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1048\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041528  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1049\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072727  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065989\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1050\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021816  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063975\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156910\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1051\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1052\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039263  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068746\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1053\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032121  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026787\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1054\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055678  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035402\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1055\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040641  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058950\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040790\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1056\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062896  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057006\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1057\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067367  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075399\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039401\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163350\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1058\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065694  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061188\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168367\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1059\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051842  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064772\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1060\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072208  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059192\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1061\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065585  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065965\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025468\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155222\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1062\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045277  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.078018\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036480\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1063\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068115  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161158\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1064\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.087474  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062994\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035424\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.163001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1065\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076260  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059010\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.186310\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1066\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064028  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057639\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1067\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.059716\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184420\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1068\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032051  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062870\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.055007\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183047\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1069\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031159  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056943\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156783\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1070\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049487  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072394\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019889\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.138942\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1071\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054465  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.083008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044811\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181199\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1072\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037580  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070452\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178533\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1073\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071771  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054642\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161943\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1074\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051609  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053747\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036391\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1075\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063910\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025908\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1076\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047723  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1077\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054393  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057964\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036508\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163095\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1078\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046523  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054737\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1079\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029334  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051631\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038183\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1080\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074833  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068869\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164235\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1081\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058946  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154195\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1082\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047116  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024252\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1083\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030010  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1084\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.120510  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.077040\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.058807\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.196923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1085\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039296\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.183307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1086\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051834  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030908\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163621\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1087\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037801  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.182735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1088\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074703  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038149\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165312\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1089\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049476  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1090\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051270  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.087749\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1091\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025674  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026892\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1092\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080712  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061699\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1093\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050449  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.073447\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176893\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1094\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058063  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1095\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027191  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042162\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171135\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1096\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055125\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024815\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1097\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028112  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051375\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028403\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1098\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038490  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032366\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1099\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053490  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.184699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1100\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086782  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045774\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023233\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146343\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1101\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049906  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060263\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.143324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1102\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033478  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068381\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.134840\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059505\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1104\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.088137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068158\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038394\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.169567\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1105\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051075  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049744\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1106\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059714\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030013\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150639\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1107\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060919  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035428  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163538\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1109\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058975\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041673  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1111\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052421  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047406\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035642\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1112\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080150  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032385\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159657\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035545  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060080\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1114\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077380  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155213\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1115\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059155  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161826\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1116\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024638  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.071886\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152386\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1117\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040257  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.070502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047472\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180703\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042787  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025469\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1119\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064134  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064373\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155845\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1120\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026792\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1121\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065494  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.072395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042239\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173891\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1122\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041206  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057419\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033680\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156292\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1123\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.126251  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056673\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024663\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.149467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1125\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043624  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023985\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148941\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080930  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019010\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.135955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1127\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032669  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053972\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1128\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045510  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045650\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020432\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.138740\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1129\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047421  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061224\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1130\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039437  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054008\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029942\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1131\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031452  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1132\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029504  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044878\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1133\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030976  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052863\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040225\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159888\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1134\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067308\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163903\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1135\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055132  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033737\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1136\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041236  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148637\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1137\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080058  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039877\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1138\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.113975  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055041\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.139583\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1139\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.129319  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064847\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034608\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159199\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1140\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049952  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056403\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026728\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151593\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1141\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052536  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051980\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159796\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1142\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035166  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064914\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162521\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1143\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1144\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045100  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048596\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025633\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.153619\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1145\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034675  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069001\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1146\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029674  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060497\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051464\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1147\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079224  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051796\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042478\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164051\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1148\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042746  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031711\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033633  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051464\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171366\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1150\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057463  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169902\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1151\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038489  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052017\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020191\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.140321\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1152\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023570\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.140294\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036422  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158827\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1154\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085758  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052172\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034026\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166382\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1155\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039408  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058252\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024382\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1156\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150638\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1157\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032220  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040279\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156630\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1158\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036328  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061023\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026122\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152963\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1159\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070961  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061249\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1160\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070664  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060181\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042071\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170540\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1161\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025203  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172634\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1162\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050729  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060327\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180612\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1163\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065265  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1164\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.093764  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033723\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.156439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.077382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069028\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029419\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1166\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047628\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036370\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1167\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.075280  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049378\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024111\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1168\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032550  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019771\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1169\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047458\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034685\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1170\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041602\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175260\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1171\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.098863  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042357\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1172\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039207  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1173\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037506  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155992\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1174\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043252  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.076816\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035142\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151287\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1175\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029905  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061449\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032677\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160987\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1176\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035593  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162887\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1177\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053933\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146585\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1178\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034669  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063475\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1179\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031700  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057577\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031040\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1180\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064848  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053680\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1181\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048293  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053315\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1182\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100204  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023075\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144983\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1183\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054271  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048918\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041302\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168767\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1184\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073759  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047987\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051157\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.185219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062160  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051116\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1186\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062456  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052304\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030978\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154930\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1187\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033046  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041209\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027391\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153710\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1188\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.121865  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052919\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030187\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154472\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1189\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039457  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161029\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1190\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030554  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024928\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150269\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053683\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033912\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157840\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029152  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050592\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1193\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050585  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054247\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036678\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1194\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058879  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169036\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1195\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029189  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030104\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1196\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065391\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032269\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154762\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1197\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043480  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039143\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1198\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047977  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033282\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156134\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1199\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041559  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038879\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1200\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041992  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027183\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1201\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035827  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050930\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039936\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155916\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1202\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055689  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050625\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025580\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144103\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1203\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076372  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055614\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1204\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023339  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040032\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.163597\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1205\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028556  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148142\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1206\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048190  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047731\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042088\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166841\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052603  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049030\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025840\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1208\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047485  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052143\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142676\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1209\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072372  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060478\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.174734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1210\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068211  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061369\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146736\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1211\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032290  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049711\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025589\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1212\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067883  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055431\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1213\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050965  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049988\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1214\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037703  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054656\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041684\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167500\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1215\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038530  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060630\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156747\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1216\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034044  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046014\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150650\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057784  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037529\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172001\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1218\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040191  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057404\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1219\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029502  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054896\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019058\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144914\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1220\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042413  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170488\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1221\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058360\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031152\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167409\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1222\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043775  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033953\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179702\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1223\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054706  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054425\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026636\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157247\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1224\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019340  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035096\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.160728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1225\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035740  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055284\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033932\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1226\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080318  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061733\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026730\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151388\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1227\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035752  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062889\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1228\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034955  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051967\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032821\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159817\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1229\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052300  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049702\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027362\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028620  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056070\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026611\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1231\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042874  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057754\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024455\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151767\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1232\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068717  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053566\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025878\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147272\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1233\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031174\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1234\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072950  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169877\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1235\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1236\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039787  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052459\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163584\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1237\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047291  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062332\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152633\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1238\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065684\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048271\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176512\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1239\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031145  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024651\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145834\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1240\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037622  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061426\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145389\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1241\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.084881  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050708\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173654\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1242\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040855\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156587\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1243\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022075  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059425\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017087\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142145\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1244\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061195  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059803\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046552\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.170115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1245\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042086  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044366\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040802\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165523\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1246\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079014  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036705\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1247\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056215  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033452\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1248\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.063828  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044024\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1249\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026280  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031352\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161168\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1250\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036520  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168699\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1251\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050789  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052292\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1252\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048212  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1253\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.065163  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043218\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1254\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056131  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050353\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037697\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164502\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1255\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.050460  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056433\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025843\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155181\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1256\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044823  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176524\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055655  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.061736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.196469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.108595  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.180543\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1259\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033841  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052895\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033569\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167291\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027715  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156311\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1261\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.073942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.062153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152940\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1262\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054221  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020700\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.141577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1263\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021692  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023463\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1264\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032880  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043473\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.175121\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1265\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038045  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059831\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034770\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161950\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1266\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030397  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057020\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049419\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171016\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1267\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.078303  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.137431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1268\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033831  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046195\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153973\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031541  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047718\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021384\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150174\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052083  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051109\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028113\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1271\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053411  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041448\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170169\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1272\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028136  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045277\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022093\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152434\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1273\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052167  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048177\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1274\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.089013  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058487\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033714\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1275\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047706\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158132\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1276\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039685  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050302\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1277\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027906  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038493\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038969\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1278\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054927  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069626\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035632\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166894\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1279\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033915  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048981\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028175\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147692\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050712\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1281\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.064650  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019557\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1282\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057087  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041582\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1283\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047064  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151686\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1284\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030254  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039151\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.176257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1285\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044876  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046457\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030250\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164251\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1286\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036679  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032644\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163373\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1287\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049916\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176541\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1288\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032045  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055585\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1289\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067396  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058932\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032527\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165824\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1290\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.024431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048412\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153693\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1291\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066574  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021418\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1292\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061258  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178449\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1293\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043962  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026489\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1294\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042253  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043221\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151342\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1295\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066077  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029177\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152168\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1296\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022273  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055858\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1297\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035645  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.067292\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045372\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170315\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1298\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.095261  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145163\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1299\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027625  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.047559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170738\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1300\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037799  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169188\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1301\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038904  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069389\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028317\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154969\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1302\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032605  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025090\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147578\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1303\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030616  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064712\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158198\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1304\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029462  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059507\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029400\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.163621\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1305\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041425  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045906\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1306\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.127875  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057429\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023033\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.140212\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1307\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020038  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047678\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035920\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163459\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1308\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037579  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028265\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159941\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1309\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051758  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152118\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1310\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034246  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048161\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161311\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1311\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035923  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.061636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162719\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1312\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036648  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050936\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1313\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.068845  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169562\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1314\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046572  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022961\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.143768\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1315\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029428  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042184\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040132\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165041\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1316\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051537  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.048586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169108\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059607  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042488\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1318\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040219  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042541\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169407\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1319\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054530  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045153\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159099\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1320\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066530  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066531\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151005\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1321\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044056  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053890\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022211\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1322\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054029  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053427\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023140\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1323\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.096809  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.064225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153396\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1324\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036530  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049640\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037841\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.165070\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1325\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019695  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058717\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157951\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1326\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.069658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051117\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1327\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028071  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037525\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1328\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058934  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051834\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179773\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1329\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054433  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161616\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1330\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074053  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149871\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1331\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072297  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1332\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041934  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033890\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161056\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1333\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032834  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048341\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028040\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149262\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1334\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.092452  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038829\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157073\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1335\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076492  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.059637\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032316\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1336\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031793  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043423\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146592\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1337\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053959  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035382\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164094\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1338\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066547  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026857\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155100\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1339\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034885  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022311\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034778  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046097\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034017\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162300\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1341\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030062  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046763\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029599\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1342\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048182  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025127\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146342\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1343\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.119006  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055669\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162513\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1344\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041919  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027759\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.157935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1345\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048728  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050729\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036299\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163171\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1346\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052319  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058909\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170648\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1347\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051249  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054660\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038378\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164206\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1348\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038347  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051269\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031498\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156799\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1349\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.079272  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155272\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1350\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035012  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054521\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032582\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160195\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1351\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054451  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049373\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1352\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061201  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021307\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1353\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043338  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040735\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025310\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148274\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1354\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.042077  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058149\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035040\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157246\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.091092  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043395\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167594\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1356\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.016176  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032970\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1357\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.056592  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034885\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1358\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058106  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052323\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020671\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.140608\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1359\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040714  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146299\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1360\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.071402  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048664\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032972\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167960\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1361\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044019  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047490\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031112\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1362\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050687\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175273\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1363\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044725\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032873\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1364\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039900  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033561\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.160790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1365\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.031691  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045904\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042730\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163714\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1366\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036038  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048657\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038362\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162144\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1367\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055746\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029394\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158200\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1368\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051014  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.189741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1369\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022688  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053697\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165110\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1370\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030774  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037669\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166678\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1371\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059388  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049510\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1372\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038926  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049453\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039906\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1373\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022537  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.141935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1374\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046405  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044856\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027890\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153827\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1375\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048076  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056898\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034423\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164384\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1376\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.100429  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055474\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1377\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045960  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055757\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036344\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164805\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1378\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022225  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171474\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1379\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051562  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157794\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1380\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062784  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044815\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1381\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082106  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030109\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155707\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1382\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025787  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.040719\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160808\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1383\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046431  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023358\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.139774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1384\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036415  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063230\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051361\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.172180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1385\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058903  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055179\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034824\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149062\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1386\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052186\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026559\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.143641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1387\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.085121  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150413\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1388\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045655  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051846\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030653\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155080\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1389\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.037661  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049679\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034682\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164113\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1390\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032596  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049546\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1391\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.106400  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053650\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027933\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1392\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017527  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027118\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148743\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1393\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041142  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045194\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045820\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172463\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1394\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044219  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049433\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045430\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167791\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1395\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034821  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045835\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160817\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1396\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.082202  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047805\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029326\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1397\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.303137  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.063525\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030410\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155323\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1398\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.054393  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1399\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.059308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1400\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044200  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038893\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028742\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157720\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1401\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045898  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045112\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036757\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161317\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1402\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022384  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028291\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142347\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1403\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061514  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060310\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044223\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1404\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046348  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053329\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024070\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.141779\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1405\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038565  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043105\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.046690\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1406\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045918  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040242\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161091\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1407\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033554  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044871\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050517\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1408\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028373  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037926\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.167514\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1409\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046439  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043669\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027062\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1410\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049921  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027201\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.154674\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1411\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026511  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.060594\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044521\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166570\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1412\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026867  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1413\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.021573  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050971\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027036\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1414\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038519  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057034\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039784\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169333\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1415\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048244  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027500\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1416\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039658  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043563\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.163148\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1417\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.066862  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.066152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1418\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040317  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044372\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.040859\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171724\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1419\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048619  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044219\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150531\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1420\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045636  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045928\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1421\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032605  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039470\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024750\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.152347\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1422\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023853  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033659\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.172086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1423\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.060474  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1424\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025476  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048667\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044003\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.182582\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1425\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.049278  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027851\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1426\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039793  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045058\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149452\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1427\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035274  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043176\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036724\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1428\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036052  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042769\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028605\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.149739\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1429\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043994  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048280\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032030\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1430\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026091  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042080\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019861\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142586\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1431\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038167  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144369\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1432\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038947  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157160\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1433\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045942  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028416\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.158402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1434\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.022796  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046130\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.051826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1435\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051574  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049373\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.066126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.188666\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1436\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028791  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.068782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036549\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162002\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1437\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.080323  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.034097\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.161895\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1438\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.040793  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049030\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028047\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1439\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026709  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045066\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.044662\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178330\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1440\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036289  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.052563\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026810\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1441\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.023354  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.039884\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036384\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.169831\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1442\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046230  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051241\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.019505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.150353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1443\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052235  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.050864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177903\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1444\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.099180  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050468\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.036681\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.161456\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1445\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.053082  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046530\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1446\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032779  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047492\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.023392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.140544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1447\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039801  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037440\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1448\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032629  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.045916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039017\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165749\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1449\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025751  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041193\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1450\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.067526  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056725\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146162\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1451\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029382  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055916\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.026115\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.145558\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1452\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033401  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053921\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027338\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1453\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.045969  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046389\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039194\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.170969\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1454\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034172  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042123\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033227\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1455\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.061698  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.069083\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045788\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173763\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1456\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.130649  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051745\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.020536\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144130\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1457\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.027227  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054127\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176284\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1458\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035267  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029776\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147705\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1459\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026914  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.138553\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1460\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.070410  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.065102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054645\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175815\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1461\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043621  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042268\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1462\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030866  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031664\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156219\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1463\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.052521  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028062\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1464\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.047665  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049247\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.038449\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.173808\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1465\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029702  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.075342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.062611\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181146\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1466\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.035671  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047611\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.147515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1467\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.038639  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024279\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.140911\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1468\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.072407  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030507\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1469\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033351  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.058576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159522\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1470\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.017551  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046887\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.027643\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151133\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1471\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030237  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.018119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.139093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1472\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.076177  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044186\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022753\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.144848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1473\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033258  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.042987\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.171774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1474\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.046096  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.053051\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.033881\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1475\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026359  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041728\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1476\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030863  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.044173\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037794\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.166273\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1477\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044509  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048325\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.155464\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1478\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.020851  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048351\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032867\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157167\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1479\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043943  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.047099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035337\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156120\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1480\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.041352  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043192\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.035000\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.160138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1481\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.051156  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.042122\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028588\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157377\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1482\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036754  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.055191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.054298\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.179244\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1483\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028837  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.045426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.168126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1484\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039308  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.057386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.041838\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg val loss per batch: 0.156341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1485\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.057667  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.054102\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.052743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.175306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1486\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.025642  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.041427\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.025411\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142164\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1487\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.032948  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043226\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.024938\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.143641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1488\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.044151  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051339\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.030387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.153872\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1489\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.026505  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.021012\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.137550\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1490\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.048830  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.050817\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.028220\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.151981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1491\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.086544  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043418\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037142\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.165225\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1492\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.036798  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.157655\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1493\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.043850  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.048160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022858\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.146945\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1494\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028383  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.043218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031913\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.156035\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1495\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.074035  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.046600\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.031972\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.162541\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1496\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018538  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038210\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.072963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.210419\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1497\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.034921  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.056480\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.063585\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.193957\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1498\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029105  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.049364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.043487\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.177695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1499\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.062727  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.051342\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.037235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.173349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 1500\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.058737  [    0/ 1000]\n",
      "avg train loss per batch in training: 0.038988\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.022999\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.148258\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seperator()\n",
    "print(\"model architecture\")\n",
    "seperator()\n",
    "print(model_bn)\n",
    "total_parameters = sum(p.numel() for p in model_bn.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses_live_bn = []\n",
    "trn_losses_bn = []\n",
    "val_losses_bn = []\n",
    "\n",
    "# set counter to 0 for early stopping\n",
    "trigger_counter = 0\n",
    "val_loss_last = 1e10\n",
    "for t in range(epochs):\n",
    "    seperator()\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    seperator()\n",
    "    trn_loss_live = train_epoch(trn_dataloader, model_bn, loss_fn, optimizer)\n",
    "    trn_losses_live_es.append(trn_loss_live)\n",
    "    seperator()\n",
    "    model_bn.eval()\n",
    "    trn_loss = trn_pass(trn_dataloader, model_bn, loss_fn)\n",
    "    trn_losses_bn.append(trn_loss)\n",
    "    seperator()\n",
    "    val_loss = val_pass(val_dataloader, model_bn, loss_fn)\n",
    "    val_losses_bn.append(val_loss)\n",
    "    model_bn.train()\n",
    "    \n",
    "    seperator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "c3 = 'tab:orange'\n",
    "\n",
    "axs.plot(trn_losses_bn, label=\"train loss\", color=c1)\n",
    "axs.plot(val_losses_bn, label=\"val   loss\", color=c2)\n",
    "axs.plot(trn_losses_live_bn, label=\"live train loss\", color=c3)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"epoch\")\n",
    "axs.set_ylabel(\"MSE\")\n",
    "\n",
    "axs.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this has not solved the over-fitting problem, but at least you now know how to implement batch norm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using combinations of these methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have looked at each of these regularisation methods individually, however they work best when used together.  For example you could use either dropout or L2 regularisation to stop your network overfitting, then use early stopping to stop the training when the loss has converged.  This way we do not waste computational resources after our network has converged.  We could then make even better use of our computational resources by introducing batch normalisation.\n",
    "\n",
    "We note that often to get these methods working as you want them to, some research has to be done to find the best hyper-parameters, e.g. the $p$ parameter in dropout, the weight-decay in L2 regularisation, or the patience in early stopping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
